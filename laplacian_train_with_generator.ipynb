{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as nprnd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from keras.layers import Input, merge\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Activation, Dropout, Reshape, Flatten\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "N = 300\n",
    "M = 20000\n",
    "PERCENTILE_RANGE = np.arange(0, 101)\n",
    "NN_WIDTH = 500\n",
    "NUM_EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode_object_array(arr):\n",
    "    '''One hot encode a numpy array of objects (e.g. strings)'''\n",
    "    uniques, ids = np.unique(arr, return_inverse=True)\n",
    "    return np_utils.to_categorical(ids, len(uniques))\n",
    "\n",
    "\n",
    "def myGenerator():\n",
    "    \n",
    "    while 1:\n",
    "        sigma = 10.0 * nprnd.rand()\n",
    "        print(\"\\nTraining sigma=\" + str(sigma))\n",
    "\n",
    "        Z1 = nprnd.normal(0, sigma, (N, M))\n",
    "        Z2 = nprnd.normal(0, sigma, (N, M))\n",
    "\n",
    "        # H0 model parameter estimation under H0:\n",
    "        muH0 = np.mean(np.concatenate([Z1, Z2], axis=0), axis=0)\n",
    "        sigmaH0 = np.sqrt(np.var(np.concatenate([Z1, Z2], axis=0), axis=0))\n",
    "        # H1 model parameter estimation under H0:\n",
    "        muZ1H1 = np.mean(Z1, axis=0)\n",
    "        muZ2H1 = np.mean(Z2, axis=0)\n",
    "        sigmaZ1H1 = np.sqrt(np.var(Z1, axis=0))\n",
    "        sigmaZ2H1 = np.sqrt(np.var(Z2, axis=0))\n",
    "\n",
    "        TZ = np.sum(\n",
    "            stats.norm.logpdf(Z1, loc=muZ1H1, scale=sigmaZ1H1) + stats.norm.logpdf(Z2, loc=muZ2H1, scale=sigmaZ2H1)\n",
    "            - stats.norm.logpdf(Z1, loc=muH0, scale=sigmaH0) - stats.norm.logpdf(Z2, loc=muH0, scale=sigmaH0), axis=0)\n",
    "        percentileH0 = np.percentile(TZ, PERCENTILE_RANGE)\n",
    "        ecdfH0 = interp1d(percentileH0, PERCENTILE_RANGE, fill_value=(0.0, 100.0), bounds_error=False)\n",
    "        p1 = np.sum(ecdfH0(TZ) <= 5) / M\n",
    "\n",
    "        # for mu in mu_vec:\n",
    "        #    print(\"\\nTraining mu=\" + str(mu))\n",
    "        h_vec = (nprnd.rand(1, M) > 0.5)\n",
    "        sign_vec = 2*(nprnd.rand(1, M) > 0.5)-1\n",
    "        mu_vec = 1 * sigma * nprnd.rand(1, M) * h_vec  * sign_vec\n",
    "\n",
    "        X = nprnd.normal(0, sigma, (N, M))\n",
    "        Y = nprnd.normal(0, sigma, (N, M)) + mu_vec\n",
    "\n",
    "        # H0 model parameter estimation under H0:\n",
    "        muXYH0 = np.mean(np.concatenate([X, Y], axis=0), axis=0)\n",
    "        sigmaXYH0 = np.sqrt(np.var(np.concatenate([X, Y], axis=0), axis=0))\n",
    "        # H1 model parameter estimation under H0:\n",
    "        muXH1 = np.mean(X, axis=0)\n",
    "        muYH1 = np.mean(Y, axis=0)\n",
    "        sigmaXH1 = np.sqrt(np.var(X, axis=0))\n",
    "        sigmaYH1 = np.sqrt(np.var(Y, axis=0))\n",
    "\n",
    "        TXY = np.sum(\n",
    "            stats.norm.logpdf(X, loc=muXH1, scale=sigmaXH1) + stats.norm.logpdf(Y, loc=muYH1, scale=sigmaYH1) -\n",
    "            stats.norm.logpdf(X, loc=muXYH0, scale=sigmaXYH0) - stats.norm.logpdf(Y, loc=muXYH0, scale=sigmaXYH0),\n",
    "            axis=0)\n",
    "        pValues = 100 - ecdfH0(TXY)\n",
    "        pValues = pValues.reshape((pValues.shape[0],1))\n",
    "\n",
    "        Xtrain = np.concatenate((X.transpose(), Y.transpose()), axis=1)\n",
    "        Xtrain -= Xtrain.min(axis=1).reshape((Xtrain.shape[0],1))\n",
    "        Xtrain /= Xtrain.max(axis=1).reshape((Xtrain.shape[0],1))\n",
    "        yield (Xtrain, {'main_output': pValues, 'hypothesis': h_vec.transpose() } )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nn_model(input_size):\n",
    "\n",
    "    main_input = Input(shape=(input_size,), name='main_input')\n",
    "\n",
    "    #x = Reshape((input_size, 1))(main_input)\n",
    "    #x = Convolution1D(nb_filter=64, filter_length=3, subsample_length=1, activation='relu')(x)\n",
    "    #x = Convolution1D(nb_filter=128, filter_length=3, subsample_length=1, activation='relu')(x)\n",
    "    #x = Convolution1D(nb_filter=128, filter_length=3, subsample_length=1, activation='relu')(x)\n",
    "    #x = Flatten()(x)\n",
    "\n",
    "    x = Dense(NN_WIDTH, activation='relu')(main_input)\n",
    "    x = Dense(NN_WIDTH, activation='relu')(x)\n",
    "    x = Dense(NN_WIDTH, activation='relu')(x)\n",
    "    x = Dense(NN_WIDTH, activation='relu')(x)\n",
    "    x = Dense(NN_WIDTH, activation='relu')(x)\n",
    "    x = Dense(NN_WIDTH, activation='relu')(x)\n",
    "\n",
    "    main_output = Dense(1, activation='relu', name='main_output')(x)\n",
    "\n",
    "    model = Model(inputs=[main_input], outputs=[main_output])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss={'main_output': 'mean_squared_error'},\n",
    "                  metrics={'main_output': \"mean_squared_error\"})\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gaussianGenerator = myGenerator()\n",
    "model = get_nn_model(2*N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel/__main__.py:1: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., steps_per_epoch=1, verbose=1, epochs=1000, workers=1)`\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training sigma=3.5698226692956982Epoch 1/1000\n",
      "\n",
      "\n",
      "Training sigma=1.2670505284499511\n",
      "1/1 [==============================] - 8s - loss: 1852.6331 - mean_squared_error: 1852.6313\n",
      "Epoch 2/1000\n",
      "\n",
      "Training sigma=1.8893342851276584\n",
      "1/1 [==============================] - 7s - loss: 1775.6158 - mean_squared_error: 1775.6145\n",
      "Epoch 3/1000\n",
      "\n",
      "Training sigma=7.8306674508806005\n",
      "1/1 [==============================] - 8s - loss: 1556.0691 - mean_squared_error: 1556.0681\n",
      "Epoch 4/1000\n",
      "\n",
      "Training sigma=6.618693476387014\n",
      "1/1 [==============================] - 8s - loss: 1193.9531 - mean_squared_error: 1193.9524\n",
      "Epoch 5/1000\n",
      "\n",
      "Training sigma=8.510099097790912\n",
      "1/1 [==============================] - 8s - loss: 1405.4430 - mean_squared_error: 1405.4420\n",
      "Epoch 6/1000\n",
      "\n",
      "Training sigma=8.571275177393687\n",
      "1/1 [==============================] - 8s - loss: 1102.2598 - mean_squared_error: 1102.2590\n",
      "Epoch 7/1000\n",
      "\n",
      "Training sigma=6.316970017063015\n",
      "1/1 [==============================] - 7s - loss: 1096.7997 - mean_squared_error: 1096.7990\n",
      "Epoch 8/1000\n",
      "\n",
      "Training sigma=3.6129332079734655\n",
      "1/1 [==============================] - 8s - loss: 1141.0352 - mean_squared_error: 1141.0344\n",
      "Epoch 9/1000\n",
      "\n",
      "Training sigma=2.410188020069237\n",
      "1/1 [==============================] - 8s - loss: 1216.5676 - mean_squared_error: 1216.5667\n",
      "Epoch 10/1000\n",
      "\n",
      "Training sigma=9.30591511948855\n",
      "1/1 [==============================] - 7s - loss: 1145.2524 - mean_squared_error: 1145.2516\n",
      "Epoch 11/1000\n",
      "\n",
      "Training sigma=7.104485458706407\n",
      "1/1 [==============================] - 7s - loss: 1093.2874 - mean_squared_error: 1093.2866\n",
      "Epoch 12/1000\n",
      "\n",
      "Training sigma=1.561610742603774\n",
      "1/1 [==============================] - 7s - loss: 1063.7445 - mean_squared_error: 1063.7438\n",
      "Epoch 13/1000\n",
      "\n",
      "Training sigma=9.63971639342273\n",
      "1/1 [==============================] - 7s - loss: 1121.6561 - mean_squared_error: 1121.6553\n",
      "Epoch 14/1000\n",
      "\n",
      "Training sigma=1.1625665317085987\n",
      "1/1 [==============================] - 7s - loss: 1127.8519 - mean_squared_error: 1127.8510\n",
      "Epoch 15/1000\n",
      "\n",
      "Training sigma=3.930355663519701\n",
      "1/1 [==============================] - 7s - loss: 1074.8256 - mean_squared_error: 1074.8248\n",
      "Epoch 16/1000\n",
      "\n",
      "Training sigma=5.251188692135235\n",
      "1/1 [==============================] - 7s - loss: 1064.7843 - mean_squared_error: 1064.7836\n",
      "Epoch 17/1000\n",
      "\n",
      "Training sigma=5.235842723069856\n",
      "1/1 [==============================] - 7s - loss: 1085.9305 - mean_squared_error: 1085.9298\n",
      "Epoch 18/1000\n",
      "\n",
      "Training sigma=6.116183076291627\n",
      "1/1 [==============================] - 7s - loss: 1087.7561 - mean_squared_error: 1087.7554\n",
      "Epoch 19/1000\n",
      "\n",
      "Training sigma=1.570750850619388\n",
      "1/1 [==============================] - 7s - loss: 1117.9351 - mean_squared_error: 1117.9343\n",
      "Epoch 20/1000\n",
      "\n",
      "Training sigma=9.939675101084052\n",
      "1/1 [==============================] - 7s - loss: 1065.0009 - mean_squared_error: 1065.0002\n",
      "Epoch 21/1000\n",
      "\n",
      "Training sigma=9.67947828094087\n",
      "1/1 [==============================] - 8s - loss: 1060.9098 - mean_squared_error: 1060.9091\n",
      "Epoch 22/1000\n",
      "\n",
      "Training sigma=3.2598058211470207\n",
      "1/1 [==============================] - 7s - loss: 1061.5857 - mean_squared_error: 1061.5850\n",
      "Epoch 23/1000\n",
      "\n",
      "Training sigma=3.967395709653859\n",
      "1/1 [==============================] - 7s - loss: 1092.5450 - mean_squared_error: 1092.5443\n",
      "Epoch 24/1000\n",
      "\n",
      "Training sigma=6.109483393437704\n",
      "1/1 [==============================] - 8s - loss: 1056.8557 - mean_squared_error: 1056.8550\n",
      "Epoch 25/1000\n",
      "\n",
      "Training sigma=4.367287813071182\n",
      "1/1 [==============================] - 7s - loss: 1057.4056 - mean_squared_error: 1057.4048\n",
      "Epoch 26/1000\n",
      "\n",
      "Training sigma=1.7720789687165361\n",
      "1/1 [==============================] - 7s - loss: 1075.6498 - mean_squared_error: 1075.6489\n",
      "Epoch 27/1000\n",
      "\n",
      "Training sigma=8.295916065769395\n",
      "1/1 [==============================] - 7s - loss: 1067.8926 - mean_squared_error: 1067.8918\n",
      "Epoch 28/1000\n",
      "\n",
      "Training sigma=5.755515834467261\n",
      "1/1 [==============================] - 7s - loss: 1080.2615 - mean_squared_error: 1080.2609\n",
      "Epoch 29/1000\n",
      "\n",
      "Training sigma=9.473760512401398\n",
      "1/1 [==============================] - 7s - loss: 1072.9073 - mean_squared_error: 1072.9065\n",
      "Epoch 30/1000\n",
      "\n",
      "Training sigma=9.984381433023055\n",
      "1/1 [==============================] - 7s - loss: 1060.3472 - mean_squared_error: 1060.3464\n",
      "Epoch 31/1000\n",
      "\n",
      "Training sigma=7.477816965915805\n",
      "1/1 [==============================] - 7s - loss: 1064.7401 - mean_squared_error: 1064.7395\n",
      "Epoch 32/1000\n",
      "\n",
      "Training sigma=2.185123692985569\n",
      "1/1 [==============================] - 7s - loss: 1072.9148 - mean_squared_error: 1072.9139\n",
      "Epoch 33/1000\n",
      "\n",
      "Training sigma=9.282497709143145\n",
      "1/1 [==============================] - 7s - loss: 1064.2699 - mean_squared_error: 1064.2692\n",
      "Epoch 34/1000\n",
      "\n",
      "Training sigma=0.5031245488517155\n",
      "1/1 [==============================] - 8s - loss: 1065.2039 - mean_squared_error: 1065.2034\n",
      "Epoch 35/1000\n",
      "\n",
      "Training sigma=0.7581953498698524\n",
      "1/1 [==============================] - 8s - loss: 1061.3077 - mean_squared_error: 1061.3070\n",
      "Epoch 36/1000\n",
      "\n",
      "Training sigma=3.6435824987177856\n",
      "1/1 [==============================] - 7s - loss: 1064.8145 - mean_squared_error: 1064.8137\n",
      "Epoch 37/1000\n",
      "\n",
      "Training sigma=8.799570084915059\n",
      "1/1 [==============================] - 7s - loss: 1049.1483 - mean_squared_error: 1049.1476\n",
      "Epoch 38/1000\n",
      "\n",
      "Training sigma=8.876080506882843\n",
      "1/1 [==============================] - 7s - loss: 1045.6880 - mean_squared_error: 1045.6871\n",
      "Epoch 39/1000\n",
      "\n",
      "Training sigma=2.075966633429375\n",
      "1/1 [==============================] - 7s - loss: 1062.1261 - mean_squared_error: 1062.1252\n",
      "Epoch 40/1000\n",
      "\n",
      "Training sigma=5.569587858003716\n",
      "1/1 [==============================] - 7s - loss: 1070.8206 - mean_squared_error: 1070.8197\n",
      "Epoch 41/1000\n",
      "\n",
      "Training sigma=9.041825004941945\n",
      "1/1 [==============================] - 8s - loss: 1054.9589 - mean_squared_error: 1054.9580\n",
      "Epoch 42/1000\n",
      "\n",
      "Training sigma=5.060208189730479\n",
      "1/1 [==============================] - 7s - loss: 1072.0176 - mean_squared_error: 1072.0168\n",
      "Epoch 43/1000\n",
      "\n",
      "Training sigma=0.4259659064570209\n",
      "1/1 [==============================] - 7s - loss: 1053.4667 - mean_squared_error: 1053.4659\n",
      "Epoch 44/1000\n",
      "\n",
      "Training sigma=5.79274176198306\n",
      "1/1 [==============================] - 7s - loss: 1066.7974 - mean_squared_error: 1066.7965\n",
      "Epoch 45/1000\n",
      "\n",
      "Training sigma=5.662100850229973\n",
      "1/1 [==============================] - 7s - loss: 1059.3126 - mean_squared_error: 1059.3118\n",
      "Epoch 46/1000\n",
      "\n",
      "Training sigma=7.271929732513294\n",
      "1/1 [==============================] - 7s - loss: 1069.2781 - mean_squared_error: 1069.2773\n",
      "Epoch 47/1000\n",
      "\n",
      "Training sigma=3.706294926031256\n",
      "1/1 [==============================] - 7s - loss: 1056.5177 - mean_squared_error: 1056.5170\n",
      "Epoch 48/1000\n",
      "\n",
      "Training sigma=9.742668928946792\n",
      "1/1 [==============================] - 7s - loss: 1068.0276 - mean_squared_error: 1068.0269\n",
      "Epoch 49/1000\n",
      "\n",
      "Training sigma=4.358508457356339\n",
      "1/1 [==============================] - 7s - loss: 1060.9683 - mean_squared_error: 1060.9675\n",
      "Epoch 50/1000\n",
      "\n",
      "Training sigma=5.3432049893827696\n",
      "1/1 [==============================] - 7s - loss: 1057.7920 - mean_squared_error: 1057.7911\n",
      "Epoch 51/1000\n",
      "\n",
      "Training sigma=5.436603535672232\n",
      "1/1 [==============================] - 7s - loss: 1058.8651 - mean_squared_error: 1058.8644\n",
      "Epoch 52/1000\n",
      "\n",
      "Training sigma=9.157459197661067\n",
      "1/1 [==============================] - 7s - loss: 1068.1335 - mean_squared_error: 1068.1328\n",
      "Epoch 53/1000\n",
      "\n",
      "Training sigma=1.1639837534864006\n",
      "1/1 [==============================] - 7s - loss: 1068.6129 - mean_squared_error: 1068.6121\n",
      "Epoch 54/1000\n",
      "\n",
      "Training sigma=0.4648435694800035\n",
      "1/1 [==============================] - 7s - loss: 1053.6791 - mean_squared_error: 1053.6782\n",
      "Epoch 55/1000\n",
      "\n",
      "Training sigma=2.6185581367783097\n",
      "1/1 [==============================] - 7s - loss: 1030.8954 - mean_squared_error: 1030.8948\n",
      "Epoch 56/1000\n",
      "\n",
      "Training sigma=9.083815488713729\n",
      "1/1 [==============================] - 7s - loss: 1050.2574 - mean_squared_error: 1050.2566\n",
      "Epoch 57/1000\n",
      "\n",
      "Training sigma=1.7613242044199262\n",
      "1/1 [==============================] - 7s - loss: 1049.4717 - mean_squared_error: 1049.4709\n",
      "Epoch 58/1000\n",
      "\n",
      "Training sigma=0.9130029536348849\n",
      "1/1 [==============================] - 7s - loss: 1055.6835 - mean_squared_error: 1055.6827\n",
      "Epoch 59/1000\n",
      "\n",
      "Training sigma=7.89876038306933\n",
      "1/1 [==============================] - 7s - loss: 1062.4551 - mean_squared_error: 1062.4543\n",
      "Epoch 60/1000\n",
      "\n",
      "Training sigma=7.812583324876622\n",
      "1/1 [==============================] - 8s - loss: 1070.1980 - mean_squared_error: 1070.1971\n",
      "Epoch 61/1000\n",
      "\n",
      "Training sigma=7.889097328333961\n",
      "1/1 [==============================] - 8s - loss: 1058.3230 - mean_squared_error: 1058.3221\n",
      "Epoch 62/1000\n",
      "\n",
      "Training sigma=6.291527071890563\n",
      "1/1 [==============================] - 7s - loss: 1073.1272 - mean_squared_error: 1073.1265\n",
      "Epoch 63/1000\n",
      "\n",
      "Training sigma=0.5066126742688326\n",
      "1/1 [==============================] - 7s - loss: 1060.9452 - mean_squared_error: 1060.9445\n",
      "Epoch 64/1000\n",
      "\n",
      "Training sigma=5.552411029635903\n",
      "1/1 [==============================] - 8s - loss: 1073.6953 - mean_squared_error: 1073.6943\n",
      "Epoch 65/1000\n",
      "\n",
      "Training sigma=9.191242067413846\n",
      "1/1 [==============================] - 7s - loss: 1066.6304 - mean_squared_error: 1066.6296\n",
      "Epoch 66/1000\n",
      "\n",
      "Training sigma=8.561909176800212\n",
      "1/1 [==============================] - 7s - loss: 1067.9819 - mean_squared_error: 1067.9812\n",
      "Epoch 67/1000\n",
      "\n",
      "Training sigma=5.007184410316254\n",
      "1/1 [==============================] - 7s - loss: 1062.7532 - mean_squared_error: 1062.7523\n",
      "Epoch 68/1000\n",
      "\n",
      "Training sigma=8.194695581389178\n",
      "1/1 [==============================] - 7s - loss: 1054.5040 - mean_squared_error: 1054.5032\n",
      "Epoch 69/1000\n",
      "\n",
      "Training sigma=2.4998939720570745\n",
      "1/1 [==============================] - 7s - loss: 1065.1552 - mean_squared_error: 1065.1544\n",
      "Epoch 70/1000\n",
      "\n",
      "Training sigma=6.687156714694254\n",
      "1/1 [==============================] - 7s - loss: 1058.1736 - mean_squared_error: 1058.1730\n",
      "Epoch 71/1000\n",
      "\n",
      "Training sigma=7.886590465767991\n",
      "1/1 [==============================] - 7s - loss: 1065.7753 - mean_squared_error: 1065.7745\n",
      "Epoch 72/1000\n",
      "\n",
      "Training sigma=1.2558933058777855\n",
      "1/1 [==============================] - 8s - loss: 1045.9275 - mean_squared_error: 1045.9268\n",
      "Epoch 73/1000\n",
      "\n",
      "Training sigma=6.555768263128372\n",
      "1/1 [==============================] - 7s - loss: 1046.4307 - mean_squared_error: 1046.4298\n",
      "Epoch 74/1000\n",
      "\n",
      "Training sigma=7.089248329941268\n",
      "1/1 [==============================] - 7s - loss: 1049.2555 - mean_squared_error: 1049.2549\n",
      "Epoch 75/1000\n",
      "\n",
      "Training sigma=9.32256799521487\n",
      "1/1 [==============================] - 7s - loss: 1043.7042 - mean_squared_error: 1043.7035\n",
      "Epoch 76/1000\n",
      "\n",
      "Training sigma=7.600062940484381\n",
      "1/1 [==============================] - 7s - loss: 1048.1366 - mean_squared_error: 1048.1359\n",
      "Epoch 77/1000\n",
      "\n",
      "Training sigma=2.1206624197479\n",
      "1/1 [==============================] - 7s - loss: 1046.8754 - mean_squared_error: 1046.8745\n",
      "Epoch 78/1000\n",
      "\n",
      "Training sigma=7.773224505560579\n",
      "1/1 [==============================] - 7s - loss: 1063.6440 - mean_squared_error: 1063.6433\n",
      "Epoch 79/1000\n",
      "\n",
      "Training sigma=8.751508184610739\n",
      "1/1 [==============================] - 7s - loss: 1046.5870 - mean_squared_error: 1046.5863\n",
      "Epoch 80/1000\n",
      "\n",
      "Training sigma=1.0683184138116597\n",
      "1/1 [==============================] - 7s - loss: 1051.2825 - mean_squared_error: 1051.2817\n",
      "Epoch 81/1000\n",
      "\n",
      "Training sigma=3.8177031513010737\n",
      "1/1 [==============================] - 7s - loss: 1039.7377 - mean_squared_error: 1039.7369\n",
      "Epoch 82/1000\n",
      "\n",
      "Training sigma=4.556160211268923\n",
      "1/1 [==============================] - 7s - loss: 1040.8167 - mean_squared_error: 1040.8160\n",
      "Epoch 83/1000\n",
      "\n",
      "Training sigma=7.722189735966132\n",
      "1/1 [==============================] - 7s - loss: 1036.8667 - mean_squared_error: 1036.8661\n",
      "Epoch 84/1000\n",
      "\n",
      "Training sigma=1.571847212579628\n",
      "1/1 [==============================] - 7s - loss: 1053.4524 - mean_squared_error: 1053.4517\n",
      "Epoch 85/1000\n",
      "\n",
      "Training sigma=0.2963898221904371\n",
      "1/1 [==============================] - 7s - loss: 1047.9056 - mean_squared_error: 1047.9048\n",
      "Epoch 86/1000\n",
      "\n",
      "Training sigma=2.2944178420851005\n",
      "1/1 [==============================] - 7s - loss: 1049.6829 - mean_squared_error: 1049.6821\n",
      "Epoch 87/1000\n",
      "\n",
      "Training sigma=1.5756927027468492\n",
      "1/1 [==============================] - 7s - loss: 1033.0078 - mean_squared_error: 1033.0070\n",
      "Epoch 88/1000\n",
      "\n",
      "Training sigma=6.678394988415705\n",
      "1/1 [==============================] - 7s - loss: 1033.7386 - mean_squared_error: 1033.7379\n",
      "Epoch 89/1000\n",
      "\n",
      "Training sigma=6.100761160943971\n",
      "1/1 [==============================] - 7s - loss: 1031.5343 - mean_squared_error: 1031.5336\n",
      "Epoch 90/1000\n",
      "\n",
      "Training sigma=1.6771418448373665\n",
      "1/1 [==============================] - 7s - loss: 1040.4526 - mean_squared_error: 1040.4518\n",
      "Epoch 91/1000\n",
      "\n",
      "Training sigma=2.435932968108606\n",
      "1/1 [==============================] - 7s - loss: 1031.4612 - mean_squared_error: 1031.4604\n",
      "Epoch 92/1000\n",
      "\n",
      "Training sigma=3.8674179591050017\n",
      "1/1 [==============================] - 7s - loss: 1017.4263 - mean_squared_error: 1017.4255\n",
      "Epoch 93/1000\n",
      "\n",
      "Training sigma=9.298563416482974\n",
      "1/1 [==============================] - 7s - loss: 1026.9452 - mean_squared_error: 1026.9442\n",
      "Epoch 94/1000\n",
      "\n",
      "Training sigma=7.945103743640457\n",
      "1/1 [==============================] - 6s - loss: 1040.1862 - mean_squared_error: 1040.1853\n",
      "Epoch 95/1000\n",
      "\n",
      "Training sigma=1.5519562548543375\n",
      "1/1 [==============================] - 7s - loss: 1032.7349 - mean_squared_error: 1032.7341\n",
      "Epoch 96/1000\n",
      "\n",
      "Training sigma=5.6607924684380775\n",
      "1/1 [==============================] - 7s - loss: 1010.4476 - mean_squared_error: 1010.4470\n",
      "Epoch 97/1000\n",
      "\n",
      "Training sigma=9.23662023123368\n",
      "1/1 [==============================] - 7s - loss: 1017.2354 - mean_squared_error: 1017.2346\n",
      "Epoch 98/1000\n",
      "\n",
      "Training sigma=1.3382292786075667\n",
      "1/1 [==============================] - 6s - loss: 1007.1358 - mean_squared_error: 1007.1351\n",
      "Epoch 99/1000\n",
      "\n",
      "Training sigma=6.004187675679744\n",
      "1/1 [==============================] - 7s - loss: 1018.5482 - mean_squared_error: 1018.5475\n",
      "Epoch 100/1000\n",
      "\n",
      "Training sigma=9.649520674514653\n",
      "1/1 [==============================] - 7s - loss: 998.1890 - mean_squared_error: 998.1883\n",
      "Epoch 101/1000\n",
      "\n",
      "Training sigma=7.951744094918212\n",
      "1/1 [==============================] - 7s - loss: 1006.8250 - mean_squared_error: 1006.8242\n",
      "Epoch 102/1000\n",
      "\n",
      "Training sigma=9.461972405313325\n",
      "1/1 [==============================] - 7s - loss: 1000.6433 - mean_squared_error: 1000.6426\n",
      "Epoch 103/1000\n",
      "\n",
      "Training sigma=2.6685809490008827\n",
      "1/1 [==============================] - 7s - loss: 983.8049 - mean_squared_error: 983.8041\n",
      "Epoch 104/1000\n",
      "\n",
      "Training sigma=6.9594619994449\n",
      "1/1 [==============================] - 7s - loss: 968.7263 - mean_squared_error: 968.7256\n",
      "Epoch 105/1000\n",
      "\n",
      "Training sigma=9.642510684541856\n",
      "1/1 [==============================] - 7s - loss: 965.5720 - mean_squared_error: 965.5712\n",
      "Epoch 106/1000\n",
      "\n",
      "Training sigma=2.5761330408102467\n",
      "1/1 [==============================] - 6s - loss: 945.5906 - mean_squared_error: 945.5900\n",
      "Epoch 107/1000\n",
      "\n",
      "Training sigma=6.558639130012169\n",
      "1/1 [==============================] - 6s - loss: 953.7596 - mean_squared_error: 953.7591\n",
      "Epoch 108/1000\n",
      "\n",
      "Training sigma=5.893826293051763\n",
      "1/1 [==============================] - 7s - loss: 949.9883 - mean_squared_error: 949.9877\n",
      "Epoch 109/1000\n",
      "\n",
      "Training sigma=3.0816380882981553\n",
      "1/1 [==============================] - 7s - loss: 945.9517 - mean_squared_error: 945.9509\n",
      "Epoch 110/1000\n",
      "\n",
      "Training sigma=8.03483350090535\n",
      "1/1 [==============================] - 7s - loss: 918.7802 - mean_squared_error: 918.7794\n",
      "Epoch 111/1000\n",
      "\n",
      "Training sigma=4.454825148194254\n",
      "1/1 [==============================] - 7s - loss: 922.9620 - mean_squared_error: 922.9615\n",
      "Epoch 112/1000\n",
      "\n",
      "Training sigma=0.8060356846137329\n",
      "1/1 [==============================] - 7s - loss: 942.6717 - mean_squared_error: 942.6709\n",
      "Epoch 113/1000\n",
      "\n",
      "Training sigma=1.2338922409377195\n",
      "1/1 [==============================] - 7s - loss: 867.9715 - mean_squared_error: 867.9709\n",
      "Epoch 114/1000\n",
      "\n",
      "Training sigma=8.544567058091097\n",
      "1/1 [==============================] - 7s - loss: 898.0783 - mean_squared_error: 898.0777\n",
      "Epoch 115/1000\n",
      "\n",
      "Training sigma=6.8876411936704685\n",
      "1/1 [==============================] - 6s - loss: 878.3108 - mean_squared_error: 878.3101\n",
      "Epoch 116/1000\n",
      "\n",
      "Training sigma=5.766912748938459\n",
      "1/1 [==============================] - 7s - loss: 838.2720 - mean_squared_error: 838.2715\n",
      "Epoch 117/1000\n",
      "\n",
      "Training sigma=8.582771669809713\n",
      "1/1 [==============================] - 7s - loss: 871.6008 - mean_squared_error: 871.6002\n",
      "Epoch 118/1000\n",
      "\n",
      "Training sigma=0.5586653370947914\n",
      "1/1 [==============================] - 7s - loss: 851.6072 - mean_squared_error: 851.6066\n",
      "Epoch 119/1000\n",
      "\n",
      "Training sigma=4.657358925789134\n",
      "1/1 [==============================] - 6s - loss: 778.4170 - mean_squared_error: 778.4164\n",
      "Epoch 120/1000\n",
      "\n",
      "Training sigma=6.872670901807345\n",
      "1/1 [==============================] - 6s - loss: 839.9794 - mean_squared_error: 839.9789\n",
      "Epoch 121/1000\n",
      "\n",
      "Training sigma=4.418946336707697\n",
      "1/1 [==============================] - 7s - loss: 825.0853 - mean_squared_error: 825.0847\n",
      "Epoch 122/1000\n",
      "\n",
      "Training sigma=1.0523814510966\n",
      "1/1 [==============================] - 7s - loss: 760.0247 - mean_squared_error: 760.0242\n",
      "Epoch 123/1000\n",
      "\n",
      "Training sigma=4.392229106139018\n",
      "1/1 [==============================] - 6s - loss: 862.1771 - mean_squared_error: 862.1765\n",
      "Epoch 124/1000\n",
      "\n",
      "Training sigma=6.776408047657254\n",
      "1/1 [==============================] - 6s - loss: 907.5172 - mean_squared_error: 907.5165\n",
      "Epoch 125/1000\n",
      "\n",
      "Training sigma=5.587945945900939\n",
      "1/1 [==============================] - 6s - loss: 729.4797 - mean_squared_error: 729.4793\n",
      "Epoch 126/1000\n",
      "\n",
      "Training sigma=3.35178544107652\n",
      "1/1 [==============================] - 7s - loss: 856.6435 - mean_squared_error: 856.6429\n",
      "Epoch 127/1000\n",
      "\n",
      "Training sigma=1.2768337969797172\n",
      "1/1 [==============================] - 6s - loss: 724.9176 - mean_squared_error: 724.9171\n",
      "Epoch 128/1000\n",
      "\n",
      "Training sigma=3.192735980815442\n",
      "1/1 [==============================] - 7s - loss: 804.0089 - mean_squared_error: 804.0082\n",
      "Epoch 129/1000\n",
      "\n",
      "Training sigma=1.367616629596392\n",
      "1/1 [==============================] - 7s - loss: 685.5829 - mean_squared_error: 685.5823\n",
      "Epoch 130/1000\n",
      "\n",
      "Training sigma=8.382799315559073\n",
      "1/1 [==============================] - 7s - loss: 763.2586 - mean_squared_error: 763.2580\n",
      "Epoch 131/1000\n",
      "\n",
      "Training sigma=4.365622185227446\n",
      "1/1 [==============================] - 6s - loss: 664.9346 - mean_squared_error: 664.9341\n",
      "Epoch 132/1000\n",
      "\n",
      "Training sigma=6.277312333452799\n",
      "1/1 [==============================] - 6s - loss: 700.3815 - mean_squared_error: 700.3811\n",
      "Epoch 133/1000\n",
      "\n",
      "Training sigma=3.3218065073067624\n",
      "1/1 [==============================] - 7s - loss: 665.8351 - mean_squared_error: 665.8346\n",
      "Epoch 134/1000\n",
      "\n",
      "Training sigma=9.65154710047072\n",
      "1/1 [==============================] - 7s - loss: 585.9794 - mean_squared_error: 585.9791\n",
      "Epoch 135/1000\n",
      "\n",
      "Training sigma=8.361925592360066\n",
      "1/1 [==============================] - 6s - loss: 602.7539 - mean_squared_error: 602.7535\n",
      "Epoch 136/1000\n",
      "\n",
      "Training sigma=8.633282757548011\n",
      "1/1 [==============================] - 6s - loss: 899.1347 - mean_squared_error: 899.1340\n",
      "Epoch 137/1000\n",
      "\n",
      "Training sigma=3.518942275810686\n",
      "1/1 [==============================] - 6s - loss: 1278.5604 - mean_squared_error: 1278.5596\n",
      "Epoch 138/1000\n",
      "\n",
      "Training sigma=1.646976938449204\n",
      "1/1 [==============================] - 7s - loss: 717.2447 - mean_squared_error: 717.2441\n",
      "Epoch 139/1000\n",
      "\n",
      "Training sigma=7.665433554836448\n",
      "1/1 [==============================] - 6s - loss: 747.7788 - mean_squared_error: 747.7783\n",
      "Epoch 140/1000\n",
      "\n",
      "Training sigma=0.22598960686628367\n",
      "1/1 [==============================] - 6s - loss: 791.7396 - mean_squared_error: 791.7390\n",
      "Epoch 141/1000\n",
      "\n",
      "Training sigma=4.556853251622216\n",
      "1/1 [==============================] - 7s - loss: 675.7040 - mean_squared_error: 675.7036\n",
      "Epoch 142/1000\n",
      "\n",
      "Training sigma=1.0713562353711725\n",
      "1/1 [==============================] - 6s - loss: 814.6797 - mean_squared_error: 814.6792\n",
      "Epoch 143/1000\n",
      "\n",
      "Training sigma=2.7944150641135748\n",
      "1/1 [==============================] - 7s - loss: 837.3670 - mean_squared_error: 837.3664\n",
      "Epoch 144/1000\n",
      "\n",
      "Training sigma=1.2905776949924164\n",
      "1/1 [==============================] - 7s - loss: 683.7701 - mean_squared_error: 683.7697\n",
      "Epoch 145/1000\n",
      "\n",
      "Training sigma=5.1124462286026375\n",
      "1/1 [==============================] - 6s - loss: 741.1686 - mean_squared_error: 741.1681\n",
      "Epoch 146/1000\n",
      "\n",
      "Training sigma=1.1146389335512774\n",
      "1/1 [==============================] - 6s - loss: 744.5374 - mean_squared_error: 744.5368\n",
      "Epoch 147/1000\n",
      "\n",
      "Training sigma=6.541594509409304\n",
      "1/1 [==============================] - 6s - loss: 654.2151 - mean_squared_error: 654.2147\n",
      "Epoch 148/1000\n",
      "\n",
      "Training sigma=7.723294395003198\n",
      "1/1 [==============================] - 6s - loss: 691.3326 - mean_squared_error: 691.3322\n",
      "Epoch 149/1000\n",
      "\n",
      "Training sigma=2.290343058909643\n",
      "1/1 [==============================] - 7s - loss: 662.2725 - mean_squared_error: 662.2720\n",
      "Epoch 150/1000\n",
      "\n",
      "Training sigma=9.992453143974238\n",
      "1/1 [==============================] - 7s - loss: 629.4584 - mean_squared_error: 629.4579\n",
      "Epoch 151/1000\n",
      "\n",
      "Training sigma=8.0851513870893\n",
      "1/1 [==============================] - 7s - loss: 652.4257 - mean_squared_error: 652.4252\n",
      "Epoch 152/1000\n",
      "\n",
      "Training sigma=7.7507179378190125\n",
      "1/1 [==============================] - 7s - loss: 619.9123 - mean_squared_error: 619.9119\n",
      "Epoch 153/1000\n",
      "\n",
      "Training sigma=8.803793178364309\n",
      "1/1 [==============================] - 7s - loss: 623.0013 - mean_squared_error: 623.0009\n",
      "Epoch 154/1000\n",
      "\n",
      "Training sigma=4.082506633402808\n",
      "1/1 [==============================] - 6s - loss: 592.3959 - mean_squared_error: 592.3955\n",
      "Epoch 155/1000\n",
      "\n",
      "Training sigma=0.6803764400629342\n",
      "1/1 [==============================] - 6s - loss: 594.9641 - mean_squared_error: 594.9636\n",
      "Epoch 156/1000\n",
      "\n",
      "Training sigma=6.532836282077125\n",
      "1/1 [==============================] - 7s - loss: 553.7058 - mean_squared_error: 553.7055\n",
      "Epoch 157/1000\n",
      "\n",
      "Training sigma=9.03597767759608\n",
      "1/1 [==============================] - 6s - loss: 577.3915 - mean_squared_error: 577.3911\n",
      "Epoch 158/1000\n",
      "\n",
      "Training sigma=5.900628968416201\n",
      "1/1 [==============================] - 6s - loss: 537.0451 - mean_squared_error: 537.0447\n",
      "Epoch 159/1000\n",
      "\n",
      "Training sigma=7.4000685593812925\n",
      "1/1 [==============================] - 7s - loss: 506.0107 - mean_squared_error: 506.0104\n",
      "Epoch 160/1000\n",
      "\n",
      "Training sigma=5.9363535037575765\n",
      "1/1 [==============================] - 7s - loss: 564.2260 - mean_squared_error: 564.2256\n",
      "Epoch 161/1000\n",
      "\n",
      "Training sigma=7.82802795152498\n",
      "1/1 [==============================] - 7s - loss: 655.2443 - mean_squared_error: 655.2438\n",
      "Epoch 162/1000\n",
      "\n",
      "Training sigma=2.3798372688174108\n",
      "1/1 [==============================] - 7s - loss: 786.5837 - mean_squared_error: 786.5831\n",
      "Epoch 163/1000\n",
      "\n",
      "Training sigma=5.838394096754092\n",
      "1/1 [==============================] - 6s - loss: 530.0215 - mean_squared_error: 530.0211\n",
      "Epoch 164/1000\n",
      "\n",
      "Training sigma=2.745198054929654\n",
      "1/1 [==============================] - 7s - loss: 606.4852 - mean_squared_error: 606.4849\n",
      "Epoch 165/1000\n",
      "\n",
      "Training sigma=9.715609281235091\n",
      "1/1 [==============================] - 6s - loss: 604.6440 - mean_squared_error: 604.6436\n",
      "Epoch 166/1000\n",
      "\n",
      "Training sigma=2.0448278536238607\n",
      "1/1 [==============================] - 6s - loss: 574.6502 - mean_squared_error: 574.6498\n",
      "Epoch 167/1000\n",
      "\n",
      "Training sigma=8.663001667594855\n",
      "1/1 [==============================] - 7s - loss: 599.0935 - mean_squared_error: 599.0931\n",
      "Epoch 168/1000\n",
      "\n",
      "Training sigma=6.288353628376026\n",
      "1/1 [==============================] - 6s - loss: 521.3026 - mean_squared_error: 521.3022\n",
      "Epoch 169/1000\n",
      "\n",
      "Training sigma=0.6019014901029007\n",
      "1/1 [==============================] - 6s - loss: 560.2236 - mean_squared_error: 560.2232\n",
      "Epoch 170/1000\n",
      "\n",
      "Training sigma=4.494915276596626\n",
      "1/1 [==============================] - 7s - loss: 516.8759 - mean_squared_error: 516.8755\n",
      "Epoch 171/1000\n",
      "\n",
      "Training sigma=2.3241230291115986\n",
      "1/1 [==============================] - 6s - loss: 526.4584 - mean_squared_error: 526.4579\n",
      "Epoch 172/1000\n",
      "\n",
      "Training sigma=7.180587260942689\n",
      "1/1 [==============================] - 6s - loss: 501.5836 - mean_squared_error: 501.5832\n",
      "Epoch 173/1000\n",
      "\n",
      "Training sigma=6.976025243101496\n",
      "1/1 [==============================] - 7s - loss: 524.6962 - mean_squared_error: 524.6959\n",
      "Epoch 174/1000\n",
      "\n",
      "Training sigma=5.770264620951382\n",
      "1/1 [==============================] - 7s - loss: 501.3070 - mean_squared_error: 501.3066\n",
      "Epoch 175/1000\n",
      "\n",
      "Training sigma=8.941664748390643\n",
      "1/1 [==============================] - 7s - loss: 533.1349 - mean_squared_error: 533.1346\n",
      "Epoch 176/1000\n",
      "\n",
      "Training sigma=2.0643791606643735\n",
      "1/1 [==============================] - 6s - loss: 480.3696 - mean_squared_error: 480.3693\n",
      "Epoch 177/1000\n",
      "\n",
      "Training sigma=7.330432749306105\n",
      "1/1 [==============================] - 7s - loss: 516.0851 - mean_squared_error: 516.0848\n",
      "Epoch 178/1000\n",
      "\n",
      "Training sigma=4.054997242962925\n",
      "1/1 [==============================] - 7s - loss: 485.7715 - mean_squared_error: 485.7711\n",
      "Epoch 179/1000\n",
      "\n",
      "Training sigma=3.0406947696217324\n",
      "1/1 [==============================] - 7s - loss: 474.2456 - mean_squared_error: 474.2453\n",
      "Epoch 180/1000\n",
      "\n",
      "Training sigma=1.7024905714921068\n",
      "1/1 [==============================] - 7s - loss: 495.4609 - mean_squared_error: 495.4606\n",
      "Epoch 181/1000\n",
      "\n",
      "Training sigma=2.231598000869246\n",
      "1/1 [==============================] - 6s - loss: 473.2873 - mean_squared_error: 473.2870\n",
      "Epoch 182/1000\n",
      "\n",
      "Training sigma=7.372803255625364\n",
      "1/1 [==============================] - 6s - loss: 457.4304 - mean_squared_error: 457.4301\n",
      "Epoch 183/1000\n",
      "\n",
      "Training sigma=5.279616021138639\n",
      "1/1 [==============================] - 7s - loss: 513.0959 - mean_squared_error: 513.0956\n",
      "Epoch 184/1000\n",
      "\n",
      "Training sigma=7.154482927882433\n",
      "1/1 [==============================] - 7s - loss: 668.2867 - mean_squared_error: 668.2863\n",
      "Epoch 185/1000\n",
      "\n",
      "Training sigma=1.855228903413756\n",
      "1/1 [==============================] - 6s - loss: 611.0201 - mean_squared_error: 611.0197\n",
      "Epoch 186/1000\n",
      "\n",
      "Training sigma=3.885398584146713\n",
      "1/1 [==============================] - 7s - loss: 553.0173 - mean_squared_error: 553.0168\n",
      "Epoch 187/1000\n",
      "\n",
      "Training sigma=7.8030197052172525\n",
      "1/1 [==============================] - 7s - loss: 529.5898 - mean_squared_error: 529.5895\n",
      "Epoch 188/1000\n",
      "\n",
      "Training sigma=5.466547478114262\n",
      "1/1 [==============================] - 7s - loss: 572.9976 - mean_squared_error: 572.9972\n",
      "Epoch 189/1000\n",
      "\n",
      "Training sigma=1.8245391928145749\n",
      "1/1 [==============================] - 7s - loss: 507.2231 - mean_squared_error: 507.2227\n",
      "Epoch 190/1000\n",
      "\n",
      "Training sigma=7.092558090313946\n",
      "1/1 [==============================] - 7s - loss: 594.8934 - mean_squared_error: 594.8929\n",
      "Epoch 191/1000\n",
      "\n",
      "Training sigma=9.553215887383082\n",
      "1/1 [==============================] - 6s - loss: 479.3265 - mean_squared_error: 479.3262\n",
      "Epoch 192/1000\n",
      "\n",
      "Training sigma=0.4367423931345038\n",
      "1/1 [==============================] - 6s - loss: 525.8928 - mean_squared_error: 525.8925\n",
      "Epoch 193/1000\n",
      "\n",
      "Training sigma=2.7353010695803617\n",
      "1/1 [==============================] - 6s - loss: 456.5019 - mean_squared_error: 456.5015\n",
      "Epoch 194/1000\n",
      "\n",
      "Training sigma=2.95179602738369\n",
      "1/1 [==============================] - 6s - loss: 552.4174 - mean_squared_error: 552.4170\n",
      "Epoch 195/1000\n",
      "\n",
      "Training sigma=5.034470545945675\n",
      "1/1 [==============================] - 7s - loss: 472.1689 - mean_squared_error: 472.1686\n",
      "Epoch 196/1000\n",
      "\n",
      "Training sigma=1.2490083265301155\n",
      "1/1 [==============================] - 6s - loss: 521.6038 - mean_squared_error: 521.6034\n",
      "Epoch 197/1000\n",
      "\n",
      "Training sigma=1.7586548339788088\n",
      "1/1 [==============================] - 6s - loss: 461.6554 - mean_squared_error: 461.6550\n",
      "Epoch 198/1000\n",
      "\n",
      "Training sigma=9.924784998605151\n",
      "1/1 [==============================] - 6s - loss: 487.6723 - mean_squared_error: 487.6720\n",
      "Epoch 199/1000\n",
      "\n",
      "Training sigma=3.6416514689731794\n",
      "1/1 [==============================] - 7s - loss: 475.4265 - mean_squared_error: 475.4261\n",
      "Epoch 200/1000\n",
      "\n",
      "Training sigma=9.466010579039546\n",
      "1/1 [==============================] - 6s - loss: 465.6952 - mean_squared_error: 465.6949\n",
      "Epoch 201/1000\n",
      "\n",
      "Training sigma=2.9487889215551433\n",
      "1/1 [==============================] - 7s - loss: 445.9591 - mean_squared_error: 445.9589\n",
      "Epoch 202/1000\n",
      "\n",
      "Training sigma=1.8084537440668935\n",
      "1/1 [==============================] - 7s - loss: 456.7108 - mean_squared_error: 456.7105\n",
      "Epoch 203/1000\n",
      "\n",
      "Training sigma=6.430765309054573\n",
      "1/1 [==============================] - 6s - loss: 438.0402 - mean_squared_error: 438.0399\n",
      "Epoch 204/1000\n",
      "\n",
      "Training sigma=8.070911367069442\n",
      "1/1 [==============================] - 6s - loss: 457.2426 - mean_squared_error: 457.2423\n",
      "Epoch 205/1000\n",
      "\n",
      "Training sigma=0.11855836869680947\n",
      "1/1 [==============================] - 7s - loss: 412.6899 - mean_squared_error: 412.6896\n",
      "Epoch 206/1000\n",
      "\n",
      "Training sigma=9.007980801835796\n",
      "1/1 [==============================] - 7s - loss: 490.7025 - mean_squared_error: 490.7021\n",
      "Epoch 207/1000\n",
      "\n",
      "Training sigma=6.640916541903179\n",
      "1/1 [==============================] - 6s - loss: 591.8758 - mean_squared_error: 591.8754\n",
      "Epoch 208/1000\n",
      "\n",
      "Training sigma=2.9758338753497893\n",
      "1/1 [==============================] - 6s - loss: 625.2238 - mean_squared_error: 625.2233\n",
      "Epoch 209/1000\n",
      "\n",
      "Training sigma=2.8499140558602\n",
      "1/1 [==============================] - 6s - loss: 634.5545 - mean_squared_error: 634.5540\n",
      "Epoch 210/1000\n",
      "\n",
      "Training sigma=3.5892207869697126\n",
      "1/1 [==============================] - 7s - loss: 580.4987 - mean_squared_error: 580.4983\n",
      "Epoch 211/1000\n",
      "\n",
      "Training sigma=6.84659950709574\n",
      "1/1 [==============================] - 6s - loss: 584.2749 - mean_squared_error: 584.2745\n",
      "Epoch 212/1000\n",
      "\n",
      "Training sigma=2.5705768759265846\n",
      "1/1 [==============================] - 6s - loss: 546.2953 - mean_squared_error: 546.2949\n",
      "Epoch 213/1000\n",
      "\n",
      "Training sigma=9.729280391518321\n",
      "1/1 [==============================] - 6s - loss: 516.3685 - mean_squared_error: 516.3682\n",
      "Epoch 214/1000\n",
      "\n",
      "Training sigma=4.202439483849604\n",
      "1/1 [==============================] - 6s - loss: 558.9866 - mean_squared_error: 558.9861\n",
      "Epoch 215/1000\n",
      "\n",
      "Training sigma=2.708626505207011\n",
      "1/1 [==============================] - 6s - loss: 472.4141 - mean_squared_error: 472.4137\n",
      "Epoch 216/1000\n",
      "\n",
      "Training sigma=3.69835299738149\n",
      "1/1 [==============================] - 7s - loss: 468.7300 - mean_squared_error: 468.7296\n",
      "Epoch 217/1000\n",
      "\n",
      "Training sigma=3.53112538185301\n",
      "1/1 [==============================] - 6s - loss: 436.0611 - mean_squared_error: 436.0608\n",
      "Epoch 218/1000\n",
      "\n",
      "Training sigma=3.013488164125684\n",
      "1/1 [==============================] - 7s - loss: 484.1135 - mean_squared_error: 484.1132\n",
      "Epoch 219/1000\n",
      "\n",
      "Training sigma=7.469868797538472\n",
      "1/1 [==============================] - 6s - loss: 450.3500 - mean_squared_error: 450.3497\n",
      "Epoch 220/1000\n",
      "\n",
      "Training sigma=4.099762146465523\n",
      "1/1 [==============================] - 6s - loss: 477.2133 - mean_squared_error: 477.2130\n",
      "Epoch 221/1000\n",
      "\n",
      "Training sigma=0.22405839287908402\n",
      "1/1 [==============================] - 6s - loss: 549.2764 - mean_squared_error: 549.2760\n",
      "Epoch 222/1000\n",
      "\n",
      "Training sigma=6.180182256959263\n",
      "1/1 [==============================] - 7s - loss: 776.5902 - mean_squared_error: 776.5897\n",
      "Epoch 223/1000\n",
      "\n",
      "Training sigma=9.308969914232513\n",
      "1/1 [==============================] - 6s - loss: 569.3613 - mean_squared_error: 569.3609\n",
      "Epoch 224/1000\n",
      "\n",
      "Training sigma=0.44587266271817194\n",
      "1/1 [==============================] - 6s - loss: 472.6354 - mean_squared_error: 472.6351\n",
      "Epoch 225/1000\n",
      "\n",
      "Training sigma=8.951141359097626\n",
      "1/1 [==============================] - 6s - loss: 477.4563 - mean_squared_error: 477.4560\n",
      "Epoch 226/1000\n",
      "\n",
      "Training sigma=8.109106973273631\n",
      "1/1 [==============================] - 6s - loss: 521.8612 - mean_squared_error: 521.8608\n",
      "Epoch 227/1000\n",
      "\n",
      "Training sigma=7.1166787985365945\n",
      "1/1 [==============================] - 7s - loss: 482.4192 - mean_squared_error: 482.4188\n",
      "Epoch 228/1000\n",
      "\n",
      "Training sigma=1.0343873998098163\n",
      "1/1 [==============================] - 6s - loss: 514.1918 - mean_squared_error: 514.1913\n",
      "Epoch 229/1000\n",
      "\n",
      "Training sigma=7.2588065802470645\n",
      "1/1 [==============================] - 6s - loss: 505.8285 - mean_squared_error: 505.8282\n",
      "Epoch 230/1000\n",
      "\n",
      "Training sigma=0.6781372113579398\n",
      "1/1 [==============================] - 6s - loss: 494.0307 - mean_squared_error: 494.0304\n",
      "Epoch 231/1000\n",
      "\n",
      "Training sigma=3.593838383718182\n",
      "1/1 [==============================] - 6s - loss: 481.4749 - mean_squared_error: 481.4745\n",
      "Epoch 232/1000\n",
      "\n",
      "Training sigma=3.724681585898819\n",
      "1/1 [==============================] - 7s - loss: 491.8203 - mean_squared_error: 491.8199\n",
      "Epoch 233/1000\n",
      "\n",
      "Training sigma=2.6870943787148303\n",
      "1/1 [==============================] - 6s - loss: 454.1103 - mean_squared_error: 454.1099\n",
      "Epoch 234/1000\n",
      "\n",
      "Training sigma=3.68196034546703\n",
      "1/1 [==============================] - 6s - loss: 470.7254 - mean_squared_error: 470.7251\n",
      "Epoch 235/1000\n",
      "\n",
      "Training sigma=8.93917721041538\n",
      "1/1 [==============================] - 6s - loss: 455.7154 - mean_squared_error: 455.7150\n",
      "Epoch 236/1000\n",
      "\n",
      "Training sigma=2.285509681066449\n",
      "1/1 [==============================] - 6s - loss: 447.2856 - mean_squared_error: 447.2853\n",
      "Epoch 237/1000\n",
      "\n",
      "Training sigma=3.9569437754228307\n",
      "1/1 [==============================] - 6s - loss: 456.0681 - mean_squared_error: 456.0678\n",
      "Epoch 238/1000\n",
      "\n",
      "Training sigma=2.1284067742855917\n",
      "1/1 [==============================] - 7s - loss: 430.0630 - mean_squared_error: 430.0627\n",
      "Epoch 239/1000\n",
      "\n",
      "Training sigma=9.987993988502918\n",
      "1/1 [==============================] - 6s - loss: 447.8087 - mean_squared_error: 447.8084\n",
      "Epoch 240/1000\n",
      "\n",
      "Training sigma=5.527228817296202\n",
      "1/1 [==============================] - 6s - loss: 425.8198 - mean_squared_error: 425.8195\n",
      "Epoch 241/1000\n",
      "\n",
      "Training sigma=5.410059965223124\n",
      "1/1 [==============================] - 6s - loss: 419.5053 - mean_squared_error: 419.5051\n",
      "Epoch 242/1000\n",
      "\n",
      "Training sigma=2.9838709713622666\n",
      "1/1 [==============================] - 6s - loss: 418.5021 - mean_squared_error: 418.5018\n",
      "Epoch 243/1000\n",
      "\n",
      "Training sigma=1.9277610756510344\n",
      "1/1 [==============================] - 6s - loss: 414.1518 - mean_squared_error: 414.1515\n",
      "Epoch 244/1000\n",
      "\n",
      "Training sigma=0.18036777805367943\n",
      "1/1 [==============================] - 7s - loss: 414.9742 - mean_squared_error: 414.9738\n",
      "Epoch 245/1000\n",
      "\n",
      "Training sigma=1.9942987718592098\n",
      "1/1 [==============================] - 7s - loss: 394.6449 - mean_squared_error: 394.6446\n",
      "Epoch 246/1000\n",
      "\n",
      "Training sigma=4.29372165170276\n",
      "1/1 [==============================] - 7s - loss: 407.4477 - mean_squared_error: 407.4474\n",
      "Epoch 247/1000\n",
      "\n",
      "Training sigma=6.954994183672398\n",
      "1/1 [==============================] - 6s - loss: 408.8138 - mean_squared_error: 408.8134\n",
      "Epoch 248/1000\n",
      "\n",
      "Training sigma=5.115431585254331\n",
      "1/1 [==============================] - 6s - loss: 424.4985 - mean_squared_error: 424.4983\n",
      "Epoch 249/1000\n",
      "\n",
      "Training sigma=0.4456086848374441\n",
      "1/1 [==============================] - 6s - loss: 415.6302 - mean_squared_error: 415.6299\n",
      "Epoch 250/1000\n",
      "\n",
      "Training sigma=2.4988689076098627\n",
      "1/1 [==============================] - 6s - loss: 396.4715 - mean_squared_error: 396.4713\n",
      "Epoch 251/1000\n",
      "\n",
      "Training sigma=7.685248881594128\n",
      "1/1 [==============================] - 6s - loss: 386.0408 - mean_squared_error: 386.0404\n",
      "Epoch 252/1000\n",
      "\n",
      "Training sigma=0.660870025437712\n",
      "1/1 [==============================] - 7s - loss: 387.2084 - mean_squared_error: 387.2081\n",
      "Epoch 253/1000\n",
      "\n",
      "Training sigma=9.233080985884094\n",
      "1/1 [==============================] - 7s - loss: 379.3326 - mean_squared_error: 379.3323\n",
      "Epoch 254/1000\n",
      "\n",
      "Training sigma=9.412798534112452\n",
      "1/1 [==============================] - 6s - loss: 380.7288 - mean_squared_error: 380.7285\n",
      "Epoch 255/1000\n",
      "\n",
      "Training sigma=8.246362585770617\n",
      "1/1 [==============================] - 6s - loss: 388.9607 - mean_squared_error: 388.9604\n",
      "Epoch 256/1000\n",
      "\n",
      "Training sigma=5.411015089987341\n",
      "1/1 [==============================] - 6s - loss: 371.3370 - mean_squared_error: 371.3368\n",
      "Epoch 257/1000\n",
      "\n",
      "Training sigma=4.745016121559798\n",
      "1/1 [==============================] - 6s - loss: 374.5835 - mean_squared_error: 374.5833\n",
      "Epoch 258/1000\n",
      "\n",
      "Training sigma=3.661538262154899\n",
      "1/1 [==============================] - 6s - loss: 391.9248 - mean_squared_error: 391.9245\n",
      "Epoch 259/1000\n",
      "\n",
      "Training sigma=8.346146428849863\n",
      "1/1 [==============================] - 7s - loss: 410.7888 - mean_squared_error: 410.7885\n",
      "Epoch 260/1000\n",
      "\n",
      "Training sigma=9.780768184386865\n",
      "1/1 [==============================] - 6s - loss: 472.9424 - mean_squared_error: 472.9421\n",
      "Epoch 261/1000\n",
      "\n",
      "Training sigma=5.734557867257606\n",
      "1/1 [==============================] - 6s - loss: 439.3645 - mean_squared_error: 439.3642\n",
      "Epoch 262/1000\n",
      "\n",
      "Training sigma=8.75503971388519\n",
      "1/1 [==============================] - 7s - loss: 417.8012 - mean_squared_error: 417.8009\n",
      "Epoch 263/1000\n",
      "\n",
      "Training sigma=2.7947270859460804\n",
      "1/1 [==============================] - 6s - loss: 375.9802 - mean_squared_error: 375.9799\n",
      "Epoch 264/1000\n",
      "\n",
      "Training sigma=7.9063147124470206\n",
      "1/1 [==============================] - 7s - loss: 398.5685 - mean_squared_error: 398.5682\n",
      "Epoch 265/1000\n",
      "\n",
      "Training sigma=3.29359032626603\n",
      "1/1 [==============================] - 7s - loss: 396.2368 - mean_squared_error: 396.2365\n",
      "Epoch 266/1000\n",
      "\n",
      "Training sigma=0.9890068999629731\n",
      "1/1 [==============================] - 7s - loss: 383.2125 - mean_squared_error: 383.2122\n",
      "Epoch 267/1000\n",
      "\n",
      "Training sigma=9.155217907260575\n",
      "1/1 [==============================] - 7s - loss: 383.2950 - mean_squared_error: 383.2948\n",
      "Epoch 268/1000\n",
      "\n",
      "Training sigma=0.4090628206487601\n",
      "1/1 [==============================] - 6s - loss: 371.6122 - mean_squared_error: 371.6120\n",
      "Epoch 269/1000\n",
      "\n",
      "Training sigma=4.503495569568044\n",
      "1/1 [==============================] - 7s - loss: 362.5897 - mean_squared_error: 362.5894\n",
      "Epoch 270/1000\n",
      "\n",
      "Training sigma=4.839228318619129\n",
      "1/1 [==============================] - 6s - loss: 356.1142 - mean_squared_error: 356.1139\n",
      "Epoch 271/1000\n",
      "\n",
      "Training sigma=9.43623807190489\n",
      "1/1 [==============================] - 6s - loss: 362.8203 - mean_squared_error: 362.8201\n",
      "Epoch 272/1000\n",
      "\n",
      "Training sigma=2.2690547250958613\n",
      "1/1 [==============================] - 6s - loss: 359.2875 - mean_squared_error: 359.2872\n",
      "Epoch 273/1000\n",
      "\n",
      "Training sigma=0.6247149173008937\n",
      "1/1 [==============================] - 7s - loss: 351.0238 - mean_squared_error: 351.0236\n",
      "Epoch 274/1000\n",
      "\n",
      "Training sigma=2.761727999941267\n",
      "1/1 [==============================] - 6s - loss: 360.8041 - mean_squared_error: 360.8038\n",
      "Epoch 275/1000\n",
      "\n",
      "Training sigma=6.272032109770661\n",
      "1/1 [==============================] - 7s - loss: 349.4800 - mean_squared_error: 349.4798\n",
      "Epoch 276/1000\n",
      "\n",
      "Training sigma=3.214457300568876\n",
      "1/1 [==============================] - 7s - loss: 345.8101 - mean_squared_error: 345.8098\n",
      "Epoch 277/1000\n",
      "\n",
      "Training sigma=0.4897295596399509\n",
      "1/1 [==============================] - 6s - loss: 351.2534 - mean_squared_error: 351.2532\n",
      "Epoch 278/1000\n",
      "\n",
      "Training sigma=6.667911729504441\n",
      "1/1 [==============================] - 7s - loss: 356.0738 - mean_squared_error: 356.0735\n",
      "Epoch 279/1000\n",
      "\n",
      "Training sigma=8.399820175243951\n",
      "1/1 [==============================] - 6s - loss: 360.4131 - mean_squared_error: 360.4128\n",
      "Epoch 280/1000\n",
      "\n",
      "Training sigma=5.337710114452307\n",
      "1/1 [==============================] - 6s - loss: 407.3466 - mean_squared_error: 407.3463\n",
      "Epoch 281/1000\n",
      "\n",
      "Training sigma=9.222688350687054\n",
      "1/1 [==============================] - 6s - loss: 343.6501 - mean_squared_error: 343.6498\n",
      "Epoch 282/1000\n",
      "\n",
      "Training sigma=0.8727435944152628\n",
      "1/1 [==============================] - 6s - loss: 406.9392 - mean_squared_error: 406.9389\n",
      "Epoch 283/1000\n",
      "\n",
      "Training sigma=8.89133389876461\n",
      "1/1 [==============================] - 6s - loss: 530.7603 - mean_squared_error: 530.7599\n",
      "Epoch 284/1000\n",
      "\n",
      "Training sigma=8.296689434536017\n",
      "1/1 [==============================] - 6s - loss: 589.7079 - mean_squared_error: 589.7075\n",
      "Epoch 285/1000\n",
      "\n",
      "Training sigma=3.284815335970488\n",
      "1/1 [==============================] - 7s - loss: 515.9124 - mean_squared_error: 515.9119\n",
      "Epoch 286/1000\n",
      "\n",
      "Training sigma=4.109292684809658\n",
      "1/1 [==============================] - 6s - loss: 369.6800 - mean_squared_error: 369.6797\n",
      "Epoch 287/1000\n",
      "\n",
      "Training sigma=4.015660423578246\n",
      "1/1 [==============================] - 7s - loss: 767.2203 - mean_squared_error: 767.2198\n",
      "Epoch 288/1000\n",
      "\n",
      "Training sigma=4.255955877600304\n",
      "1/1 [==============================] - 7s - loss: 513.5200 - mean_squared_error: 513.5196\n",
      "Epoch 289/1000\n",
      "\n",
      "Training sigma=7.006730839755848\n",
      "1/1 [==============================] - 7s - loss: 644.3765 - mean_squared_error: 644.3760\n",
      "Epoch 290/1000\n",
      "\n",
      "Training sigma=2.5121870538842006\n",
      "1/1 [==============================] - 6s - loss: 692.6853 - mean_squared_error: 692.6848\n",
      "Epoch 291/1000\n",
      "\n",
      "Training sigma=5.636461431479162\n",
      "1/1 [==============================] - 6s - loss: 661.5020 - mean_squared_error: 661.5015\n",
      "Epoch 292/1000\n",
      "\n",
      "Training sigma=2.285360425883395\n",
      "1/1 [==============================] - 7s - loss: 648.1588 - mean_squared_error: 648.1583\n",
      "Epoch 293/1000\n",
      "\n",
      "Training sigma=6.699887050232814\n",
      "1/1 [==============================] - 6s - loss: 594.5467 - mean_squared_error: 594.5463\n",
      "Epoch 294/1000\n",
      "\n",
      "Training sigma=1.3821226847449586\n",
      "1/1 [==============================] - 6s - loss: 581.7513 - mean_squared_error: 581.7510\n",
      "Epoch 295/1000\n",
      "\n",
      "Training sigma=2.3557730688232015\n",
      "1/1 [==============================] - 7s - loss: 558.9760 - mean_squared_error: 558.9755\n",
      "Epoch 296/1000\n",
      "\n",
      "Training sigma=6.221220606055987\n",
      "1/1 [==============================] - 7s - loss: 544.2274 - mean_squared_error: 544.2270\n",
      "Epoch 297/1000\n",
      "\n",
      "Training sigma=5.600630651585954\n",
      "1/1 [==============================] - 6s - loss: 522.4072 - mean_squared_error: 522.4069\n",
      "Epoch 298/1000\n",
      "\n",
      "Training sigma=9.016371652322686\n",
      "1/1 [==============================] - 7s - loss: 522.8353 - mean_squared_error: 522.8350\n",
      "Epoch 299/1000\n",
      "\n",
      "Training sigma=9.416462480035076\n",
      "1/1 [==============================] - 6s - loss: 495.0570 - mean_squared_error: 495.0567\n",
      "Epoch 300/1000\n",
      "\n",
      "Training sigma=1.6836463705038351\n",
      "1/1 [==============================] - 6s - loss: 472.5600 - mean_squared_error: 472.5597\n",
      "Epoch 301/1000\n",
      "\n",
      "Training sigma=7.208187872944273\n",
      "1/1 [==============================] - 7s - loss: 477.7012 - mean_squared_error: 477.7008\n",
      "Epoch 302/1000\n",
      "\n",
      "Training sigma=2.3278986453801034\n",
      "1/1 [==============================] - 7s - loss: 465.0616 - mean_squared_error: 465.0613\n",
      "Epoch 303/1000\n",
      "\n",
      "Training sigma=1.5991308773006907\n",
      "1/1 [==============================] - 6s - loss: 464.0602 - mean_squared_error: 464.0599\n",
      "Epoch 304/1000\n",
      "\n",
      "Training sigma=8.510352441272875\n",
      "1/1 [==============================] - 6s - loss: 450.9787 - mean_squared_error: 450.9784\n",
      "Epoch 305/1000\n",
      "\n",
      "Training sigma=2.183307954536283\n",
      "1/1 [==============================] - 6s - loss: 455.3103 - mean_squared_error: 455.3099\n",
      "Epoch 306/1000\n",
      "\n",
      "Training sigma=0.20105828162269912\n",
      "1/1 [==============================] - 6s - loss: 446.0225 - mean_squared_error: 446.0221\n",
      "Epoch 307/1000\n",
      "\n",
      "Training sigma=1.6494481140099027\n",
      "1/1 [==============================] - 6s - loss: 445.6772 - mean_squared_error: 445.6769\n",
      "Epoch 308/1000\n",
      "\n",
      "Training sigma=0.7519727957472899\n",
      "1/1 [==============================] - 7s - loss: 437.5117 - mean_squared_error: 437.5114\n",
      "Epoch 309/1000\n",
      "\n",
      "Training sigma=7.069764654533511\n",
      "1/1 [==============================] - 7s - loss: 433.4603 - mean_squared_error: 433.4600\n",
      "Epoch 310/1000\n",
      "\n",
      "Training sigma=7.278978149434608\n",
      "1/1 [==============================] - 6s - loss: 423.8183 - mean_squared_error: 423.8180\n",
      "Epoch 311/1000\n",
      "\n",
      "Training sigma=3.7339656942667157\n",
      "1/1 [==============================] - 6s - loss: 430.4620 - mean_squared_error: 430.4617\n",
      "Epoch 312/1000\n",
      "\n",
      "Training sigma=7.762362429088298\n",
      "1/1 [==============================] - 6s - loss: 424.3451 - mean_squared_error: 424.3448\n",
      "Epoch 313/1000\n",
      "\n",
      "Training sigma=7.360781103945505\n",
      "1/1 [==============================] - 7s - loss: 406.6596 - mean_squared_error: 406.6593\n",
      "Epoch 314/1000\n",
      "\n",
      "Training sigma=6.775822163105638\n",
      "1/1 [==============================] - 6s - loss: 432.0424 - mean_squared_error: 432.0421\n",
      "Epoch 315/1000\n",
      "\n",
      "Training sigma=7.975873423486799\n",
      "1/1 [==============================] - 6s - loss: 447.3052 - mean_squared_error: 447.3049\n",
      "Epoch 316/1000\n",
      "\n",
      "Training sigma=1.145735779961371\n",
      "1/1 [==============================] - 7s - loss: 447.3481 - mean_squared_error: 447.3478\n",
      "Epoch 317/1000\n",
      "\n",
      "Training sigma=8.964473373361084\n",
      "1/1 [==============================] - 6s - loss: 436.0500 - mean_squared_error: 436.0498\n",
      "Epoch 318/1000\n",
      "\n",
      "Training sigma=9.488028742060317\n",
      "1/1 [==============================] - 6s - loss: 407.4350 - mean_squared_error: 407.4347\n",
      "Epoch 319/1000\n",
      "\n",
      "Training sigma=9.208179080706207\n",
      "1/1 [==============================] - 7s - loss: 399.5988 - mean_squared_error: 399.5985\n",
      "Epoch 320/1000\n",
      "\n",
      "Training sigma=3.783652936331415\n",
      "1/1 [==============================] - 6s - loss: 392.4079 - mean_squared_error: 392.4076\n",
      "Epoch 321/1000\n",
      "\n",
      "Training sigma=6.0473411732946545\n",
      "1/1 [==============================] - 7s - loss: 399.9158 - mean_squared_error: 399.9155\n",
      "Epoch 322/1000\n",
      "\n",
      "Training sigma=7.175007351119829\n",
      "1/1 [==============================] - 6s - loss: 401.6072 - mean_squared_error: 401.6070\n",
      "Epoch 323/1000\n",
      "\n",
      "Training sigma=9.105508190235822\n",
      "1/1 [==============================] - 7s - loss: 399.9083 - mean_squared_error: 399.9080\n",
      "Epoch 324/1000\n",
      "\n",
      "Training sigma=8.554468423676084\n",
      "1/1 [==============================] - 6s - loss: 394.4207 - mean_squared_error: 394.4204\n",
      "Epoch 325/1000\n",
      "\n",
      "Training sigma=6.535341375138785\n",
      "1/1 [==============================] - 6s - loss: 391.3931 - mean_squared_error: 391.3928\n",
      "Epoch 326/1000\n",
      "\n",
      "Training sigma=5.07936456293587\n",
      "1/1 [==============================] - 6s - loss: 383.6590 - mean_squared_error: 383.6587\n",
      "Epoch 327/1000\n",
      "\n",
      "Training sigma=1.948525835469217\n",
      "1/1 [==============================] - 6s - loss: 391.2599 - mean_squared_error: 391.2596\n",
      "Epoch 328/1000\n",
      "\n",
      "Training sigma=3.540093559811901\n",
      "1/1 [==============================] - 7s - loss: 377.2640 - mean_squared_error: 377.2637\n",
      "Epoch 329/1000\n",
      "\n",
      "Training sigma=9.930620782492275\n",
      "1/1 [==============================] - 6s - loss: 387.5582 - mean_squared_error: 387.5579\n",
      "Epoch 330/1000\n",
      "\n",
      "Training sigma=2.161354802012636\n",
      "1/1 [==============================] - 6s - loss: 411.5597 - mean_squared_error: 411.5594\n",
      "Epoch 331/1000\n",
      "\n",
      "Training sigma=9.550102626293954\n",
      "1/1 [==============================] - 6s - loss: 455.4701 - mean_squared_error: 455.4698\n",
      "Epoch 332/1000\n",
      "\n",
      "Training sigma=2.076188255299707\n",
      "1/1 [==============================] - 6s - loss: 502.2697 - mean_squared_error: 502.2693\n",
      "Epoch 333/1000\n",
      "\n",
      "Training sigma=5.972984261234714\n",
      "1/1 [==============================] - 6s - loss: 460.8787 - mean_squared_error: 460.8783\n",
      "Epoch 334/1000\n",
      "\n",
      "Training sigma=5.516607251504491\n",
      "1/1 [==============================] - 6s - loss: 369.7147 - mean_squared_error: 369.7144\n",
      "Epoch 335/1000\n",
      "\n",
      "Training sigma=8.373033940957665\n",
      "1/1 [==============================] - 6s - loss: 423.5333 - mean_squared_error: 423.5330\n",
      "Epoch 336/1000\n",
      "\n",
      "Training sigma=7.2937537371592995\n",
      "1/1 [==============================] - 6s - loss: 385.5604 - mean_squared_error: 385.5601\n",
      "Epoch 337/1000\n",
      "\n",
      "Training sigma=9.89951631272445\n",
      "1/1 [==============================] - 6s - loss: 400.5449 - mean_squared_error: 400.5446\n",
      "Epoch 338/1000\n",
      "\n",
      "Training sigma=0.4623216341696823\n",
      "1/1 [==============================] - 6s - loss: 391.2206 - mean_squared_error: 391.2203\n",
      "Epoch 339/1000\n",
      "\n",
      "Training sigma=7.772283573114517\n",
      "1/1 [==============================] - 7s - loss: 389.2477 - mean_squared_error: 389.2474\n",
      "Epoch 340/1000\n",
      "\n",
      "Training sigma=4.8379779886332095\n",
      "1/1 [==============================] - 7s - loss: 379.7432 - mean_squared_error: 379.7429\n",
      "Epoch 341/1000\n",
      "\n",
      "Training sigma=0.7455135704144222\n",
      "1/1 [==============================] - 6s - loss: 374.0580 - mean_squared_error: 374.0577\n",
      "Epoch 342/1000\n",
      "\n",
      "Training sigma=1.2404295082310512\n",
      "1/1 [==============================] - 7s - loss: 363.9370 - mean_squared_error: 363.9367\n",
      "Epoch 343/1000\n",
      "\n",
      "Training sigma=2.465997952889316\n",
      "1/1 [==============================] - 6s - loss: 359.9549 - mean_squared_error: 359.9546\n",
      "Epoch 344/1000\n",
      "\n",
      "Training sigma=0.04423925895496894\n",
      "1/1 [==============================] - 6s - loss: 362.2299 - mean_squared_error: 362.2296\n",
      "Epoch 345/1000\n",
      "\n",
      "Training sigma=8.522617092977365\n",
      "1/1 [==============================] - 7s - loss: 377.4987 - mean_squared_error: 377.4984\n",
      "Epoch 346/1000\n",
      "\n",
      "Training sigma=3.4370993599754165\n",
      "1/1 [==============================] - 6s - loss: 373.3740 - mean_squared_error: 373.3737\n",
      "Epoch 347/1000\n",
      "\n",
      "Training sigma=4.2784959535310545\n",
      "1/1 [==============================] - 6s - loss: 357.0757 - mean_squared_error: 357.0755\n",
      "Epoch 348/1000\n",
      "\n",
      "Training sigma=3.890760549867985\n",
      "1/1 [==============================] - 6s - loss: 394.0614 - mean_squared_error: 394.0611\n",
      "Epoch 349/1000\n",
      "\n",
      "Training sigma=4.687657941402971\n",
      "1/1 [==============================] - 7s - loss: 403.1082 - mean_squared_error: 403.1079\n",
      "Epoch 350/1000\n",
      "\n",
      "Training sigma=1.180377849009061\n",
      "1/1 [==============================] - 6s - loss: 406.2002 - mean_squared_error: 406.1999\n",
      "Epoch 351/1000\n",
      "\n",
      "Training sigma=0.5263759196494799\n",
      "1/1 [==============================] - 6s - loss: 390.0767 - mean_squared_error: 390.0764\n",
      "Epoch 352/1000\n",
      "\n",
      "Training sigma=8.209116088299348\n",
      "1/1 [==============================] - 6s - loss: 373.4763 - mean_squared_error: 373.4760\n",
      "Epoch 353/1000\n",
      "\n",
      "Training sigma=6.966001272865129\n",
      "1/1 [==============================] - 7s - loss: 358.8778 - mean_squared_error: 358.8775\n",
      "Epoch 354/1000\n",
      "\n",
      "Training sigma=7.386374229947428\n",
      "1/1 [==============================] - 6s - loss: 445.0882 - mean_squared_error: 445.0878\n",
      "Epoch 355/1000\n",
      "\n",
      "Training sigma=5.7961654159776765\n",
      "1/1 [==============================] - 6s - loss: 398.2818 - mean_squared_error: 398.2815\n",
      "Epoch 356/1000\n",
      "\n",
      "Training sigma=2.077031495621926\n",
      "1/1 [==============================] - 7s - loss: 384.0880 - mean_squared_error: 384.0877\n",
      "Epoch 357/1000\n",
      "\n",
      "Training sigma=9.046103812992609\n",
      "1/1 [==============================] - 7s - loss: 388.8849 - mean_squared_error: 388.8846\n",
      "Epoch 358/1000\n",
      "\n",
      "Training sigma=1.3193336943745393\n",
      "1/1 [==============================] - 6s - loss: 360.6519 - mean_squared_error: 360.6517\n",
      "Epoch 359/1000\n",
      "\n",
      "Training sigma=9.94220191164601\n",
      "1/1 [==============================] - 6s - loss: 385.7983 - mean_squared_error: 385.7981\n",
      "Epoch 360/1000\n",
      "\n",
      "Training sigma=0.7054219401649464\n",
      "1/1 [==============================] - 6s - loss: 336.4757 - mean_squared_error: 336.4754\n",
      "Epoch 361/1000\n",
      "\n",
      "Training sigma=9.594114836139246\n",
      "1/1 [==============================] - 7s - loss: 354.1366 - mean_squared_error: 354.1363\n",
      "Epoch 362/1000\n",
      "\n",
      "Training sigma=5.958531129139242\n",
      "1/1 [==============================] - 6s - loss: 361.5497 - mean_squared_error: 361.5495\n",
      "Epoch 363/1000\n",
      "\n",
      "Training sigma=9.30665510975389\n",
      "1/1 [==============================] - 7s - loss: 345.5294 - mean_squared_error: 345.5292\n",
      "Epoch 364/1000\n",
      "\n",
      "Training sigma=2.532312761481079\n",
      "1/1 [==============================] - 6s - loss: 346.8342 - mean_squared_error: 346.8340\n",
      "Epoch 365/1000\n",
      "\n",
      "Training sigma=2.6495459160896173\n",
      "1/1 [==============================] - 6s - loss: 357.7847 - mean_squared_error: 357.7844\n",
      "Epoch 366/1000\n",
      "\n",
      "Training sigma=0.3314133867807345\n",
      "1/1 [==============================] - 6s - loss: 344.2895 - mean_squared_error: 344.2892\n",
      "Epoch 367/1000\n",
      "\n",
      "Training sigma=6.901782270762399\n",
      "1/1 [==============================] - 6s - loss: 325.3316 - mean_squared_error: 325.3314\n",
      "Epoch 368/1000\n",
      "\n",
      "Training sigma=2.818128770766113\n",
      "1/1 [==============================] - 6s - loss: 347.1743 - mean_squared_error: 347.1740\n",
      "Epoch 369/1000\n",
      "\n",
      "Training sigma=8.391787589298904\n",
      "1/1 [==============================] - 7s - loss: 356.4853 - mean_squared_error: 356.4851\n",
      "Epoch 370/1000\n",
      "\n",
      "Training sigma=7.679463739871332\n",
      "1/1 [==============================] - 7s - loss: 324.2072 - mean_squared_error: 324.2069\n",
      "Epoch 371/1000\n",
      "\n",
      "Training sigma=9.027885731029745\n",
      "1/1 [==============================] - 6s - loss: 347.1340 - mean_squared_error: 347.1338\n",
      "Epoch 372/1000\n",
      "\n",
      "Training sigma=4.098981647530559\n",
      "1/1 [==============================] - 6s - loss: 369.3471 - mean_squared_error: 369.3469\n",
      "Epoch 373/1000\n",
      "\n",
      "Training sigma=4.458981951276941\n",
      "1/1 [==============================] - 7s - loss: 358.1649 - mean_squared_error: 358.1646\n",
      "Epoch 374/1000\n",
      "\n",
      "Training sigma=7.92079127367977\n",
      "1/1 [==============================] - 6s - loss: 436.4570 - mean_squared_error: 436.4567\n",
      "Epoch 375/1000\n",
      "\n",
      "Training sigma=4.97596679215783\n",
      "1/1 [==============================] - 7s - loss: 397.2472 - mean_squared_error: 397.2469\n",
      "Epoch 376/1000\n",
      "\n",
      "Training sigma=7.7387785131942275\n",
      "1/1 [==============================] - 7s - loss: 433.9202 - mean_squared_error: 433.9199\n",
      "Epoch 377/1000\n",
      "\n",
      "Training sigma=8.947722418897197\n",
      "1/1 [==============================] - 7s - loss: 360.3725 - mean_squared_error: 360.3723\n",
      "Epoch 378/1000\n",
      "\n",
      "Training sigma=6.791989702847657\n",
      "1/1 [==============================] - 6s - loss: 439.9048 - mean_squared_error: 439.9045\n",
      "Epoch 379/1000\n",
      "\n",
      "Training sigma=8.646640864692575\n",
      "1/1 [==============================] - 6s - loss: 359.2182 - mean_squared_error: 359.2180\n",
      "Epoch 380/1000\n",
      "\n",
      "Training sigma=5.4346885896333275\n",
      "1/1 [==============================] - 6s - loss: 392.6160 - mean_squared_error: 392.6157\n",
      "Epoch 381/1000\n",
      "\n",
      "Training sigma=4.885811880179376\n",
      "1/1 [==============================] - 6s - loss: 343.7552 - mean_squared_error: 343.7549\n",
      "Epoch 382/1000\n",
      "\n",
      "Training sigma=4.2413249223393334\n",
      "1/1 [==============================] - 6s - loss: 393.6070 - mean_squared_error: 393.6068\n",
      "Epoch 383/1000\n",
      "\n",
      "Training sigma=2.9014918514263877\n",
      "1/1 [==============================] - 7s - loss: 329.3753 - mean_squared_error: 329.3751\n",
      "Epoch 384/1000\n",
      "\n",
      "Training sigma=5.413579506639456\n",
      "1/1 [==============================] - 7s - loss: 361.0582 - mean_squared_error: 361.0579\n",
      "Epoch 385/1000\n",
      "\n",
      "Training sigma=0.7887039627802928\n",
      "1/1 [==============================] - 6s - loss: 321.9283 - mean_squared_error: 321.9281\n",
      "Epoch 386/1000\n",
      "\n",
      "Training sigma=7.831039493327586\n",
      "1/1 [==============================] - 7s - loss: 355.6625 - mean_squared_error: 355.6622\n",
      "Epoch 387/1000\n",
      "\n",
      "Training sigma=4.10333047523898\n",
      "1/1 [==============================] - 7s - loss: 359.9901 - mean_squared_error: 359.9898\n",
      "Epoch 388/1000\n",
      "\n",
      "Training sigma=7.736727925066223\n",
      "1/1 [==============================] - 7s - loss: 330.9179 - mean_squared_error: 330.9177\n",
      "Epoch 389/1000\n",
      "\n",
      "Training sigma=2.9298248890504253\n",
      "1/1 [==============================] - 7s - loss: 409.5836 - mean_squared_error: 409.5833\n",
      "Epoch 390/1000\n",
      "\n",
      "Training sigma=6.36842225597877\n",
      "1/1 [==============================] - 7s - loss: 387.4121 - mean_squared_error: 387.4118\n",
      "Epoch 391/1000\n",
      "\n",
      "Training sigma=2.9549079785161023\n",
      "1/1 [==============================] - 7s - loss: 354.3085 - mean_squared_error: 354.3082\n",
      "Epoch 392/1000\n",
      "\n",
      "Training sigma=9.849891317084934\n",
      "1/1 [==============================] - 7s - loss: 407.6687 - mean_squared_error: 407.6684\n",
      "Epoch 393/1000\n",
      "\n",
      "Training sigma=6.8735387516079784\n",
      "1/1 [==============================] - 7s - loss: 328.4320 - mean_squared_error: 328.4318\n",
      "Epoch 394/1000\n",
      "\n",
      "Training sigma=0.8003008117783006\n",
      "1/1 [==============================] - 6s - loss: 353.7649 - mean_squared_error: 353.7646\n",
      "Epoch 395/1000\n",
      "\n",
      "Training sigma=0.534084069750036\n",
      "1/1 [==============================] - 6s - loss: 316.1033 - mean_squared_error: 316.1031\n",
      "Epoch 396/1000\n",
      "\n",
      "Training sigma=1.5291206886048414\n",
      "1/1 [==============================] - 6s - loss: 366.2889 - mean_squared_error: 366.2887\n",
      "Epoch 397/1000\n",
      "\n",
      "Training sigma=4.6543310828319715\n",
      "1/1 [==============================] - 6s - loss: 335.0374 - mean_squared_error: 335.0372\n",
      "Epoch 398/1000\n",
      "\n",
      "Training sigma=0.7240326092633731\n",
      "1/1 [==============================] - 6s - loss: 346.2615 - mean_squared_error: 346.2613\n",
      "Epoch 399/1000\n",
      "\n",
      "Training sigma=4.767240430494254\n",
      "1/1 [==============================] - 7s - loss: 329.7613 - mean_squared_error: 329.7610\n",
      "Epoch 400/1000\n",
      "\n",
      "Training sigma=2.521619227920253\n",
      "1/1 [==============================] - 6s - loss: 319.5851 - mean_squared_error: 319.5849\n",
      "Epoch 401/1000\n",
      "\n",
      "Training sigma=5.243954461807679\n",
      "1/1 [==============================] - 6s - loss: 337.6960 - mean_squared_error: 337.6957\n",
      "Epoch 402/1000\n",
      "\n",
      "Training sigma=2.973456565015846\n",
      "1/1 [==============================] - 6s - loss: 311.3411 - mean_squared_error: 311.3409\n",
      "Epoch 403/1000\n",
      "\n",
      "Training sigma=5.584803192347967\n",
      "1/1 [==============================] - 7s - loss: 337.8794 - mean_squared_error: 337.8792\n",
      "Epoch 404/1000\n",
      "\n",
      "Training sigma=5.9375053393791655\n",
      "1/1 [==============================] - 7s - loss: 320.7763 - mean_squared_error: 320.7761\n",
      "Epoch 405/1000\n",
      "\n",
      "Training sigma=8.34609045902861\n",
      "1/1 [==============================] - 7s - loss: 310.7247 - mean_squared_error: 310.7245\n",
      "Epoch 406/1000\n",
      "\n",
      "Training sigma=1.015587396922354\n",
      "1/1 [==============================] - 6s - loss: 349.2120 - mean_squared_error: 349.2117\n",
      "Epoch 407/1000\n",
      "\n",
      "Training sigma=0.8715766568815442\n",
      "1/1 [==============================] - 6s - loss: 350.0845 - mean_squared_error: 350.0843\n",
      "Epoch 408/1000\n",
      "\n",
      "Training sigma=5.159238458300703\n",
      "1/1 [==============================] - 7s - loss: 306.5204 - mean_squared_error: 306.5202\n",
      "Epoch 409/1000\n",
      "\n",
      "Training sigma=3.7808155261917786\n",
      "1/1 [==============================] - 6s - loss: 394.8943 - mean_squared_error: 394.8940\n",
      "Epoch 410/1000\n",
      "\n",
      "Training sigma=3.6279298992633557\n",
      "1/1 [==============================] - 7s - loss: 442.5292 - mean_squared_error: 442.5289\n",
      "Epoch 411/1000\n",
      "\n",
      "Training sigma=4.615869016595141\n",
      "1/1 [==============================] - 7s - loss: 396.6757 - mean_squared_error: 396.6755\n",
      "Epoch 412/1000\n",
      "\n",
      "Training sigma=8.657503369102882\n",
      "1/1 [==============================] - 6s - loss: 427.2315 - mean_squared_error: 427.2313\n",
      "Epoch 413/1000\n",
      "\n",
      "Training sigma=5.716959382337442\n",
      "1/1 [==============================] - 7s - loss: 328.7497 - mean_squared_error: 328.7495\n",
      "Epoch 414/1000\n",
      "\n",
      "Training sigma=4.386815959745397\n",
      "1/1 [==============================] - 7s - loss: 361.2275 - mean_squared_error: 361.2273\n",
      "Epoch 415/1000\n",
      "\n",
      "Training sigma=7.034729908654629\n",
      "1/1 [==============================] - 6s - loss: 328.3532 - mean_squared_error: 328.3530\n",
      "Epoch 416/1000\n",
      "\n",
      "Training sigma=6.168095681000176\n",
      "1/1 [==============================] - 7s - loss: 361.0581 - mean_squared_error: 361.0579\n",
      "Epoch 417/1000\n",
      "\n",
      "Training sigma=7.894327556212085\n",
      "1/1 [==============================] - 7s - loss: 332.3608 - mean_squared_error: 332.3606\n",
      "Epoch 418/1000\n",
      "\n",
      "Training sigma=2.600605864624328\n",
      "1/1 [==============================] - 7s - loss: 345.0004 - mean_squared_error: 345.0002\n",
      "Epoch 419/1000\n",
      "\n",
      "Training sigma=6.8100759441981475\n",
      "1/1 [==============================] - 7s - loss: 305.7373 - mean_squared_error: 305.7371\n",
      "Epoch 420/1000\n",
      "\n",
      "Training sigma=1.0810851680661182\n",
      "1/1 [==============================] - 7s - loss: 332.4093 - mean_squared_error: 332.4091\n",
      "Epoch 421/1000\n",
      "\n",
      "Training sigma=5.728157152970759\n",
      "1/1 [==============================] - 6s - loss: 329.5071 - mean_squared_error: 329.5069\n",
      "Epoch 422/1000\n",
      "\n",
      "Training sigma=6.566663139566719\n",
      "1/1 [==============================] - 7s - loss: 319.9387 - mean_squared_error: 319.9385\n",
      "Epoch 423/1000\n",
      "\n",
      "Training sigma=9.616396917184586\n",
      "1/1 [==============================] - 7s - loss: 339.8220 - mean_squared_error: 339.8218\n",
      "Epoch 424/1000\n",
      "\n",
      "Training sigma=5.700187583553102\n",
      "1/1 [==============================] - 7s - loss: 306.4411 - mean_squared_error: 306.4409\n",
      "Epoch 425/1000\n",
      "\n",
      "Training sigma=2.4401131179409075\n",
      "1/1 [==============================] - 7s - loss: 316.7779 - mean_squared_error: 316.7777\n",
      "Epoch 426/1000\n",
      "\n",
      "Training sigma=7.911378426084731\n",
      "1/1 [==============================] - 7s - loss: 324.6970 - mean_squared_error: 324.6968\n",
      "Epoch 427/1000\n",
      "\n",
      "Training sigma=9.540506887908833\n",
      "1/1 [==============================] - 7s - loss: 293.9897 - mean_squared_error: 293.9895\n",
      "Epoch 428/1000\n",
      "\n",
      "Training sigma=0.12019716094650468\n",
      "1/1 [==============================] - 7s - loss: 306.7030 - mean_squared_error: 306.7028\n",
      "Epoch 429/1000\n",
      "\n",
      "Training sigma=2.611811582342971\n",
      "1/1 [==============================] - 7s - loss: 316.6225 - mean_squared_error: 316.6223\n",
      "Epoch 430/1000\n",
      "\n",
      "Training sigma=1.262140565193346\n",
      "1/1 [==============================] - 6s - loss: 302.9007 - mean_squared_error: 302.9005\n",
      "Epoch 431/1000\n",
      "\n",
      "Training sigma=7.862320134711183\n",
      "1/1 [==============================] - 7s - loss: 307.6387 - mean_squared_error: 307.6385\n",
      "Epoch 432/1000\n",
      "\n",
      "Training sigma=3.004568278874272\n",
      "1/1 [==============================] - 7s - loss: 303.8653 - mean_squared_error: 303.8651\n",
      "Epoch 433/1000\n",
      "\n",
      "Training sigma=0.7285015485005641\n",
      "1/1 [==============================] - 6s - loss: 300.7251 - mean_squared_error: 300.7249\n",
      "Epoch 434/1000\n",
      "\n",
      "Training sigma=2.8964471745125966\n",
      "1/1 [==============================] - 7s - loss: 289.5840 - mean_squared_error: 289.5838\n",
      "Epoch 435/1000\n",
      "\n",
      "Training sigma=3.1275955599381264\n",
      "1/1 [==============================] - 7s - loss: 309.6001 - mean_squared_error: 309.5999\n",
      "Epoch 436/1000\n",
      "\n",
      "Training sigma=3.3289515781242907\n",
      "1/1 [==============================] - 7s - loss: 321.0432 - mean_squared_error: 321.0430\n",
      "Epoch 437/1000\n",
      "\n",
      "Training sigma=7.792503549926551\n",
      "1/1 [==============================] - 7s - loss: 297.7566 - mean_squared_error: 297.7564\n",
      "Epoch 438/1000\n",
      "\n",
      "Training sigma=7.532013064543898\n",
      "1/1 [==============================] - 7s - loss: 296.7359 - mean_squared_error: 296.7357\n",
      "Epoch 439/1000\n",
      "\n",
      "Training sigma=1.598783428702183\n",
      "1/1 [==============================] - 7s - loss: 308.7958 - mean_squared_error: 308.7956\n",
      "Epoch 440/1000\n",
      "\n",
      "Training sigma=9.598065247499022\n",
      "1/1 [==============================] - 7s - loss: 296.3180 - mean_squared_error: 296.3178\n",
      "Epoch 441/1000\n",
      "\n",
      "Training sigma=2.187460080316248\n",
      "1/1 [==============================] - 6s - loss: 297.9285 - mean_squared_error: 297.9283\n",
      "Epoch 442/1000\n",
      "\n",
      "Training sigma=7.830282174555538\n",
      "1/1 [==============================] - 6s - loss: 290.6457 - mean_squared_error: 290.6454\n",
      "Epoch 443/1000\n",
      "\n",
      "Training sigma=3.1162566244873426\n",
      "1/1 [==============================] - 7s - loss: 301.0363 - mean_squared_error: 301.0361\n",
      "Epoch 444/1000\n",
      "\n",
      "Training sigma=6.8207034841143095\n",
      "1/1 [==============================] - 7s - loss: 301.2868 - mean_squared_error: 301.2866\n",
      "Epoch 445/1000\n",
      "\n",
      "Training sigma=4.207780264520884\n",
      "1/1 [==============================] - 7s - loss: 308.5692 - mean_squared_error: 308.5690\n",
      "Epoch 446/1000\n",
      "\n",
      "Training sigma=4.151872234104797\n",
      "1/1 [==============================] - 6s - loss: 324.4443 - mean_squared_error: 324.4441\n",
      "Epoch 447/1000\n",
      "\n",
      "Training sigma=8.21220843477678\n",
      "1/1 [==============================] - 6s - loss: 300.7480 - mean_squared_error: 300.7478\n",
      "Epoch 448/1000\n",
      "\n",
      "Training sigma=0.8959481724107132\n",
      "1/1 [==============================] - 7s - loss: 306.4459 - mean_squared_error: 306.4457\n",
      "Epoch 449/1000\n",
      "\n",
      "Training sigma=8.283612529461164\n",
      "1/1 [==============================] - 7s - loss: 289.8974 - mean_squared_error: 289.8972\n",
      "Epoch 450/1000\n",
      "\n",
      "Training sigma=7.907755486809979\n",
      "1/1 [==============================] - 7s - loss: 298.2816 - mean_squared_error: 298.2813\n",
      "Epoch 451/1000\n",
      "\n",
      "Training sigma=7.462347332866447\n",
      "1/1 [==============================] - 7s - loss: 286.8208 - mean_squared_error: 286.8206\n",
      "Epoch 452/1000\n",
      "\n",
      "Training sigma=3.3403655384173794\n",
      "1/1 [==============================] - 7s - loss: 291.1957 - mean_squared_error: 291.1955\n",
      "Epoch 453/1000\n",
      "\n",
      "Training sigma=0.11086419592634122\n",
      "1/1 [==============================] - 7s - loss: 322.9766 - mean_squared_error: 322.9763\n",
      "Epoch 454/1000\n",
      "\n",
      "Training sigma=7.52983857659301\n",
      "1/1 [==============================] - 7s - loss: 333.6452 - mean_squared_error: 333.6450\n",
      "Epoch 455/1000\n",
      "\n",
      "Training sigma=2.5096779123460733\n",
      "1/1 [==============================] - 7s - loss: 297.4493 - mean_squared_error: 297.4490\n",
      "Epoch 456/1000\n",
      "\n",
      "Training sigma=3.251225053546457\n",
      "1/1 [==============================] - 6s - loss: 281.9184 - mean_squared_error: 281.9182\n",
      "Epoch 457/1000\n",
      "\n",
      "Training sigma=4.094329764749024\n",
      "1/1 [==============================] - 7s - loss: 286.8917 - mean_squared_error: 286.8915\n",
      "Epoch 458/1000\n",
      "\n",
      "Training sigma=7.788039426908289\n",
      "1/1 [==============================] - 7s - loss: 293.0564 - mean_squared_error: 293.0562\n",
      "Epoch 459/1000\n",
      "\n",
      "Training sigma=8.193509855433858\n",
      "1/1 [==============================] - 7s - loss: 286.8775 - mean_squared_error: 286.8773\n",
      "Epoch 460/1000\n",
      "\n",
      "Training sigma=3.4886294669938422\n",
      "1/1 [==============================] - 7s - loss: 287.5616 - mean_squared_error: 287.5614\n",
      "Epoch 461/1000\n",
      "\n",
      "Training sigma=3.9572062899855434\n",
      "1/1 [==============================] - 7s - loss: 282.5784 - mean_squared_error: 282.5782\n",
      "Epoch 462/1000\n",
      "\n",
      "Training sigma=2.9930712656024836\n",
      "1/1 [==============================] - 7s - loss: 288.1724 - mean_squared_error: 288.1722\n",
      "Epoch 463/1000\n",
      "\n",
      "Training sigma=9.223377378075392\n",
      "1/1 [==============================] - 7s - loss: 288.3682 - mean_squared_error: 288.3680\n",
      "Epoch 464/1000\n",
      "\n",
      "Training sigma=0.6348484563016477\n",
      "1/1 [==============================] - 6s - loss: 275.5475 - mean_squared_error: 275.5473\n",
      "Epoch 465/1000\n",
      "\n",
      "Training sigma=6.829609887793616\n",
      "1/1 [==============================] - 7s - loss: 284.1524 - mean_squared_error: 284.1523\n",
      "Epoch 466/1000\n",
      "\n",
      "Training sigma=9.66659069534927\n",
      "1/1 [==============================] - 7s - loss: 286.3574 - mean_squared_error: 286.3572\n",
      "Epoch 467/1000\n",
      "\n",
      "Training sigma=0.9264602038494496\n",
      "1/1 [==============================] - 6s - loss: 298.3587 - mean_squared_error: 298.3586\n",
      "Epoch 468/1000\n",
      "\n",
      "Training sigma=7.361154674892107\n",
      "1/1 [==============================] - 7s - loss: 311.7567 - mean_squared_error: 311.7565\n",
      "Epoch 469/1000\n",
      "\n",
      "Training sigma=3.9164555177775116\n",
      "1/1 [==============================] - 6s - loss: 377.4629 - mean_squared_error: 377.4626\n",
      "Epoch 470/1000\n",
      "\n",
      "Training sigma=0.3438991603258823\n",
      "1/1 [==============================] - 7s - loss: 434.5511 - mean_squared_error: 434.5507\n",
      "Epoch 471/1000\n",
      "\n",
      "Training sigma=4.538178191573035\n",
      "1/1 [==============================] - 7s - loss: 424.9417 - mean_squared_error: 424.9413\n",
      "Epoch 472/1000\n",
      "\n",
      "Training sigma=7.94078479571632\n",
      "1/1 [==============================] - 6s - loss: 332.2493 - mean_squared_error: 332.2491\n",
      "Epoch 473/1000\n",
      "\n",
      "Training sigma=5.663248854930303\n",
      "1/1 [==============================] - 7s - loss: 623.7518 - mean_squared_error: 623.7514\n",
      "Epoch 474/1000\n",
      "\n",
      "Training sigma=6.002315909369463\n",
      "1/1 [==============================] - 7s - loss: 414.5638 - mean_squared_error: 414.5634\n",
      "Epoch 475/1000\n",
      "\n",
      "Training sigma=6.020535066238225\n",
      "1/1 [==============================] - 6s - loss: 476.4884 - mean_squared_error: 476.4881\n",
      "Epoch 476/1000\n",
      "\n",
      "Training sigma=8.017662190097523\n",
      "1/1 [==============================] - 7s - loss: 398.7832 - mean_squared_error: 398.7829\n",
      "Epoch 477/1000\n",
      "\n",
      "Training sigma=6.420593759874045\n",
      "1/1 [==============================] - 7s - loss: 377.8830 - mean_squared_error: 377.8828\n",
      "Epoch 478/1000\n",
      "\n",
      "Training sigma=0.049851744806411746\n",
      "1/1 [==============================] - 6s - loss: 437.8856 - mean_squared_error: 437.8853\n",
      "Epoch 479/1000\n",
      "\n",
      "Training sigma=5.411888958799951\n",
      "1/1 [==============================] - 6s - loss: 357.2980 - mean_squared_error: 357.2977\n",
      "Epoch 480/1000\n",
      "\n",
      "Training sigma=2.8713693314404907\n",
      "1/1 [==============================] - 6s - loss: 404.0868 - mean_squared_error: 404.0864\n",
      "Epoch 481/1000\n",
      "\n",
      "Training sigma=1.6751422099352031\n",
      "1/1 [==============================] - 7s - loss: 410.0698 - mean_squared_error: 410.0695\n",
      "Epoch 482/1000\n",
      "\n",
      "Training sigma=7.421242606421763\n",
      "1/1 [==============================] - 6s - loss: 344.0018 - mean_squared_error: 344.0015\n",
      "Epoch 483/1000\n",
      "\n",
      "Training sigma=7.376454788469927\n",
      "1/1 [==============================] - 7s - loss: 373.9533 - mean_squared_error: 373.9530\n",
      "Epoch 484/1000\n",
      "\n",
      "Training sigma=9.603700637068611\n",
      "1/1 [==============================] - 7s - loss: 352.9239 - mean_squared_error: 352.9236\n",
      "Epoch 485/1000\n",
      "\n",
      "Training sigma=9.237885735327557\n",
      "1/1 [==============================] - 6s - loss: 345.5239 - mean_squared_error: 345.5237\n",
      "Epoch 486/1000\n",
      "\n",
      "Training sigma=9.729701413216507\n",
      "1/1 [==============================] - 6s - loss: 360.9948 - mean_squared_error: 360.9944\n",
      "Epoch 487/1000\n",
      "\n",
      "Training sigma=8.733134952166328\n",
      "1/1 [==============================] - 6s - loss: 329.8951 - mean_squared_error: 329.8949\n",
      "Epoch 488/1000\n",
      "\n",
      "Training sigma=0.9783128339506109\n",
      "1/1 [==============================] - 6s - loss: 352.7294 - mean_squared_error: 352.7291\n",
      "Epoch 489/1000\n",
      "\n",
      "Training sigma=9.881094407626142\n",
      "1/1 [==============================] - 6s - loss: 337.2458 - mean_squared_error: 337.2456\n",
      "Epoch 490/1000\n",
      "\n",
      "Training sigma=4.386482924940214\n",
      "1/1 [==============================] - 6s - loss: 331.1637 - mean_squared_error: 331.1634\n",
      "Epoch 491/1000\n",
      "\n",
      "Training sigma=1.5956499567325544\n",
      "1/1 [==============================] - 6s - loss: 350.0301 - mean_squared_error: 350.0298\n",
      "Epoch 492/1000\n",
      "\n",
      "Training sigma=6.710841953289981\n",
      "1/1 [==============================] - 7s - loss: 290.7846 - mean_squared_error: 290.7844\n",
      "Epoch 493/1000\n",
      "\n",
      "Training sigma=5.086642809581029\n",
      "1/1 [==============================] - 7s - loss: 331.1682 - mean_squared_error: 331.1679\n",
      "Epoch 494/1000\n",
      "\n",
      "Training sigma=4.000887276208438\n",
      "1/1 [==============================] - 7s - loss: 296.3621 - mean_squared_error: 296.3618\n",
      "Epoch 495/1000\n",
      "\n",
      "Training sigma=4.953902317438484\n",
      "1/1 [==============================] - 7s - loss: 319.1858 - mean_squared_error: 319.1855\n",
      "Epoch 496/1000\n",
      "\n",
      "Training sigma=1.0728365309016918\n",
      "1/1 [==============================] - 7s - loss: 302.8505 - mean_squared_error: 302.8503\n",
      "Epoch 497/1000\n",
      "\n",
      "Training sigma=3.911243822513005\n",
      "1/1 [==============================] - 6s - loss: 301.2386 - mean_squared_error: 301.2384\n",
      "Epoch 498/1000\n",
      "\n",
      "Training sigma=7.253122535702125\n",
      "1/1 [==============================] - 7s - loss: 306.1100 - mean_squared_error: 306.1097\n",
      "Epoch 499/1000\n",
      "\n",
      "Training sigma=9.617779714295045\n",
      "1/1 [==============================] - 7s - loss: 290.5540 - mean_squared_error: 290.5538\n",
      "Epoch 500/1000\n",
      "\n",
      "Training sigma=6.966480848325116\n",
      "1/1 [==============================] - 6s - loss: 298.8903 - mean_squared_error: 298.8901\n",
      "Epoch 501/1000\n",
      "\n",
      "Training sigma=6.916343400654954\n",
      "1/1 [==============================] - 6s - loss: 294.2687 - mean_squared_error: 294.2685\n",
      "Epoch 502/1000\n",
      "\n",
      "Training sigma=3.9719395032558094\n",
      "1/1 [==============================] - 6s - loss: 292.2255 - mean_squared_error: 292.2253\n",
      "Epoch 503/1000\n",
      "\n",
      "Training sigma=8.043392011642434\n",
      "1/1 [==============================] - 7s - loss: 294.0779 - mean_squared_error: 294.0777\n",
      "Epoch 504/1000\n",
      "\n",
      "Training sigma=2.918338925601236\n",
      "1/1 [==============================] - 7s - loss: 286.3069 - mean_squared_error: 286.3067\n",
      "Epoch 505/1000\n",
      "\n",
      "Training sigma=3.9018640341591384\n",
      "1/1 [==============================] - 7s - loss: 286.7484 - mean_squared_error: 286.7482\n",
      "Epoch 506/1000\n",
      "\n",
      "Training sigma=7.262165022149897\n",
      "1/1 [==============================] - 7s - loss: 283.0159 - mean_squared_error: 283.0156\n",
      "Epoch 507/1000\n",
      "\n",
      "Training sigma=0.8813562152449228\n",
      "1/1 [==============================] - 7s - loss: 282.1559 - mean_squared_error: 282.1557\n",
      "Epoch 508/1000\n",
      "\n",
      "Training sigma=1.2058815945799073\n",
      "1/1 [==============================] - 7s - loss: 291.1509 - mean_squared_error: 291.1507\n",
      "Epoch 509/1000\n",
      "\n",
      "Training sigma=5.707169638871434\n",
      "1/1 [==============================] - 7s - loss: 292.8086 - mean_squared_error: 292.8084\n",
      "Epoch 510/1000\n",
      "\n",
      "Training sigma=5.802786959656445\n",
      "1/1 [==============================] - 7s - loss: 289.6145 - mean_squared_error: 289.6142\n",
      "Epoch 511/1000\n",
      "\n",
      "Training sigma=1.3438213593539483\n",
      "1/1 [==============================] - 7s - loss: 279.9410 - mean_squared_error: 279.9408\n",
      "Epoch 512/1000\n",
      "\n",
      "Training sigma=5.724672438567927\n",
      "1/1 [==============================] - 7s - loss: 281.8522 - mean_squared_error: 281.8520\n",
      "Epoch 513/1000\n",
      "\n",
      "Training sigma=7.477563181304985\n",
      "1/1 [==============================] - 7s - loss: 286.3106 - mean_squared_error: 286.3104\n",
      "Epoch 514/1000\n",
      "\n",
      "Training sigma=4.00717286226125\n",
      "1/1 [==============================] - 7s - loss: 279.3206 - mean_squared_error: 279.3204\n",
      "Epoch 515/1000\n",
      "\n",
      "Training sigma=5.6942076282428395\n",
      "1/1 [==============================] - 7s - loss: 274.5984 - mean_squared_error: 274.5982\n",
      "Epoch 516/1000\n",
      "\n",
      "Training sigma=3.616448468252665\n",
      "1/1 [==============================] - 7s - loss: 275.4053 - mean_squared_error: 275.4051\n",
      "Epoch 517/1000\n",
      "\n",
      "Training sigma=7.434695913564519\n",
      "1/1 [==============================] - 6s - loss: 280.4089 - mean_squared_error: 280.4087\n",
      "Epoch 518/1000\n",
      "\n",
      "Training sigma=0.6099639542890956\n",
      "1/1 [==============================] - 7s - loss: 274.1611 - mean_squared_error: 274.1609\n",
      "Epoch 519/1000\n",
      "\n",
      "Training sigma=9.637953481552058\n",
      "1/1 [==============================] - 7s - loss: 279.9411 - mean_squared_error: 279.9409\n",
      "Epoch 520/1000\n",
      "\n",
      "Training sigma=8.457137878506924\n",
      "1/1 [==============================] - 6s - loss: 269.7555 - mean_squared_error: 269.7553\n",
      "Epoch 521/1000\n",
      "\n",
      "Training sigma=4.97448580074944\n",
      "1/1 [==============================] - 7s - loss: 282.2675 - mean_squared_error: 282.2673\n",
      "Epoch 522/1000\n",
      "\n",
      "Training sigma=4.550063010375185\n",
      "1/1 [==============================] - 6s - loss: 285.4736 - mean_squared_error: 285.4734\n",
      "Epoch 523/1000\n",
      "\n",
      "Training sigma=0.1561760929203082\n",
      "1/1 [==============================] - 6s - loss: 309.4656 - mean_squared_error: 309.4654\n",
      "Epoch 524/1000\n",
      "\n",
      "Training sigma=8.377534514564983\n",
      "1/1 [==============================] - 6s - loss: 344.8275 - mean_squared_error: 344.8272\n",
      "Epoch 525/1000\n",
      "\n",
      "Training sigma=9.458276953201436\n",
      "1/1 [==============================] - 7s - loss: 392.5012 - mean_squared_error: 392.5009\n",
      "Epoch 526/1000\n",
      "\n",
      "Training sigma=0.07736433039326696\n",
      "1/1 [==============================] - 6s - loss: 329.4974 - mean_squared_error: 329.4972\n",
      "Epoch 527/1000\n",
      "\n",
      "Training sigma=3.8358828610168825\n",
      "1/1 [==============================] - 7s - loss: 273.3795 - mean_squared_error: 273.3793\n",
      "Epoch 528/1000\n",
      "\n",
      "Training sigma=8.208435049542777\n",
      "1/1 [==============================] - 7s - loss: 301.4398 - mean_squared_error: 301.4395\n",
      "Epoch 529/1000\n",
      "\n",
      "Training sigma=5.20432057066561\n",
      "1/1 [==============================] - 7s - loss: 339.4171 - mean_squared_error: 339.4168\n",
      "Epoch 530/1000\n",
      "\n",
      "Training sigma=0.8707853344004246\n",
      "1/1 [==============================] - 7s - loss: 359.5121 - mean_squared_error: 359.5119\n",
      "Epoch 531/1000\n",
      "\n",
      "Training sigma=7.759362739947166\n",
      "1/1 [==============================] - 7s - loss: 305.8296 - mean_squared_error: 305.8293\n",
      "Epoch 532/1000\n",
      "\n",
      "Training sigma=0.18394120903448719\n",
      "1/1 [==============================] - 7s - loss: 413.8140 - mean_squared_error: 413.8137\n",
      "Epoch 533/1000\n",
      "\n",
      "Training sigma=2.2934406011641775\n",
      "1/1 [==============================] - 7s - loss: 373.5450 - mean_squared_error: 373.5447\n",
      "Epoch 534/1000\n",
      "\n",
      "Training sigma=0.3502052315249937\n",
      "1/1 [==============================] - 6s - loss: 322.5401 - mean_squared_error: 322.5399\n",
      "Epoch 535/1000\n",
      "\n",
      "Training sigma=3.591130815093896\n",
      "1/1 [==============================] - 7s - loss: 377.3562 - mean_squared_error: 377.3560\n",
      "Epoch 536/1000\n",
      "\n",
      "Training sigma=4.433696997126067\n",
      "1/1 [==============================] - 7s - loss: 297.1258 - mean_squared_error: 297.1256\n",
      "Epoch 537/1000\n",
      "\n",
      "Training sigma=5.9198455878050265\n",
      "1/1 [==============================] - 6s - loss: 329.4672 - mean_squared_error: 329.4670\n",
      "Epoch 538/1000\n",
      "\n",
      "Training sigma=4.588823634098808\n",
      "1/1 [==============================] - 7s - loss: 302.1967 - mean_squared_error: 302.1965\n",
      "Epoch 539/1000\n",
      "\n",
      "Training sigma=5.28164234217423\n",
      "1/1 [==============================] - 7s - loss: 344.5193 - mean_squared_error: 344.5190\n",
      "Epoch 540/1000\n",
      "\n",
      "Training sigma=6.49105299457701\n",
      "1/1 [==============================] - 6s - loss: 288.3601 - mean_squared_error: 288.3599\n",
      "Epoch 541/1000\n",
      "\n",
      "Training sigma=2.4531081846717306\n",
      "1/1 [==============================] - 7s - loss: 336.6461 - mean_squared_error: 336.6459\n",
      "Epoch 542/1000\n",
      "\n",
      "Training sigma=6.331307658722513\n",
      "1/1 [==============================] - 6s - loss: 290.7555 - mean_squared_error: 290.7553\n",
      "Epoch 543/1000\n",
      "\n",
      "Training sigma=7.338169342260605\n",
      "1/1 [==============================] - 6s - loss: 326.7536 - mean_squared_error: 326.7534\n",
      "Epoch 544/1000\n",
      "\n",
      "Training sigma=5.972075018971027\n",
      "1/1 [==============================] - 6s - loss: 312.6827 - mean_squared_error: 312.6825\n",
      "Epoch 545/1000\n",
      "\n",
      "Training sigma=1.0149231484132393\n",
      "1/1 [==============================] - 7s - loss: 299.5945 - mean_squared_error: 299.5942\n",
      "Epoch 546/1000\n",
      "\n",
      "Training sigma=8.649473603495423\n",
      "1/1 [==============================] - 6s - loss: 311.6623 - mean_squared_error: 311.6620\n",
      "Epoch 547/1000\n",
      "\n",
      "Training sigma=4.970321510347937\n",
      "1/1 [==============================] - 7s - loss: 282.9846 - mean_squared_error: 282.9844\n",
      "Epoch 548/1000\n",
      "\n",
      "Training sigma=7.5888102132845585\n",
      "1/1 [==============================] - 6s - loss: 309.0316 - mean_squared_error: 309.0314\n",
      "Epoch 549/1000\n",
      "\n",
      "Training sigma=8.42418976443947\n",
      "1/1 [==============================] - 6s - loss: 282.7496 - mean_squared_error: 282.7494\n",
      "Epoch 550/1000\n",
      "\n",
      "Training sigma=7.7287438049008035\n",
      "1/1 [==============================] - 7s - loss: 294.3256 - mean_squared_error: 294.3253\n",
      "Epoch 551/1000\n",
      "\n",
      "Training sigma=3.9585658867360562\n",
      "1/1 [==============================] - 7s - loss: 301.5681 - mean_squared_error: 301.5678\n",
      "Epoch 552/1000\n",
      "\n",
      "Training sigma=5.435541697675697\n",
      "1/1 [==============================] - 7s - loss: 281.6056 - mean_squared_error: 281.6053\n",
      "Epoch 553/1000\n",
      "\n",
      "Training sigma=8.229724670396713\n",
      "1/1 [==============================] - 6s - loss: 313.1688 - mean_squared_error: 313.1685\n",
      "Epoch 554/1000\n",
      "\n",
      "Training sigma=0.47333746514757746\n",
      "1/1 [==============================] - 7s - loss: 293.7865 - mean_squared_error: 293.7863\n",
      "Epoch 555/1000\n",
      "\n",
      "Training sigma=9.649642002819828\n",
      "1/1 [==============================] - 7s - loss: 274.6472 - mean_squared_error: 274.6470\n",
      "Epoch 556/1000\n",
      "\n",
      "Training sigma=4.356542256155393\n",
      "1/1 [==============================] - 6s - loss: 307.3680 - mean_squared_error: 307.3678\n",
      "Epoch 557/1000\n",
      "\n",
      "Training sigma=4.184927580138345\n",
      "1/1 [==============================] - 6s - loss: 289.3682 - mean_squared_error: 289.3680\n",
      "Epoch 558/1000\n",
      "\n",
      "Training sigma=0.9680746441623034\n",
      "1/1 [==============================] - 6s - loss: 285.0833 - mean_squared_error: 285.0831\n",
      "Epoch 559/1000\n",
      "\n",
      "Training sigma=5.207208836135308\n",
      "1/1 [==============================] - 6s - loss: 306.9457 - mean_squared_error: 306.9454\n",
      "Epoch 560/1000\n",
      "\n",
      "Training sigma=7.763995480161402\n",
      "1/1 [==============================] - 6s - loss: 288.1190 - mean_squared_error: 288.1188\n",
      "Epoch 561/1000\n",
      "\n",
      "Training sigma=5.279008734181973\n",
      "1/1 [==============================] - 7s - loss: 269.4163 - mean_squared_error: 269.4161\n",
      "Epoch 562/1000\n",
      "\n",
      "Training sigma=2.7083987709179325\n",
      "1/1 [==============================] - 6s - loss: 309.6527 - mean_squared_error: 309.6525\n",
      "Epoch 563/1000\n",
      "\n",
      "Training sigma=5.609245113507421\n",
      "1/1 [==============================] - 7s - loss: 301.2677 - mean_squared_error: 301.2674\n",
      "Epoch 564/1000\n",
      "\n",
      "Training sigma=0.7342458241876526\n",
      "1/1 [==============================] - 7s - loss: 274.6293 - mean_squared_error: 274.6291\n",
      "Epoch 565/1000\n",
      "\n",
      "Training sigma=6.577365974448241\n",
      "1/1 [==============================] - 7s - loss: 305.7856 - mean_squared_error: 305.7854\n",
      "Epoch 566/1000\n",
      "\n",
      "Training sigma=3.2963339388470168\n",
      "1/1 [==============================] - 6s - loss: 298.8295 - mean_squared_error: 298.8293\n",
      "Epoch 567/1000\n",
      "\n",
      "Training sigma=1.4508806774427452\n",
      "1/1 [==============================] - 6s - loss: 284.5255 - mean_squared_error: 284.5254\n",
      "Epoch 568/1000\n",
      "\n",
      "Training sigma=4.563204658250864\n",
      "1/1 [==============================] - 7s - loss: 324.6082 - mean_squared_error: 324.6080\n",
      "Epoch 569/1000\n",
      "\n",
      "Training sigma=0.8824668178281236\n",
      "1/1 [==============================] - 6s - loss: 314.0226 - mean_squared_error: 314.0223\n",
      "Epoch 570/1000\n",
      "\n",
      "Training sigma=5.846536537352227\n",
      "1/1 [==============================] - 6s - loss: 270.6836 - mean_squared_error: 270.6834\n",
      "Epoch 571/1000\n",
      "\n",
      "Training sigma=7.180964228686195\n",
      "1/1 [==============================] - 7s - loss: 332.7579 - mean_squared_error: 332.7577\n",
      "Epoch 572/1000\n",
      "\n",
      "Training sigma=8.961960741310492\n",
      "1/1 [==============================] - 7s - loss: 333.5277 - mean_squared_error: 333.5274\n",
      "Epoch 573/1000\n",
      "\n",
      "Training sigma=7.526903640540853\n",
      "1/1 [==============================] - 7s - loss: 273.7497 - mean_squared_error: 273.7495\n",
      "Epoch 574/1000\n",
      "\n",
      "Training sigma=4.972079733597443\n",
      "1/1 [==============================] - 7s - loss: 369.2663 - mean_squared_error: 369.2660\n",
      "Epoch 575/1000\n",
      "\n",
      "Training sigma=9.464204483519122\n",
      "1/1 [==============================] - 6s - loss: 347.2866 - mean_squared_error: 347.2863\n",
      "Epoch 576/1000\n",
      "\n",
      "Training sigma=6.72474528865531\n",
      "1/1 [==============================] - 6s - loss: 295.3205 - mean_squared_error: 295.3202\n",
      "Epoch 577/1000\n",
      "\n",
      "Training sigma=6.314307547576416\n",
      "1/1 [==============================] - 7s - loss: 372.8295 - mean_squared_error: 372.8293\n",
      "Epoch 578/1000\n",
      "\n",
      "Training sigma=6.936814296444789\n",
      "1/1 [==============================] - 7s - loss: 295.2283 - mean_squared_error: 295.2281\n",
      "Epoch 579/1000\n",
      "\n",
      "Training sigma=9.74255547110018\n",
      "1/1 [==============================] - 6s - loss: 328.0537 - mean_squared_error: 328.0535\n",
      "Epoch 580/1000\n",
      "\n",
      "Training sigma=6.990202654539241\n",
      "1/1 [==============================] - 6s - loss: 302.9563 - mean_squared_error: 302.9561\n",
      "Epoch 581/1000\n",
      "\n",
      "Training sigma=4.4757167338858235\n",
      "1/1 [==============================] - 7s - loss: 277.6305 - mean_squared_error: 277.6303\n",
      "Epoch 582/1000\n",
      "\n",
      "Training sigma=6.494013249853059\n",
      "1/1 [==============================] - 6s - loss: 307.1339 - mean_squared_error: 307.1336\n",
      "Epoch 583/1000\n",
      "\n",
      "Training sigma=2.9781855577424645\n",
      "1/1 [==============================] - 6s - loss: 283.9739 - mean_squared_error: 283.9737\n",
      "Epoch 584/1000\n",
      "\n",
      "Training sigma=9.608172365713564\n",
      "1/1 [==============================] - 6s - loss: 316.0707 - mean_squared_error: 316.0705\n",
      "Epoch 585/1000\n",
      "\n",
      "Training sigma=6.475676630082008\n",
      "1/1 [==============================] - 6s - loss: 293.8217 - mean_squared_error: 293.8216\n",
      "Epoch 586/1000\n",
      "\n",
      "Training sigma=4.493737806229183\n",
      "1/1 [==============================] - 6s - loss: 294.5875 - mean_squared_error: 294.5873\n",
      "Epoch 587/1000\n",
      "\n",
      "Training sigma=2.0763797611240156\n",
      "1/1 [==============================] - 6s - loss: 305.4367 - mean_squared_error: 305.4365\n",
      "Epoch 588/1000\n",
      "\n",
      "Training sigma=0.7988257524238429\n",
      "1/1 [==============================] - 7s - loss: 267.5283 - mean_squared_error: 267.5281\n",
      "Epoch 589/1000\n",
      "\n",
      "Training sigma=6.320060633188619\n",
      "1/1 [==============================] - 7s - loss: 306.4184 - mean_squared_error: 306.4182\n",
      "Epoch 590/1000\n",
      "\n",
      "Training sigma=1.7311728696941986\n",
      "1/1 [==============================] - 6s - loss: 275.1861 - mean_squared_error: 275.1859\n",
      "Epoch 591/1000\n",
      "\n",
      "Training sigma=1.1359971486391973\n",
      "1/1 [==============================] - 7s - loss: 283.2418 - mean_squared_error: 283.2416\n",
      "Epoch 592/1000\n",
      "\n",
      "Training sigma=0.46648377744825775\n",
      "1/1 [==============================] - 7s - loss: 291.2100 - mean_squared_error: 291.2097\n",
      "Epoch 593/1000\n",
      "\n",
      "Training sigma=8.911039421125867\n",
      "1/1 [==============================] - 7s - loss: 271.8518 - mean_squared_error: 271.8516\n",
      "Epoch 594/1000\n",
      "\n",
      "Training sigma=5.428179494038159\n",
      "1/1 [==============================] - 6s - loss: 304.5112 - mean_squared_error: 304.5110\n",
      "Epoch 595/1000\n",
      "\n",
      "Training sigma=8.080219124686606\n",
      "1/1 [==============================] - 7s - loss: 309.2054 - mean_squared_error: 309.2052\n",
      "Epoch 596/1000\n",
      "\n",
      "Training sigma=8.682869262004136\n",
      "1/1 [==============================] - 7s - loss: 278.5705 - mean_squared_error: 278.5703\n",
      "Epoch 597/1000\n",
      "\n",
      "Training sigma=6.525762672563453\n",
      "1/1 [==============================] - 7s - loss: 325.1728 - mean_squared_error: 325.1725\n",
      "Epoch 598/1000\n",
      "\n",
      "Training sigma=7.582856409838577\n",
      "1/1 [==============================] - 7s - loss: 308.3728 - mean_squared_error: 308.3726\n",
      "Epoch 599/1000\n",
      "\n",
      "Training sigma=4.8401219627786265\n",
      "1/1 [==============================] - 6s - loss: 274.5298 - mean_squared_error: 274.5295\n",
      "Epoch 600/1000\n",
      "\n",
      "Training sigma=5.317180024187726\n",
      "1/1 [==============================] - 6s - loss: 317.3022 - mean_squared_error: 317.3020\n",
      "Epoch 601/1000\n",
      "\n",
      "Training sigma=9.790798916490001\n",
      "1/1 [==============================] - 6s - loss: 283.4600 - mean_squared_error: 283.4598\n",
      "Epoch 602/1000\n",
      "\n",
      "Training sigma=5.677227868485306\n",
      "1/1 [==============================] - 7s - loss: 296.0888 - mean_squared_error: 296.0886\n",
      "Epoch 603/1000\n",
      "\n",
      "Training sigma=1.7287998715231956\n",
      "1/1 [==============================] - 7s - loss: 300.4707 - mean_squared_error: 300.4705\n",
      "Epoch 604/1000\n",
      "\n",
      "Training sigma=8.103336697178943\n",
      "1/1 [==============================] - 7s - loss: 267.2497 - mean_squared_error: 267.2495\n",
      "Epoch 605/1000\n",
      "\n",
      "Training sigma=1.7083362648835065\n",
      "1/1 [==============================] - 7s - loss: 286.1656 - mean_squared_error: 286.1654\n",
      "Epoch 606/1000\n",
      "\n",
      "Training sigma=2.5857037440668282\n",
      "1/1 [==============================] - 7s - loss: 276.0862 - mean_squared_error: 276.0861\n",
      "Epoch 607/1000\n",
      "\n",
      "Training sigma=9.056300268076528\n",
      "1/1 [==============================] - 6s - loss: 264.8886 - mean_squared_error: 264.8884\n",
      "Epoch 608/1000\n",
      "\n",
      "Training sigma=8.982204426420552\n",
      "1/1 [==============================] - 6s - loss: 287.1085 - mean_squared_error: 287.1083\n",
      "Epoch 609/1000\n",
      "\n",
      "Training sigma=6.777115100579291\n",
      "1/1 [==============================] - 7s - loss: 274.6592 - mean_squared_error: 274.6589\n",
      "Epoch 610/1000\n",
      "\n",
      "Training sigma=9.207341906009331\n",
      "1/1 [==============================] - 6s - loss: 278.6750 - mean_squared_error: 278.6748\n",
      "Epoch 611/1000\n",
      "\n",
      "Training sigma=5.227413475400101\n",
      "1/1 [==============================] - 6s - loss: 285.6969 - mean_squared_error: 285.6967\n",
      "Epoch 612/1000\n",
      "\n",
      "Training sigma=8.936134950785034\n",
      "1/1 [==============================] - 6s - loss: 274.5704 - mean_squared_error: 274.5702\n",
      "Epoch 613/1000\n",
      "\n",
      "Training sigma=1.609534759066965\n",
      "1/1 [==============================] - 6s - loss: 271.6724 - mean_squared_error: 271.6722\n",
      "Epoch 614/1000\n",
      "\n",
      "Training sigma=7.687992305354063\n",
      "1/1 [==============================] - 6s - loss: 292.2010 - mean_squared_error: 292.2008\n",
      "Epoch 615/1000\n",
      "\n",
      "Training sigma=7.853446177652447\n",
      "1/1 [==============================] - 7s - loss: 268.6098 - mean_squared_error: 268.6096\n",
      "Epoch 616/1000\n",
      "\n",
      "Training sigma=3.715589054152375\n",
      "1/1 [==============================] - 7s - loss: 267.9131 - mean_squared_error: 267.9130\n",
      "Epoch 617/1000\n",
      "\n",
      "Training sigma=6.250134588721586\n",
      "1/1 [==============================] - 6s - loss: 293.1444 - mean_squared_error: 293.1442\n",
      "Epoch 618/1000\n",
      "\n",
      "Training sigma=9.476507837149317\n",
      "1/1 [==============================] - 6s - loss: 274.7397 - mean_squared_error: 274.7395\n",
      "Epoch 619/1000\n",
      "\n",
      "Training sigma=4.895547815782844\n",
      "1/1 [==============================] - 6s - loss: 270.3735 - mean_squared_error: 270.3733\n",
      "Epoch 620/1000\n",
      "\n",
      "Training sigma=0.7110823456572946\n",
      "1/1 [==============================] - 6s - loss: 275.8586 - mean_squared_error: 275.8584\n",
      "Epoch 621/1000\n",
      "\n",
      "Training sigma=0.29678817875801067\n",
      "1/1 [==============================] - 7s - loss: 277.0077 - mean_squared_error: 277.0074\n",
      "Epoch 622/1000\n",
      "\n",
      "Training sigma=3.8649551676687577\n",
      "1/1 [==============================] - 7s - loss: 268.4314 - mean_squared_error: 268.4312\n",
      "Epoch 623/1000\n",
      "\n",
      "Training sigma=7.558703266191245\n",
      "1/1 [==============================] - 7s - loss: 267.6418 - mean_squared_error: 267.6416\n",
      "Epoch 624/1000\n",
      "\n",
      "Training sigma=8.219653211315116\n",
      "1/1 [==============================] - 6s - loss: 275.2228 - mean_squared_error: 275.2227\n",
      "Epoch 625/1000\n",
      "\n",
      "Training sigma=2.1045066114896125\n",
      "1/1 [==============================] - 7s - loss: 289.2927 - mean_squared_error: 289.2925\n",
      "Epoch 626/1000\n",
      "\n",
      "Training sigma=8.178268634023517\n",
      "1/1 [==============================] - 6s - loss: 267.9779 - mean_squared_error: 267.9777\n",
      "Epoch 627/1000\n",
      "\n",
      "Training sigma=5.355163813725544\n",
      "1/1 [==============================] - 7s - loss: 274.1946 - mean_squared_error: 274.1944\n",
      "Epoch 628/1000\n",
      "\n",
      "Training sigma=8.614120675976723\n",
      "1/1 [==============================] - 6s - loss: 274.9833 - mean_squared_error: 274.9831\n",
      "Epoch 629/1000\n",
      "\n",
      "Training sigma=5.257026729152256\n",
      "1/1 [==============================] - 7s - loss: 277.7097 - mean_squared_error: 277.7094\n",
      "Epoch 630/1000\n",
      "\n",
      "Training sigma=0.10126962271917339\n",
      "1/1 [==============================] - 7s - loss: 264.9907 - mean_squared_error: 264.9905\n",
      "Epoch 631/1000\n",
      "\n",
      "Training sigma=8.400496098183742\n",
      "1/1 [==============================] - 6s - loss: 272.9167 - mean_squared_error: 272.9164\n",
      "Epoch 632/1000\n",
      "\n",
      "Training sigma=5.491041550779011\n",
      "1/1 [==============================] - 6s - loss: 292.2097 - mean_squared_error: 292.2095\n",
      "Epoch 633/1000\n",
      "\n",
      "Training sigma=3.6350123228953013\n",
      "1/1 [==============================] - 7s - loss: 279.5539 - mean_squared_error: 279.5537\n",
      "Epoch 634/1000\n",
      "\n",
      "Training sigma=8.598872405903144\n",
      "1/1 [==============================] - 7s - loss: 270.4031 - mean_squared_error: 270.4029\n",
      "Epoch 635/1000\n",
      "\n",
      "Training sigma=4.869974631464591\n",
      "1/1 [==============================] - 6s - loss: 259.7471 - mean_squared_error: 259.7469\n",
      "Epoch 636/1000\n",
      "\n",
      "Training sigma=4.935674817457855\n",
      "1/1 [==============================] - 6s - loss: 285.4065 - mean_squared_error: 285.4062\n",
      "Epoch 637/1000\n",
      "\n",
      "Training sigma=7.446659173826074\n",
      "1/1 [==============================] - 6s - loss: 284.7305 - mean_squared_error: 284.7303\n",
      "Epoch 638/1000\n",
      "\n",
      "Training sigma=4.443534114735021\n",
      "1/1 [==============================] - 6s - loss: 272.6679 - mean_squared_error: 272.6677\n",
      "Epoch 639/1000\n",
      "\n",
      "Training sigma=2.5852024674813423\n",
      "1/1 [==============================] - 7s - loss: 263.6812 - mean_squared_error: 263.6810\n",
      "Epoch 640/1000\n",
      "\n",
      "Training sigma=9.42031180900396\n",
      "1/1 [==============================] - 6s - loss: 272.1346 - mean_squared_error: 272.1343\n",
      "Epoch 641/1000\n",
      "\n",
      "Training sigma=4.225749950257889\n",
      "1/1 [==============================] - 6s - loss: 299.5805 - mean_squared_error: 299.5803\n",
      "Epoch 642/1000\n",
      "\n",
      "Training sigma=1.317648148666587\n",
      "1/1 [==============================] - 6s - loss: 315.9134 - mean_squared_error: 315.9132\n",
      "Epoch 643/1000\n",
      "\n",
      "Training sigma=5.43202307308992\n",
      "1/1 [==============================] - 6s - loss: 281.3934 - mean_squared_error: 281.3932\n",
      "Epoch 644/1000\n",
      "\n",
      "Training sigma=2.673013019194369\n",
      "1/1 [==============================] - 6s - loss: 268.6678 - mean_squared_error: 268.6676\n",
      "Epoch 645/1000\n",
      "\n",
      "Training sigma=9.77038586293473\n",
      "1/1 [==============================] - 6s - loss: 316.0344 - mean_squared_error: 316.0342\n",
      "Epoch 646/1000\n",
      "\n",
      "Training sigma=8.072873519645453\n",
      "1/1 [==============================] - 6s - loss: 329.2174 - mean_squared_error: 329.2172\n",
      "Epoch 647/1000\n",
      "\n",
      "Training sigma=9.610542685056725\n",
      "1/1 [==============================] - 7s - loss: 288.9525 - mean_squared_error: 288.9524\n",
      "Epoch 648/1000\n",
      "\n",
      "Training sigma=2.13620233166988\n",
      "1/1 [==============================] - 6s - loss: 302.0587 - mean_squared_error: 302.0584\n",
      "Epoch 649/1000\n",
      "\n",
      "Training sigma=2.5298797946368223\n",
      "1/1 [==============================] - 6s - loss: 328.2930 - mean_squared_error: 328.2927\n",
      "Epoch 650/1000\n",
      "\n",
      "Training sigma=5.937595747369272\n",
      "1/1 [==============================] - 6s - loss: 275.1428 - mean_squared_error: 275.1426\n",
      "Epoch 651/1000\n",
      "\n",
      "Training sigma=7.370136443475223\n",
      "1/1 [==============================] - 6s - loss: 286.5071 - mean_squared_error: 286.5069\n",
      "Epoch 652/1000\n",
      "\n",
      "Training sigma=0.9523839253677635\n",
      "1/1 [==============================] - 6s - loss: 308.0872 - mean_squared_error: 308.0870\n",
      "Epoch 653/1000\n",
      "\n",
      "Training sigma=6.184144838150461\n",
      "1/1 [==============================] - 6s - loss: 268.0869 - mean_squared_error: 268.0867\n",
      "Epoch 654/1000\n",
      "\n",
      "Training sigma=8.991519076950475\n",
      "1/1 [==============================] - 6s - loss: 321.0522 - mean_squared_error: 321.0520\n",
      "Epoch 655/1000\n",
      "\n",
      "Training sigma=3.335467918821359\n",
      "1/1 [==============================] - 6s - loss: 305.8586 - mean_squared_error: 305.8584\n",
      "Epoch 656/1000\n",
      "\n",
      "Training sigma=8.509776591385638\n",
      "1/1 [==============================] - 6s - loss: 275.3813 - mean_squared_error: 275.3811\n",
      "Epoch 657/1000\n",
      "\n",
      "Training sigma=8.72159790929656\n",
      "1/1 [==============================] - 6s - loss: 323.3036 - mean_squared_error: 323.3033\n",
      "Epoch 658/1000\n",
      "\n",
      "Training sigma=0.391981118803425\n",
      "1/1 [==============================] - 6s - loss: 272.9639 - mean_squared_error: 272.9637\n",
      "Epoch 659/1000\n",
      "\n",
      "Training sigma=8.533711204253917\n",
      "1/1 [==============================] - 6s - loss: 288.4132 - mean_squared_error: 288.4130\n",
      "Epoch 660/1000\n",
      "\n",
      "Training sigma=9.122299960989933\n",
      "1/1 [==============================] - 7s - loss: 306.8266 - mean_squared_error: 306.8264\n",
      "Epoch 661/1000\n",
      "\n",
      "Training sigma=6.3812479737759205\n",
      "1/1 [==============================] - 6s - loss: 267.2455 - mean_squared_error: 267.2453\n",
      "Epoch 662/1000\n",
      "\n",
      "Training sigma=4.186454817903912\n",
      "1/1 [==============================] - 6s - loss: 289.6594 - mean_squared_error: 289.6592\n",
      "Epoch 663/1000\n",
      "\n",
      "Training sigma=8.217343996282763\n",
      "1/1 [==============================] - 6s - loss: 279.9949 - mean_squared_error: 279.9947\n",
      "Epoch 664/1000\n",
      "\n",
      "Training sigma=6.198473890314293\n",
      "1/1 [==============================] - 7s - loss: 272.8921 - mean_squared_error: 272.8919\n",
      "Epoch 665/1000\n",
      "\n",
      "Training sigma=3.2424170384918805\n",
      "1/1 [==============================] - 6s - loss: 292.7793 - mean_squared_error: 292.7791\n",
      "Epoch 666/1000\n",
      "\n",
      "Training sigma=9.341733941671357\n",
      "1/1 [==============================] - 7s - loss: 268.7887 - mean_squared_error: 268.7885\n",
      "Epoch 667/1000\n",
      "\n",
      "Training sigma=2.9998063443983902\n",
      "1/1 [==============================] - 7s - loss: 268.8082 - mean_squared_error: 268.8080\n",
      "Epoch 668/1000\n",
      "\n",
      "Training sigma=2.1257708046331523\n",
      "1/1 [==============================] - 7s - loss: 282.3642 - mean_squared_error: 282.3640\n",
      "Epoch 669/1000\n",
      "\n",
      "Training sigma=6.022819992262065\n",
      "1/1 [==============================] - 6s - loss: 267.2444 - mean_squared_error: 267.2442\n",
      "Epoch 670/1000\n",
      "\n",
      "Training sigma=7.952009244154596\n",
      "1/1 [==============================] - 6s - loss: 267.6922 - mean_squared_error: 267.6920\n",
      "Epoch 671/1000\n",
      "\n",
      "Training sigma=9.35545608764923\n",
      "1/1 [==============================] - 7s - loss: 271.5037 - mean_squared_error: 271.5034\n",
      "Epoch 672/1000\n",
      "\n",
      "Training sigma=1.6416702353100177\n",
      "1/1 [==============================] - 6s - loss: 263.1680 - mean_squared_error: 263.1678\n",
      "Epoch 673/1000\n",
      "\n",
      "Training sigma=2.6650887006887314\n",
      "1/1 [==============================] - 7s - loss: 257.7340 - mean_squared_error: 257.7338\n",
      "Epoch 674/1000\n",
      "\n",
      "Training sigma=4.749804885507954\n",
      "1/1 [==============================] - 6s - loss: 269.8040 - mean_squared_error: 269.8037\n",
      "Epoch 675/1000\n",
      "\n",
      "Training sigma=4.999614405423312\n",
      "1/1 [==============================] - 6s - loss: 272.0464 - mean_squared_error: 272.0462\n",
      "Epoch 676/1000\n",
      "\n",
      "Training sigma=0.7822445206078799\n",
      "1/1 [==============================] - 6s - loss: 252.6228 - mean_squared_error: 252.6227\n",
      "Epoch 677/1000\n",
      "\n",
      "Training sigma=0.5697013976358145\n",
      "1/1 [==============================] - 7s - loss: 255.5869 - mean_squared_error: 255.5868\n",
      "Epoch 678/1000\n",
      "\n",
      "Training sigma=1.7700828013822145\n",
      "1/1 [==============================] - 6s - loss: 263.5942 - mean_squared_error: 263.5940\n",
      "Epoch 679/1000\n",
      "\n",
      "Training sigma=6.7045155736230475\n",
      "1/1 [==============================] - 6s - loss: 259.9970 - mean_squared_error: 259.9969\n",
      "Epoch 680/1000\n",
      "\n",
      "Training sigma=3.929838548490956\n",
      "1/1 [==============================] - 7s - loss: 262.7580 - mean_squared_error: 262.7578\n",
      "Epoch 681/1000\n",
      "\n",
      "Training sigma=2.948356703257642\n",
      "1/1 [==============================] - 7s - loss: 259.4073 - mean_squared_error: 259.4071\n",
      "Epoch 682/1000\n",
      "\n",
      "Training sigma=6.756589277042079\n",
      "1/1 [==============================] - 7s - loss: 269.3757 - mean_squared_error: 269.3755\n",
      "Epoch 683/1000\n",
      "\n",
      "Training sigma=9.976221206061703\n",
      "1/1 [==============================] - 6s - loss: 268.5091 - mean_squared_error: 268.5089\n",
      "Epoch 684/1000\n",
      "\n",
      "Training sigma=7.971709514389342\n",
      "1/1 [==============================] - 7s - loss: 257.3181 - mean_squared_error: 257.3180\n",
      "Epoch 685/1000\n",
      "\n",
      "Training sigma=6.931075910087111\n",
      "1/1 [==============================] - 7s - loss: 268.4384 - mean_squared_error: 268.4382\n",
      "Epoch 686/1000\n",
      "\n",
      "Training sigma=0.22160117943404445\n",
      "1/1 [==============================] - 6s - loss: 264.5476 - mean_squared_error: 264.5474\n",
      "Epoch 687/1000\n",
      "\n",
      "Training sigma=9.712699096939163\n",
      "1/1 [==============================] - 6s - loss: 268.6373 - mean_squared_error: 268.6371\n",
      "Epoch 688/1000\n",
      "\n",
      "Training sigma=1.0402806682005428\n",
      "1/1 [==============================] - 6s - loss: 268.8701 - mean_squared_error: 268.8699\n",
      "Epoch 689/1000\n",
      "\n",
      "Training sigma=8.048982705100777\n",
      "1/1 [==============================] - 6s - loss: 282.2376 - mean_squared_error: 282.2374\n",
      "Epoch 690/1000\n",
      "\n",
      "Training sigma=3.5891170949229902\n",
      "1/1 [==============================] - 7s - loss: 282.4094 - mean_squared_error: 282.4092\n",
      "Epoch 691/1000\n",
      "\n",
      "Training sigma=3.695569591579263\n",
      "1/1 [==============================] - 7s - loss: 278.5814 - mean_squared_error: 278.5812\n",
      "Epoch 692/1000\n",
      "\n",
      "Training sigma=1.9091774890296587\n",
      "1/1 [==============================] - 6s - loss: 265.3026 - mean_squared_error: 265.3024\n",
      "Epoch 693/1000\n",
      "\n",
      "Training sigma=6.659953795290319\n",
      "1/1 [==============================] - 7s - loss: 260.3483 - mean_squared_error: 260.3481\n",
      "Epoch 694/1000\n",
      "\n",
      "Training sigma=7.278100861983573\n",
      "1/1 [==============================] - 7s - loss: 285.4497 - mean_squared_error: 285.4495\n",
      "Epoch 695/1000\n",
      "\n",
      "Training sigma=8.289032174204538\n",
      "1/1 [==============================] - 6s - loss: 307.5251 - mean_squared_error: 307.5248\n",
      "Epoch 696/1000\n",
      "\n",
      "Training sigma=0.4519917594593126\n",
      "1/1 [==============================] - 7s - loss: 308.3807 - mean_squared_error: 308.3805\n",
      "Epoch 697/1000\n",
      "\n",
      "Training sigma=5.765645695170236\n",
      "1/1 [==============================] - 7s - loss: 282.2316 - mean_squared_error: 282.2314\n",
      "Epoch 698/1000\n",
      "\n",
      "Training sigma=9.943325736799824\n",
      "1/1 [==============================] - 7s - loss: 268.3398 - mean_squared_error: 268.3395\n",
      "Epoch 699/1000\n",
      "\n",
      "Training sigma=3.813177044693339\n",
      "1/1 [==============================] - 6s - loss: 293.5824 - mean_squared_error: 293.5822\n",
      "Epoch 700/1000\n",
      "\n",
      "Training sigma=7.458360301155561\n",
      "1/1 [==============================] - 7s - loss: 302.2366 - mean_squared_error: 302.2364\n",
      "Epoch 701/1000\n",
      "\n",
      "Training sigma=7.14781978728208\n",
      "1/1 [==============================] - 7s - loss: 286.5917 - mean_squared_error: 286.5914\n",
      "Epoch 702/1000\n",
      "\n",
      "Training sigma=7.613670334511014\n",
      "1/1 [==============================] - 7s - loss: 262.3412 - mean_squared_error: 262.3410\n",
      "Epoch 703/1000\n",
      "\n",
      "Training sigma=8.940698481087821\n",
      "1/1 [==============================] - 7s - loss: 297.2564 - mean_squared_error: 297.2563\n",
      "Epoch 704/1000\n",
      "\n",
      "Training sigma=6.362901114545987\n",
      "1/1 [==============================] - 6s - loss: 312.8075 - mean_squared_error: 312.8073\n",
      "Epoch 705/1000\n",
      "\n",
      "Training sigma=6.158380074478859\n",
      "1/1 [==============================] - 7s - loss: 274.9440 - mean_squared_error: 274.9438\n",
      "Epoch 706/1000\n",
      "\n",
      "Training sigma=5.311170552559239\n",
      "1/1 [==============================] - 7s - loss: 263.3409 - mean_squared_error: 263.3407\n",
      "Epoch 707/1000\n",
      "\n",
      "Training sigma=6.203906306637896\n",
      "1/1 [==============================] - 7s - loss: 281.6605 - mean_squared_error: 281.6603\n",
      "Epoch 708/1000\n",
      "\n",
      "Training sigma=8.831469430245805\n",
      "1/1 [==============================] - 7s - loss: 268.7670 - mean_squared_error: 268.7668\n",
      "Epoch 709/1000\n",
      "\n",
      "Training sigma=4.489739441602012\n",
      "1/1 [==============================] - 6s - loss: 266.3362 - mean_squared_error: 266.3360\n",
      "Epoch 710/1000\n",
      "\n",
      "Training sigma=4.897401818670662\n",
      "1/1 [==============================] - 6s - loss: 275.6639 - mean_squared_error: 275.6637\n",
      "Epoch 711/1000\n",
      "\n",
      "Training sigma=2.1928853326846944\n",
      "1/1 [==============================] - 6s - loss: 269.9297 - mean_squared_error: 269.9295\n",
      "Epoch 712/1000\n",
      "\n",
      "Training sigma=4.780051060828029\n",
      "1/1 [==============================] - 6s - loss: 264.2048 - mean_squared_error: 264.2046\n",
      "Epoch 713/1000\n",
      "\n",
      "Training sigma=0.8131918641929603\n",
      "1/1 [==============================] - 6s - loss: 271.8535 - mean_squared_error: 271.8532\n",
      "Epoch 714/1000\n",
      "\n",
      "Training sigma=5.098169938649208\n",
      "1/1 [==============================] - 7s - loss: 263.2844 - mean_squared_error: 263.2842\n",
      "Epoch 715/1000\n",
      "\n",
      "Training sigma=3.326647992108218\n",
      "1/1 [==============================] - 6s - loss: 267.6758 - mean_squared_error: 267.6756\n",
      "Epoch 716/1000\n",
      "\n",
      "Training sigma=4.571968156184908\n",
      "1/1 [==============================] - 6s - loss: 266.5682 - mean_squared_error: 266.5680\n",
      "Epoch 717/1000\n",
      "\n",
      "Training sigma=0.0270523811264356\n",
      "1/1 [==============================] - 6s - loss: 263.9185 - mean_squared_error: 263.9183\n",
      "Epoch 718/1000\n",
      "\n",
      "Training sigma=7.955092225584797\n",
      "1/1 [==============================] - 6s - loss: 263.8803 - mean_squared_error: 263.8801\n",
      "Epoch 719/1000\n",
      "\n",
      "Training sigma=0.13164769873155735\n",
      "1/1 [==============================] - 7s - loss: 267.7443 - mean_squared_error: 267.7440\n",
      "Epoch 720/1000\n",
      "\n",
      "Training sigma=6.5854755020657505\n",
      "1/1 [==============================] - 6s - loss: 263.5944 - mean_squared_error: 263.5942\n",
      "Epoch 721/1000\n",
      "\n",
      "Training sigma=7.854945060019927\n",
      "1/1 [==============================] - 7s - loss: 266.2653 - mean_squared_error: 266.2651\n",
      "Epoch 722/1000\n",
      "\n",
      "Training sigma=7.742133501790153\n",
      "1/1 [==============================] - 7s - loss: 266.1064 - mean_squared_error: 266.1062\n",
      "Epoch 723/1000\n",
      "\n",
      "Training sigma=8.68745450476537\n",
      "1/1 [==============================] - 7s - loss: 266.4754 - mean_squared_error: 266.4752\n",
      "Epoch 724/1000\n",
      "\n",
      "Training sigma=3.444759064826127\n",
      "1/1 [==============================] - 6s - loss: 262.8340 - mean_squared_error: 262.8338\n",
      "Epoch 725/1000\n",
      "\n",
      "Training sigma=7.647670856036532\n",
      "1/1 [==============================] - 7s - loss: 268.9243 - mean_squared_error: 268.9241\n",
      "Epoch 726/1000\n",
      "\n",
      "Training sigma=5.830114934960607\n",
      "1/1 [==============================] - 6s - loss: 280.1958 - mean_squared_error: 280.1956\n",
      "Epoch 727/1000\n",
      "\n",
      "Training sigma=0.5403966362759294\n",
      "1/1 [==============================] - 7s - loss: 267.4827 - mean_squared_error: 267.4825\n",
      "Epoch 728/1000\n",
      "\n",
      "Training sigma=3.0575618366964075\n",
      "1/1 [==============================] - 7s - loss: 259.9069 - mean_squared_error: 259.9067\n",
      "Epoch 729/1000\n",
      "\n",
      "Training sigma=5.231039177023091\n",
      "1/1 [==============================] - 6s - loss: 254.8772 - mean_squared_error: 254.8770\n",
      "Epoch 730/1000\n",
      "\n",
      "Training sigma=1.4744108746142015\n",
      "1/1 [==============================] - 6s - loss: 262.6656 - mean_squared_error: 262.6655\n",
      "Epoch 731/1000\n",
      "\n",
      "Training sigma=8.517032504819028\n",
      "1/1 [==============================] - 7s - loss: 271.2403 - mean_squared_error: 271.2401\n",
      "Epoch 732/1000\n",
      "\n",
      "Training sigma=2.427506933997222\n",
      "1/1 [==============================] - 6s - loss: 270.6267 - mean_squared_error: 270.6265\n",
      "Epoch 733/1000\n",
      "\n",
      "Training sigma=2.1741634210096494\n",
      "1/1 [==============================] - 6s - loss: 260.0042 - mean_squared_error: 260.0040\n",
      "Epoch 734/1000\n",
      "\n",
      "Training sigma=9.99823339985324\n",
      "1/1 [==============================] - 6s - loss: 262.0208 - mean_squared_error: 262.0205\n",
      "Epoch 735/1000\n",
      "\n",
      "Training sigma=8.424468443850998\n",
      "1/1 [==============================] - 6s - loss: 260.2835 - mean_squared_error: 260.2833\n",
      "Epoch 736/1000\n",
      "\n",
      "Training sigma=6.881792398814049\n",
      "1/1 [==============================] - 6s - loss: 261.2003 - mean_squared_error: 261.2002\n",
      "Epoch 737/1000\n",
      "\n",
      "Training sigma=4.324958685919586\n",
      "1/1 [==============================] - 6s - loss: 279.7481 - mean_squared_error: 279.7479\n",
      "Epoch 738/1000\n",
      "\n",
      "Training sigma=8.580591942813967\n",
      "1/1 [==============================] - 6s - loss: 286.3918 - mean_squared_error: 286.3917\n",
      "Epoch 739/1000\n",
      "\n",
      "Training sigma=3.09696814492467\n",
      "1/1 [==============================] - 6s - loss: 293.8047 - mean_squared_error: 293.8046\n",
      "Epoch 740/1000\n",
      "\n",
      "Training sigma=8.70904218974989\n",
      "1/1 [==============================] - 6s - loss: 298.5055 - mean_squared_error: 298.5053\n",
      "Epoch 741/1000\n",
      "\n",
      "Training sigma=0.5426500907352516\n",
      "1/1 [==============================] - 6s - loss: 275.0194 - mean_squared_error: 275.0192\n",
      "Epoch 742/1000\n",
      "\n",
      "Training sigma=3.6408142045547054\n",
      "1/1 [==============================] - 6s - loss: 258.4757 - mean_squared_error: 258.4756\n",
      "Epoch 743/1000\n",
      "\n",
      "Training sigma=8.839428014010132\n",
      "1/1 [==============================] - 6s - loss: 273.7590 - mean_squared_error: 273.7588\n",
      "Epoch 744/1000\n",
      "\n",
      "Training sigma=3.0729324701769887\n",
      "1/1 [==============================] - 6s - loss: 297.9594 - mean_squared_error: 297.9592\n",
      "Epoch 745/1000\n",
      "\n",
      "Training sigma=1.7525610853805862\n",
      "1/1 [==============================] - 7s - loss: 302.2503 - mean_squared_error: 302.2501\n",
      "Epoch 746/1000\n",
      "\n",
      "Training sigma=6.746088147207786\n",
      "1/1 [==============================] - 6s - loss: 261.0212 - mean_squared_error: 261.0210\n",
      "Epoch 747/1000\n",
      "\n",
      "Training sigma=6.161566749973858\n",
      "1/1 [==============================] - 6s - loss: 302.1774 - mean_squared_error: 302.1772\n",
      "Epoch 748/1000\n",
      "\n",
      "Training sigma=8.809390108944449\n",
      "1/1 [==============================] - 6s - loss: 331.9042 - mean_squared_error: 331.9040\n",
      "Epoch 749/1000\n",
      "\n",
      "Training sigma=3.3768566682727674\n",
      "1/1 [==============================] - 6s - loss: 289.7057 - mean_squared_error: 289.7055\n",
      "Epoch 750/1000\n",
      "\n",
      "Training sigma=8.969412445045938\n",
      "1/1 [==============================] - 6s - loss: 275.4406 - mean_squared_error: 275.4404\n",
      "Epoch 751/1000\n",
      "\n",
      "Training sigma=3.4560584270638275\n",
      "1/1 [==============================] - 7s - loss: 295.0138 - mean_squared_error: 295.0136\n",
      "Epoch 752/1000\n",
      "\n",
      "Training sigma=3.2792929566259765\n",
      "1/1 [==============================] - 6s - loss: 296.1590 - mean_squared_error: 296.1588\n",
      "Epoch 753/1000\n",
      "\n",
      "Training sigma=3.0202998601854993\n",
      "1/1 [==============================] - 6s - loss: 256.3279 - mean_squared_error: 256.3278\n",
      "Epoch 754/1000\n",
      "\n",
      "Training sigma=5.608680015821145\n",
      "1/1 [==============================] - 6s - loss: 276.9437 - mean_squared_error: 276.9435\n",
      "Epoch 755/1000\n",
      "\n",
      "Training sigma=6.75516274359394\n",
      "1/1 [==============================] - 7s - loss: 277.2966 - mean_squared_error: 277.2964\n",
      "Epoch 756/1000\n",
      "\n",
      "Training sigma=5.166682564936348\n",
      "1/1 [==============================] - 7s - loss: 260.5162 - mean_squared_error: 260.5161\n",
      "Epoch 757/1000\n",
      "\n",
      "Training sigma=6.166093633056311\n",
      "1/1 [==============================] - 6s - loss: 278.2546 - mean_squared_error: 278.2544\n",
      "Epoch 758/1000\n",
      "\n",
      "Training sigma=3.219491880270793\n",
      "1/1 [==============================] - 6s - loss: 273.1576 - mean_squared_error: 273.1574\n",
      "Epoch 759/1000\n",
      "\n",
      "Training sigma=0.7610668497643447\n",
      "1/1 [==============================] - 7s - loss: 267.2806 - mean_squared_error: 267.2804\n",
      "Epoch 760/1000\n",
      "\n",
      "Training sigma=7.197278681563226\n",
      "1/1 [==============================] - 7s - loss: 264.8273 - mean_squared_error: 264.8271\n",
      "Epoch 761/1000\n",
      "\n",
      "Training sigma=0.5120718621796028\n",
      "1/1 [==============================] - 6s - loss: 267.0742 - mean_squared_error: 267.0740\n",
      "Epoch 762/1000\n",
      "\n",
      "Training sigma=0.9630763748388871\n",
      "1/1 [==============================] - 7s - loss: 271.1508 - mean_squared_error: 271.1507\n",
      "Epoch 763/1000\n",
      "\n",
      "Training sigma=7.90033280953574\n",
      "1/1 [==============================] - 6s - loss: 258.8267 - mean_squared_error: 258.8265\n",
      "Epoch 764/1000\n",
      "\n",
      "Training sigma=0.790666612184534\n",
      "1/1 [==============================] - 7s - loss: 266.8090 - mean_squared_error: 266.8088\n",
      "Epoch 765/1000\n",
      "\n",
      "Training sigma=6.27046819116127\n",
      "1/1 [==============================] - 6s - loss: 266.9733 - mean_squared_error: 266.9731\n",
      "Epoch 766/1000\n",
      "\n",
      "Training sigma=2.111107767336083\n",
      "1/1 [==============================] - 6s - loss: 266.5380 - mean_squared_error: 266.5378\n",
      "Epoch 767/1000\n",
      "\n",
      "Training sigma=7.505720938379562\n",
      "1/1 [==============================] - 6s - loss: 261.4451 - mean_squared_error: 261.4449\n",
      "Epoch 768/1000\n",
      "\n",
      "Training sigma=5.7122111237100945\n",
      "1/1 [==============================] - 6s - loss: 261.1972 - mean_squared_error: 261.1970\n",
      "Epoch 769/1000\n",
      "\n",
      "Training sigma=0.2893107584817223\n",
      "1/1 [==============================] - 7s - loss: 259.5191 - mean_squared_error: 259.5189\n",
      "Epoch 770/1000\n",
      "\n",
      "Training sigma=5.9145682353967155\n",
      "1/1 [==============================] - 6s - loss: 272.5558 - mean_squared_error: 272.5556\n",
      "Epoch 771/1000\n",
      "\n",
      "Training sigma=7.25544987786575\n",
      "1/1 [==============================] - 7s - loss: 275.3690 - mean_squared_error: 275.3689\n",
      "Epoch 772/1000\n",
      "\n",
      "Training sigma=6.6161823747661686\n",
      "1/1 [==============================] - 7s - loss: 261.3282 - mean_squared_error: 261.3280\n",
      "Epoch 773/1000\n",
      "\n",
      "Training sigma=8.733787492421794\n",
      "1/1 [==============================] - 6s - loss: 258.8670 - mean_squared_error: 258.8668\n",
      "Epoch 774/1000\n",
      "\n",
      "Training sigma=1.7388056667669183\n",
      "1/1 [==============================] - 6s - loss: 270.3447 - mean_squared_error: 270.3445\n",
      "Epoch 775/1000\n",
      "\n",
      "Training sigma=5.164922453890601\n",
      "1/1 [==============================] - 7s - loss: 263.9435 - mean_squared_error: 263.9434\n",
      "Epoch 776/1000\n",
      "\n",
      "Training sigma=1.5811493134988774\n",
      "1/1 [==============================] - 7s - loss: 268.4673 - mean_squared_error: 268.4672\n",
      "Epoch 777/1000\n",
      "\n",
      "Training sigma=3.9298516247610538\n",
      "1/1 [==============================] - 7s - loss: 263.3891 - mean_squared_error: 263.3889\n",
      "Epoch 778/1000\n",
      "\n",
      "Training sigma=2.927390444291235\n",
      "1/1 [==============================] - 7s - loss: 257.7073 - mean_squared_error: 257.7072\n",
      "Epoch 779/1000\n",
      "\n",
      "Training sigma=5.070336527962649\n",
      "1/1 [==============================] - 7s - loss: 264.3558 - mean_squared_error: 264.3557\n",
      "Epoch 780/1000\n",
      "\n",
      "Training sigma=4.627611451293817\n",
      "1/1 [==============================] - 6s - loss: 273.1233 - mean_squared_error: 273.1231\n",
      "Epoch 781/1000\n",
      "\n",
      "Training sigma=5.634954935426118\n",
      "1/1 [==============================] - 6s - loss: 268.7911 - mean_squared_error: 268.7910\n",
      "Epoch 782/1000\n",
      "\n",
      "Training sigma=3.875482426123172\n",
      "1/1 [==============================] - 7s - loss: 275.1304 - mean_squared_error: 275.1302\n",
      "Epoch 783/1000\n",
      "\n",
      "Training sigma=0.8893728594605155\n",
      "1/1 [==============================] - 7s - loss: 264.2770 - mean_squared_error: 264.2769\n",
      "Epoch 784/1000\n",
      "\n",
      "Training sigma=6.924956566172166\n",
      "1/1 [==============================] - 7s - loss: 259.7386 - mean_squared_error: 259.7385\n",
      "Epoch 785/1000\n",
      "\n",
      "Training sigma=7.978894561960736\n",
      "1/1 [==============================] - 7s - loss: 267.2548 - mean_squared_error: 267.2546\n",
      "Epoch 786/1000\n",
      "\n",
      "Training sigma=7.567341917705822\n",
      "1/1 [==============================] - 7s - loss: 271.1621 - mean_squared_error: 271.1620\n",
      "Epoch 787/1000\n",
      "\n",
      "Training sigma=9.61664463078475\n",
      "1/1 [==============================] - 7s - loss: 264.6687 - mean_squared_error: 264.6685\n",
      "Epoch 788/1000\n",
      "\n",
      "Training sigma=8.05434087546162\n",
      "1/1 [==============================] - 6s - loss: 261.1165 - mean_squared_error: 261.1163\n",
      "Epoch 789/1000\n",
      "\n",
      "Training sigma=3.973700306301172\n",
      "1/1 [==============================] - 7s - loss: 260.1649 - mean_squared_error: 260.1647\n",
      "Epoch 790/1000\n",
      "\n",
      "Training sigma=3.33194453498007\n",
      "1/1 [==============================] - 7s - loss: 257.0970 - mean_squared_error: 257.0968\n",
      "Epoch 791/1000\n",
      "\n",
      "Training sigma=7.093593159100426\n",
      "1/1 [==============================] - 6s - loss: 267.2940 - mean_squared_error: 267.2939\n",
      "Epoch 792/1000\n",
      "\n",
      "Training sigma=9.674816758508683\n",
      "1/1 [==============================] - 6s - loss: 261.0029 - mean_squared_error: 261.0027\n",
      "Epoch 793/1000\n",
      "\n",
      "Training sigma=2.346806973223691\n",
      "1/1 [==============================] - 6s - loss: 258.8076 - mean_squared_error: 258.8074\n",
      "Epoch 794/1000\n",
      "\n",
      "Training sigma=8.375653389384006\n",
      "1/1 [==============================] - 6s - loss: 252.6020 - mean_squared_error: 252.6018\n",
      "Epoch 795/1000\n",
      "\n",
      "Training sigma=7.926640235476238\n",
      "1/1 [==============================] - 7s - loss: 261.1765 - mean_squared_error: 261.1763\n",
      "Epoch 796/1000\n",
      "\n",
      "Training sigma=4.252521558861676\n",
      "1/1 [==============================] - 7s - loss: 253.8652 - mean_squared_error: 253.8650\n",
      "Epoch 797/1000\n",
      "\n",
      "Training sigma=8.56002590440071\n",
      "1/1 [==============================] - 7s - loss: 261.7096 - mean_squared_error: 261.7094\n",
      "Epoch 798/1000\n",
      "\n",
      "Training sigma=8.628748407990303\n",
      "1/1 [==============================] - 6s - loss: 258.3284 - mean_squared_error: 258.3282\n",
      "Epoch 799/1000\n",
      "\n",
      "Training sigma=7.114549565891696\n",
      "1/1 [==============================] - 7s - loss: 256.0139 - mean_squared_error: 256.0137\n",
      "Epoch 800/1000\n",
      "\n",
      "Training sigma=8.250081709761266\n",
      "1/1 [==============================] - 7s - loss: 256.2783 - mean_squared_error: 256.2781\n",
      "Epoch 801/1000\n",
      "\n",
      "Training sigma=2.6064221545144575\n",
      "1/1 [==============================] - 7s - loss: 267.4879 - mean_squared_error: 267.4877\n",
      "Epoch 802/1000\n",
      "\n",
      "Training sigma=4.000375489327541\n",
      "1/1 [==============================] - 6s - loss: 278.5612 - mean_squared_error: 278.5610\n",
      "Epoch 803/1000\n",
      "\n",
      "Training sigma=0.673987166601755\n",
      "1/1 [==============================] - 7s - loss: 291.4426 - mean_squared_error: 291.4424\n",
      "Epoch 804/1000\n",
      "\n",
      "Training sigma=5.453308844406223\n",
      "1/1 [==============================] - 7s - loss: 283.5457 - mean_squared_error: 283.5455\n",
      "Epoch 805/1000\n",
      "\n",
      "Training sigma=6.983792739693623\n",
      "1/1 [==============================] - 7s - loss: 290.6861 - mean_squared_error: 290.6859\n",
      "Epoch 806/1000\n",
      "\n",
      "Training sigma=5.756542295313948\n",
      "1/1 [==============================] - 6s - loss: 268.0374 - mean_squared_error: 268.0372\n",
      "Epoch 807/1000\n",
      "\n",
      "Training sigma=0.09713349413971617\n",
      "1/1 [==============================] - 7s - loss: 261.4637 - mean_squared_error: 261.4635\n",
      "Epoch 808/1000\n",
      "\n",
      "Training sigma=4.92020366845512\n",
      "1/1 [==============================] - 6s - loss: 251.6148 - mean_squared_error: 251.6146\n",
      "Epoch 809/1000\n",
      "\n",
      "Training sigma=3.1813326124500794\n",
      "1/1 [==============================] - 6s - loss: 268.7217 - mean_squared_error: 268.7216\n",
      "Epoch 810/1000\n",
      "\n",
      "Training sigma=6.015792703330458\n",
      "1/1 [==============================] - 7s - loss: 272.1701 - mean_squared_error: 272.1699\n",
      "Epoch 811/1000\n",
      "\n",
      "Training sigma=1.064811447329802\n",
      "1/1 [==============================] - 7s - loss: 259.4354 - mean_squared_error: 259.4352\n",
      "Epoch 812/1000\n",
      "\n",
      "Training sigma=3.9046997597129307\n",
      "1/1 [==============================] - 6s - loss: 261.4221 - mean_squared_error: 261.4218\n",
      "Epoch 813/1000\n",
      "\n",
      "Training sigma=2.2440597262665607\n",
      "1/1 [==============================] - 7s - loss: 275.1403 - mean_squared_error: 275.1401\n",
      "Epoch 814/1000\n",
      "\n",
      "Training sigma=8.796739217255533\n",
      "1/1 [==============================] - 6s - loss: 266.1722 - mean_squared_error: 266.1720\n",
      "Epoch 815/1000\n",
      "\n",
      "Training sigma=2.9456321131792293\n",
      "1/1 [==============================] - 7s - loss: 262.1766 - mean_squared_error: 262.1764\n",
      "Epoch 816/1000\n",
      "\n",
      "Training sigma=5.820708735936938\n",
      "1/1 [==============================] - 7s - loss: 260.0334 - mean_squared_error: 260.0331\n",
      "Epoch 817/1000\n",
      "\n",
      "Training sigma=7.865578734882792\n",
      "1/1 [==============================] - 7s - loss: 260.6956 - mean_squared_error: 260.6955\n",
      "Epoch 818/1000\n",
      "\n",
      "Training sigma=1.1064351209055867\n",
      "1/1 [==============================] - 7s - loss: 265.0104 - mean_squared_error: 265.0102\n",
      "Epoch 819/1000\n",
      "\n",
      "Training sigma=8.161197364615461\n",
      "1/1 [==============================] - 6s - loss: 270.0254 - mean_squared_error: 270.0251\n",
      "Epoch 820/1000\n",
      "\n",
      "Training sigma=6.170460616319712\n",
      "1/1 [==============================] - 7s - loss: 265.6421 - mean_squared_error: 265.6419\n",
      "Epoch 821/1000\n",
      "\n",
      "Training sigma=9.510368015790773\n",
      "1/1 [==============================] - 7s - loss: 259.7129 - mean_squared_error: 259.7127\n",
      "Epoch 822/1000\n",
      "\n",
      "Training sigma=5.389839786289143\n",
      "1/1 [==============================] - 6s - loss: 258.3228 - mean_squared_error: 258.3226\n",
      "Epoch 823/1000\n",
      "\n",
      "Training sigma=8.667718644073142\n",
      "1/1 [==============================] - 6s - loss: 253.9626 - mean_squared_error: 253.9624\n",
      "Epoch 824/1000\n",
      "\n",
      "Training sigma=6.160226201463113\n",
      "1/1 [==============================] - 7s - loss: 259.8061 - mean_squared_error: 259.8059\n",
      "Epoch 825/1000\n",
      "\n",
      "Training sigma=7.807677490946108\n",
      "1/1 [==============================] - 6s - loss: 262.4753 - mean_squared_error: 262.4752\n",
      "Epoch 826/1000\n",
      "\n",
      "Training sigma=9.316260731435714\n",
      "1/1 [==============================] - 7s - loss: 262.4522 - mean_squared_error: 262.4521\n",
      "Epoch 827/1000\n",
      "\n",
      "Training sigma=9.160666676546835\n",
      "1/1 [==============================] - 6s - loss: 273.2334 - mean_squared_error: 273.2332\n",
      "Epoch 828/1000\n",
      "\n",
      "Training sigma=4.42090879648819\n",
      "1/1 [==============================] - 6s - loss: 289.2048 - mean_squared_error: 289.2046\n",
      "Epoch 829/1000\n",
      "\n",
      "Training sigma=3.7778264404848994\n",
      "1/1 [==============================] - 6s - loss: 275.4373 - mean_squared_error: 275.4370\n",
      "Epoch 830/1000\n",
      "\n",
      "Training sigma=3.9940217238598996\n",
      "1/1 [==============================] - 6s - loss: 272.5070 - mean_squared_error: 272.5067\n",
      "Epoch 831/1000\n",
      "\n",
      "Training sigma=8.059169809192104\n",
      "1/1 [==============================] - 7s - loss: 262.3998 - mean_squared_error: 262.3997\n",
      "Epoch 832/1000\n",
      "\n",
      "Training sigma=2.9619023529415567\n",
      "1/1 [==============================] - 7s - loss: 261.1117 - mean_squared_error: 261.1115\n",
      "Epoch 833/1000\n",
      "\n",
      "Training sigma=2.303539511549789\n",
      "1/1 [==============================] - 6s - loss: 265.8303 - mean_squared_error: 265.8301\n",
      "Epoch 834/1000\n",
      "\n",
      "Training sigma=5.307060152367122\n",
      "1/1 [==============================] - 7s - loss: 256.8155 - mean_squared_error: 256.8153\n",
      "Epoch 835/1000\n",
      "\n",
      "Training sigma=7.241667984612322\n",
      "1/1 [==============================] - 6s - loss: 266.9189 - mean_squared_error: 266.9187\n",
      "Epoch 836/1000\n",
      "\n",
      "Training sigma=8.321324385858073\n",
      "1/1 [==============================] - 7s - loss: 261.9928 - mean_squared_error: 261.9927\n",
      "Epoch 837/1000\n",
      "\n",
      "Training sigma=3.8465724237790466\n",
      "1/1 [==============================] - 6s - loss: 264.8775 - mean_squared_error: 264.8773\n",
      "Epoch 838/1000\n",
      "\n",
      "Training sigma=3.8047184251183164\n",
      "1/1 [==============================] - 7s - loss: 254.0336 - mean_squared_error: 254.0334\n",
      "Epoch 839/1000\n",
      "\n",
      "Training sigma=7.9574299850434596\n",
      "1/1 [==============================] - 7s - loss: 254.9458 - mean_squared_error: 254.9456\n",
      "Epoch 840/1000\n",
      "\n",
      "Training sigma=4.919829657395106\n",
      "1/1 [==============================] - 7s - loss: 258.0862 - mean_squared_error: 258.0860\n",
      "Epoch 841/1000\n",
      "\n",
      "Training sigma=7.642136234558708\n",
      "1/1 [==============================] - 6s - loss: 258.1352 - mean_squared_error: 258.1350\n",
      "Epoch 842/1000\n",
      "\n",
      "Training sigma=1.0856009720021864\n",
      "1/1 [==============================] - 7s - loss: 263.9424 - mean_squared_error: 263.9422\n",
      "Epoch 843/1000\n",
      "\n",
      "Training sigma=4.592989798663847\n",
      "1/1 [==============================] - 6s - loss: 255.9566 - mean_squared_error: 255.9564\n",
      "Epoch 844/1000\n",
      "\n",
      "Training sigma=2.7939725515676592\n",
      "1/1 [==============================] - 7s - loss: 259.0901 - mean_squared_error: 259.0899\n",
      "Epoch 845/1000\n",
      "\n",
      "Training sigma=7.85010775259265\n",
      "1/1 [==============================] - 7s - loss: 262.7823 - mean_squared_error: 262.7822\n",
      "Epoch 846/1000\n",
      "\n",
      "Training sigma=2.105056352690311\n",
      "1/1 [==============================] - 7s - loss: 262.8307 - mean_squared_error: 262.8305\n",
      "Epoch 847/1000\n",
      "\n",
      "Training sigma=2.9352608805854974\n",
      "1/1 [==============================] - 7s - loss: 271.8074 - mean_squared_error: 271.8073\n",
      "Epoch 848/1000\n",
      "\n",
      "Training sigma=2.440830864088256\n",
      "1/1 [==============================] - 7s - loss: 289.3023 - mean_squared_error: 289.3022\n",
      "Epoch 849/1000\n",
      "\n",
      "Training sigma=5.963053915069097\n",
      "1/1 [==============================] - 6s - loss: 311.2482 - mean_squared_error: 311.2480\n",
      "Epoch 850/1000\n",
      "\n",
      "Training sigma=8.660426287384663\n",
      "1/1 [==============================] - 6s - loss: 330.3195 - mean_squared_error: 330.3193\n",
      "Epoch 851/1000\n",
      "\n",
      "Training sigma=5.175969999171919\n",
      "1/1 [==============================] - 7s - loss: 333.8813 - mean_squared_error: 333.8811\n",
      "Epoch 852/1000\n",
      "\n",
      "Training sigma=9.853463704126101\n",
      "1/1 [==============================] - 6s - loss: 300.8741 - mean_squared_error: 300.8739\n",
      "Epoch 853/1000\n",
      "\n",
      "Training sigma=1.0253107778354331\n",
      "1/1 [==============================] - 6s - loss: 260.8973 - mean_squared_error: 260.8972\n",
      "Epoch 854/1000\n",
      "\n",
      "Training sigma=0.4029029001418927\n",
      "1/1 [==============================] - 6s - loss: 300.8706 - mean_squared_error: 300.8704\n",
      "Epoch 855/1000\n",
      "\n",
      "Training sigma=1.8654066678330683\n",
      "1/1 [==============================] - 6s - loss: 334.9511 - mean_squared_error: 334.9509\n",
      "Epoch 856/1000\n",
      "\n",
      "Training sigma=3.941265044823037\n",
      "1/1 [==============================] - 7s - loss: 274.1844 - mean_squared_error: 274.1842\n",
      "Epoch 857/1000\n",
      "\n",
      "Training sigma=9.845496715193109\n",
      "1/1 [==============================] - 7s - loss: 298.1656 - mean_squared_error: 298.1655\n",
      "Epoch 858/1000\n",
      "\n",
      "Training sigma=4.5604327404012865\n",
      "1/1 [==============================] - 6s - loss: 302.9636 - mean_squared_error: 302.9633\n",
      "Epoch 859/1000\n",
      "\n",
      "Training sigma=4.345391640573604\n",
      "1/1 [==============================] - 6s - loss: 263.4127 - mean_squared_error: 263.4125\n",
      "Epoch 860/1000\n",
      "\n",
      "Training sigma=9.75288148819051\n",
      "1/1 [==============================] - 6s - loss: 302.2666 - mean_squared_error: 302.2664\n",
      "Epoch 861/1000\n",
      "\n",
      "Training sigma=4.19799457607273\n",
      "1/1 [==============================] - 6s - loss: 298.8030 - mean_squared_error: 298.8028\n",
      "Epoch 862/1000\n",
      "\n",
      "Training sigma=3.4231205949084975\n",
      "1/1 [==============================] - 6s - loss: 272.2218 - mean_squared_error: 272.2216\n",
      "Epoch 863/1000\n",
      "\n",
      "Training sigma=1.822335503960022\n",
      "1/1 [==============================] - 6s - loss: 318.3637 - mean_squared_error: 318.3635\n",
      "Epoch 864/1000\n",
      "\n",
      "Training sigma=5.163356547386471\n",
      "1/1 [==============================] - 6s - loss: 295.9261 - mean_squared_error: 295.9259\n",
      "Epoch 865/1000\n",
      "\n",
      "Training sigma=8.194995713954162\n",
      "1/1 [==============================] - 6s - loss: 267.7726 - mean_squared_error: 267.7724\n",
      "Epoch 866/1000\n",
      "\n",
      "Training sigma=7.738185707487578\n",
      "1/1 [==============================] - 7s - loss: 313.0251 - mean_squared_error: 313.0248\n",
      "Epoch 867/1000\n",
      "\n",
      "Training sigma=9.83501184168641\n",
      "1/1 [==============================] - 7s - loss: 294.5873 - mean_squared_error: 294.5870\n",
      "Epoch 868/1000\n",
      "\n",
      "Training sigma=7.544842530764719\n",
      "1/1 [==============================] - 6s - loss: 274.3085 - mean_squared_error: 274.3083\n",
      "Epoch 869/1000\n",
      "\n",
      "Training sigma=1.0806302750238128\n",
      "1/1 [==============================] - 7s - loss: 323.1977 - mean_squared_error: 323.1974\n",
      "Epoch 870/1000\n",
      "\n",
      "Training sigma=3.309184180797674\n",
      "1/1 [==============================] - 6s - loss: 301.3896 - mean_squared_error: 301.3894\n",
      "Epoch 871/1000\n",
      "\n",
      "Training sigma=9.853303190936268\n",
      "1/1 [==============================] - 7s - loss: 277.7915 - mean_squared_error: 277.7913\n",
      "Epoch 872/1000\n",
      "\n",
      "Training sigma=5.627452530410678\n",
      "1/1 [==============================] - 7s - loss: 322.3510 - mean_squared_error: 322.3508\n",
      "Epoch 873/1000\n",
      "\n",
      "Training sigma=6.034608869294585\n",
      "1/1 [==============================] - 7s - loss: 283.4013 - mean_squared_error: 283.4011\n",
      "Epoch 874/1000\n",
      "\n",
      "Training sigma=3.95344958664663\n",
      "1/1 [==============================] - 6s - loss: 273.5830 - mean_squared_error: 273.5828\n",
      "Epoch 875/1000\n",
      "\n",
      "Training sigma=2.9031765655361843\n",
      "1/1 [==============================] - 7s - loss: 296.3907 - mean_squared_error: 296.3904\n",
      "Epoch 876/1000\n",
      "\n",
      "Training sigma=0.5471998778413334\n",
      "1/1 [==============================] - 7s - loss: 266.3025 - mean_squared_error: 266.3023\n",
      "Epoch 877/1000\n",
      "\n",
      "Training sigma=6.733002662486419\n",
      "1/1 [==============================] - 6s - loss: 283.2737 - mean_squared_error: 283.2735\n",
      "Epoch 878/1000\n",
      "\n",
      "Training sigma=0.5326557969360746\n",
      "1/1 [==============================] - 7s - loss: 266.9955 - mean_squared_error: 266.9953\n",
      "Epoch 879/1000\n",
      "\n",
      "Training sigma=5.672123861862069\n",
      "1/1 [==============================] - 6s - loss: 266.1821 - mean_squared_error: 266.1819\n",
      "Epoch 880/1000\n",
      "\n",
      "Training sigma=3.300686868584263\n",
      "1/1 [==============================] - 7s - loss: 268.0045 - mean_squared_error: 268.0044\n",
      "Epoch 881/1000\n",
      "\n",
      "Training sigma=3.2064522544212304\n",
      "1/1 [==============================] - 7s - loss: 266.3800 - mean_squared_error: 266.3798\n",
      "Epoch 882/1000\n",
      "\n",
      "Training sigma=2.4047293270567973\n",
      "1/1 [==============================] - 7s - loss: 272.4203 - mean_squared_error: 272.4201\n",
      "Epoch 883/1000\n",
      "\n",
      "Training sigma=6.288358843776484\n",
      "1/1 [==============================] - 7s - loss: 275.0459 - mean_squared_error: 275.0457\n",
      "Epoch 884/1000\n",
      "\n",
      "Training sigma=4.166744995845263\n",
      "1/1 [==============================] - 7s - loss: 266.9647 - mean_squared_error: 266.9644\n",
      "Epoch 885/1000\n",
      "\n",
      "Training sigma=4.32841001804789\n",
      "1/1 [==============================] - 7s - loss: 264.0429 - mean_squared_error: 264.0427\n",
      "Epoch 886/1000\n",
      "\n",
      "Training sigma=7.040565525988746\n",
      "1/1 [==============================] - 7s - loss: 271.5112 - mean_squared_error: 271.5110\n",
      "Epoch 887/1000\n",
      "\n",
      "Training sigma=8.470459639748984\n",
      "1/1 [==============================] - 7s - loss: 265.6753 - mean_squared_error: 265.6750\n",
      "Epoch 888/1000\n",
      "\n",
      "Training sigma=9.966969737064954\n",
      "1/1 [==============================] - 6s - loss: 263.0006 - mean_squared_error: 263.0004\n",
      "Epoch 889/1000\n",
      "\n",
      "Training sigma=0.2533860524624776\n",
      "1/1 [==============================] - 7s - loss: 284.0405 - mean_squared_error: 284.0403\n",
      "Epoch 890/1000\n",
      "\n",
      "Training sigma=2.1607882169033243\n",
      "1/1 [==============================] - 7s - loss: 260.7230 - mean_squared_error: 260.7228\n",
      "Epoch 891/1000\n",
      "\n",
      "Training sigma=0.14291561617355897\n",
      "1/1 [==============================] - 7s - loss: 260.3434 - mean_squared_error: 260.3432\n",
      "Epoch 892/1000\n",
      "\n",
      "Training sigma=1.9959559191546627\n",
      "1/1 [==============================] - 7s - loss: 260.5330 - mean_squared_error: 260.5328\n",
      "Epoch 893/1000\n",
      "\n",
      "Training sigma=9.394504645370121\n",
      "1/1 [==============================] - 7s - loss: 264.4723 - mean_squared_error: 264.4721\n",
      "Epoch 894/1000\n",
      "\n",
      "Training sigma=3.183525409698423\n",
      "1/1 [==============================] - 6s - loss: 256.5047 - mean_squared_error: 256.5045\n",
      "Epoch 895/1000\n",
      "\n",
      "Training sigma=6.945916758820003\n",
      "1/1 [==============================] - 7s - loss: 251.5767 - mean_squared_error: 251.5765\n",
      "Epoch 896/1000\n",
      "\n",
      "Training sigma=1.8446797468257714\n",
      "1/1 [==============================] - 7s - loss: 258.3073 - mean_squared_error: 258.3070\n",
      "Epoch 897/1000\n",
      "\n",
      "Training sigma=4.357834438223293\n",
      "1/1 [==============================] - 7s - loss: 254.9078 - mean_squared_error: 254.9076\n",
      "Epoch 898/1000\n",
      "\n",
      "Training sigma=2.3936720211283258\n",
      "1/1 [==============================] - 6s - loss: 260.0264 - mean_squared_error: 260.0262\n",
      "Epoch 899/1000\n",
      "\n",
      "Training sigma=7.532454331425663\n",
      "1/1 [==============================] - 6s - loss: 253.8108 - mean_squared_error: 253.8107\n",
      "Epoch 900/1000\n",
      "\n",
      "Training sigma=2.578437158028345\n",
      "1/1 [==============================] - 7s - loss: 266.3483 - mean_squared_error: 266.3481\n",
      "Epoch 901/1000\n",
      "\n",
      "Training sigma=7.4933614590973585\n",
      "1/1 [==============================] - 6s - loss: 267.2924 - mean_squared_error: 267.2922\n",
      "Epoch 902/1000\n",
      "\n",
      "Training sigma=8.574104977675038\n",
      "1/1 [==============================] - 7s - loss: 266.8729 - mean_squared_error: 266.8727\n",
      "Epoch 903/1000\n",
      "\n",
      "Training sigma=3.231017553635853\n",
      "1/1 [==============================] - 7s - loss: 256.2811 - mean_squared_error: 256.2809\n",
      "Epoch 904/1000\n",
      "\n",
      "Training sigma=8.235824542253283\n",
      "1/1 [==============================] - 6s - loss: 269.0610 - mean_squared_error: 269.0608\n",
      "Epoch 905/1000\n",
      "\n",
      "Training sigma=1.2691703248224695\n",
      "1/1 [==============================] - 6s - loss: 277.6249 - mean_squared_error: 277.6247\n",
      "Epoch 906/1000\n",
      "\n",
      "Training sigma=9.99083543843123\n",
      "1/1 [==============================] - 6s - loss: 264.6539 - mean_squared_error: 264.6537\n",
      "Epoch 907/1000\n",
      "\n",
      "Training sigma=0.060202428916501516\n",
      "1/1 [==============================] - 6s - loss: 264.9965 - mean_squared_error: 264.9962\n",
      "Epoch 908/1000\n",
      "\n",
      "Training sigma=9.177231627237397\n",
      "1/1 [==============================] - 7s - loss: 268.9218 - mean_squared_error: 268.9215\n",
      "Epoch 909/1000\n",
      "\n",
      "Training sigma=3.9970138333644867\n",
      "1/1 [==============================] - 7s - loss: 280.6109 - mean_squared_error: 280.6107\n",
      "Epoch 910/1000\n",
      "\n",
      "Training sigma=9.284772009470323\n",
      "1/1 [==============================] - 7s - loss: 272.1634 - mean_squared_error: 272.1632\n",
      "Epoch 911/1000\n",
      "\n",
      "Training sigma=5.187761539587768\n",
      "1/1 [==============================] - 6s - loss: 263.5060 - mean_squared_error: 263.5057\n",
      "Epoch 912/1000\n",
      "\n",
      "Training sigma=7.439036600171065\n",
      "1/1 [==============================] - 7s - loss: 261.8394 - mean_squared_error: 261.8392\n",
      "Epoch 913/1000\n",
      "\n",
      "Training sigma=5.643819584838139\n",
      "1/1 [==============================] - 6s - loss: 282.5822 - mean_squared_error: 282.5820\n",
      "Epoch 914/1000\n",
      "\n",
      "Training sigma=0.9639956317346365\n",
      "1/1 [==============================] - 7s - loss: 292.8207 - mean_squared_error: 292.8205\n",
      "Epoch 915/1000\n",
      "\n",
      "Training sigma=2.6070232012862284\n",
      "1/1 [==============================] - 6s - loss: 280.1052 - mean_squared_error: 280.1049\n",
      "Epoch 916/1000\n",
      "\n",
      "Training sigma=3.7285954732103646\n",
      "1/1 [==============================] - 6s - loss: 259.2195 - mean_squared_error: 259.2193\n",
      "Epoch 917/1000\n",
      "\n",
      "Training sigma=7.454376022634794\n",
      "1/1 [==============================] - 6s - loss: 267.5562 - mean_squared_error: 267.5560\n",
      "Epoch 918/1000\n",
      "\n",
      "Training sigma=6.7756497789996715\n",
      "1/1 [==============================] - 7s - loss: 285.0091 - mean_squared_error: 285.0088\n",
      "Epoch 919/1000\n",
      "\n",
      "Training sigma=8.771812702839792\n",
      "1/1 [==============================] - 6s - loss: 283.1435 - mean_squared_error: 283.1432\n",
      "Epoch 920/1000\n",
      "\n",
      "Training sigma=5.949890538308012\n",
      "1/1 [==============================] - 7s - loss: 267.2628 - mean_squared_error: 267.2626\n",
      "Epoch 921/1000\n",
      "\n",
      "Training sigma=5.7416625842671865\n",
      "1/1 [==============================] - 6s - loss: 277.2836 - mean_squared_error: 277.2834\n",
      "Epoch 922/1000\n",
      "\n",
      "Training sigma=9.398167534523779\n",
      "1/1 [==============================] - 7s - loss: 290.7303 - mean_squared_error: 290.7301\n",
      "Epoch 923/1000\n",
      "\n",
      "Training sigma=6.475518544555526\n",
      "1/1 [==============================] - 6s - loss: 287.2027 - mean_squared_error: 287.2025\n",
      "Epoch 924/1000\n",
      "\n",
      "Training sigma=3.6175840071180323\n",
      "1/1 [==============================] - 7s - loss: 255.7655 - mean_squared_error: 255.7654\n",
      "Epoch 925/1000\n",
      "\n",
      "Training sigma=3.6987713553351798\n",
      "1/1 [==============================] - 6s - loss: 286.2687 - mean_squared_error: 286.2685\n",
      "Epoch 926/1000\n",
      "\n",
      "Training sigma=6.239542234076739\n",
      "1/1 [==============================] - 7s - loss: 299.2434 - mean_squared_error: 299.2432\n",
      "Epoch 927/1000\n",
      "\n",
      "Training sigma=6.515515296263762\n",
      "1/1 [==============================] - 6s - loss: 266.2683 - mean_squared_error: 266.2681\n",
      "Epoch 928/1000\n",
      "\n",
      "Training sigma=3.7098047283816262\n",
      "1/1 [==============================] - 7s - loss: 274.5344 - mean_squared_error: 274.5341\n",
      "Epoch 929/1000\n",
      "\n",
      "Training sigma=8.687131259587055\n",
      "1/1 [==============================] - 6s - loss: 283.1709 - mean_squared_error: 283.1707\n",
      "Epoch 930/1000\n",
      "\n",
      "Training sigma=9.570655394367783\n",
      "1/1 [==============================] - 6s - loss: 263.4131 - mean_squared_error: 263.4129\n",
      "Epoch 931/1000\n",
      "\n",
      "Training sigma=5.33721653909914\n",
      "1/1 [==============================] - 7s - loss: 261.2467 - mean_squared_error: 261.2465\n",
      "Epoch 932/1000\n",
      "\n",
      "Training sigma=7.530140641416443\n",
      "1/1 [==============================] - 6s - loss: 270.9395 - mean_squared_error: 270.9393\n",
      "Epoch 933/1000\n",
      "\n",
      "Training sigma=7.444343774732813\n",
      "1/1 [==============================] - 6s - loss: 271.4861 - mean_squared_error: 271.4859\n",
      "Epoch 934/1000\n",
      "\n",
      "Training sigma=9.81898364930523\n",
      "1/1 [==============================] - 6s - loss: 269.1193 - mean_squared_error: 269.1191\n",
      "Epoch 935/1000\n",
      "\n",
      "Training sigma=0.8111476055244682\n",
      "1/1 [==============================] - 6s - loss: 265.0409 - mean_squared_error: 265.0406\n",
      "Epoch 936/1000\n",
      "\n",
      "Training sigma=3.771568018860582\n",
      "1/1 [==============================] - 7s - loss: 267.5858 - mean_squared_error: 267.5856\n",
      "Epoch 937/1000\n",
      "\n",
      "Training sigma=0.8991259590305256\n",
      "1/1 [==============================] - 7s - loss: 273.0783 - mean_squared_error: 273.0780\n",
      "Epoch 938/1000\n",
      "\n",
      "Training sigma=5.587927493065264\n",
      "1/1 [==============================] - 7s - loss: 263.8610 - mean_squared_error: 263.8608\n",
      "Epoch 939/1000\n",
      "\n",
      "Training sigma=4.5702865413787555\n",
      "1/1 [==============================] - 7s - loss: 255.9587 - mean_squared_error: 255.9585\n",
      "Epoch 940/1000\n",
      "\n",
      "Training sigma=3.104510516500346\n",
      "1/1 [==============================] - 7s - loss: 268.8022 - mean_squared_error: 268.8019\n",
      "Epoch 941/1000\n",
      "\n",
      "Training sigma=4.867832287093366\n",
      "1/1 [==============================] - 6s - loss: 260.0375 - mean_squared_error: 260.0373\n",
      "Epoch 942/1000\n",
      "\n",
      "Training sigma=1.6923776864645657\n",
      "1/1 [==============================] - 6s - loss: 253.7727 - mean_squared_error: 253.7725\n",
      "Epoch 943/1000\n",
      "\n",
      "Training sigma=9.149828094201698\n",
      "1/1 [==============================] - 7s - loss: 263.2535 - mean_squared_error: 263.2533\n",
      "Epoch 944/1000\n",
      "\n",
      "Training sigma=3.4968599434859104\n",
      "1/1 [==============================] - 7s - loss: 266.0073 - mean_squared_error: 266.0071\n",
      "Epoch 945/1000\n",
      "\n",
      "Training sigma=1.5796076410204463\n",
      "1/1 [==============================] - 6s - loss: 263.9803 - mean_squared_error: 263.9802\n",
      "Epoch 946/1000\n",
      "\n",
      "Training sigma=8.370463186241505\n",
      "1/1 [==============================] - 6s - loss: 267.2210 - mean_squared_error: 267.2208\n",
      "Epoch 947/1000\n",
      "\n",
      "Training sigma=2.135704955475677\n",
      "1/1 [==============================] - 7s - loss: 266.0363 - mean_squared_error: 266.0362\n",
      "Epoch 948/1000\n",
      "\n",
      "Training sigma=1.9046512617941114\n",
      "1/1 [==============================] - 6s - loss: 259.9984 - mean_squared_error: 259.9983\n",
      "Epoch 949/1000\n",
      "\n",
      "Training sigma=0.36327961204082215\n",
      "1/1 [==============================] - 7s - loss: 261.1682 - mean_squared_error: 261.1680\n",
      "Epoch 950/1000\n",
      "\n",
      "Training sigma=5.689116629246669\n",
      "1/1 [==============================] - 7s - loss: 266.4105 - mean_squared_error: 266.4103\n",
      "Epoch 951/1000\n",
      "\n",
      "Training sigma=5.687364052071093\n",
      "1/1 [==============================] - 7s - loss: 263.4294 - mean_squared_error: 263.4292\n",
      "Epoch 952/1000\n",
      "\n",
      "Training sigma=1.1090736529095935\n",
      "1/1 [==============================] - 7s - loss: 262.9421 - mean_squared_error: 262.9419\n",
      "Epoch 953/1000\n",
      "\n",
      "Training sigma=2.8665185134588835\n",
      "1/1 [==============================] - 6s - loss: 255.4095 - mean_squared_error: 255.4093\n",
      "Epoch 954/1000\n",
      "\n",
      "Training sigma=6.9676188917958335\n",
      "1/1 [==============================] - 7s - loss: 258.2441 - mean_squared_error: 258.2439\n",
      "Epoch 955/1000\n",
      "\n",
      "Training sigma=9.831293500620248\n",
      "1/1 [==============================] - 6s - loss: 258.0306 - mean_squared_error: 258.0305\n",
      "Epoch 956/1000\n",
      "\n",
      "Training sigma=2.501255994836974\n",
      "1/1 [==============================] - 6s - loss: 255.0848 - mean_squared_error: 255.0846\n",
      "Epoch 957/1000\n",
      "\n",
      "Training sigma=6.173379878224368\n",
      "1/1 [==============================] - 6s - loss: 262.6966 - mean_squared_error: 262.6964\n",
      "Epoch 958/1000\n",
      "\n",
      "Training sigma=1.8933347049181815\n",
      "1/1 [==============================] - 6s - loss: 252.4046 - mean_squared_error: 252.4045\n",
      "Epoch 959/1000\n",
      "\n",
      "Training sigma=7.859465208348107\n",
      "1/1 [==============================] - 6s - loss: 253.5710 - mean_squared_error: 253.5708\n",
      "Epoch 960/1000\n",
      "\n",
      "Training sigma=5.340832908214766\n",
      "1/1 [==============================] - 7s - loss: 266.2765 - mean_squared_error: 266.2763\n",
      "Epoch 961/1000\n",
      "\n",
      "Training sigma=1.119582308980548\n",
      "1/1 [==============================] - 7s - loss: 273.2419 - mean_squared_error: 273.2417\n",
      "Epoch 962/1000\n",
      "\n",
      "Training sigma=2.317152659376911\n",
      "1/1 [==============================] - 6s - loss: 265.6016 - mean_squared_error: 265.6014\n",
      "Epoch 963/1000\n",
      "\n",
      "Training sigma=6.796398091723729\n",
      "1/1 [==============================] - 6s - loss: 267.5840 - mean_squared_error: 267.5838\n",
      "Epoch 964/1000\n",
      "\n",
      "Training sigma=0.8109239678893809\n",
      "1/1 [==============================] - 7s - loss: 268.2786 - mean_squared_error: 268.2784\n",
      "Epoch 965/1000\n",
      "\n",
      "Training sigma=4.322456414806038\n",
      "1/1 [==============================] - 7s - loss: 266.6158 - mean_squared_error: 266.6156\n",
      "Epoch 966/1000\n",
      "\n",
      "Training sigma=7.282176630950924\n",
      "1/1 [==============================] - 6s - loss: 254.2629 - mean_squared_error: 254.2628\n",
      "Epoch 967/1000\n",
      "\n",
      "Training sigma=0.5885071510920803\n",
      "1/1 [==============================] - 6s - loss: 266.2304 - mean_squared_error: 266.2302\n",
      "Epoch 968/1000\n",
      "\n",
      "Training sigma=0.34595402834494693\n",
      "1/1 [==============================] - 7s - loss: 253.0163 - mean_squared_error: 253.0161\n",
      "Epoch 969/1000\n",
      "\n",
      "Training sigma=4.680293260775502\n",
      "1/1 [==============================] - 7s - loss: 254.7624 - mean_squared_error: 254.7622\n",
      "Epoch 970/1000\n",
      "\n",
      "Training sigma=2.891025170830895\n",
      "1/1 [==============================] - 7s - loss: 253.8963 - mean_squared_error: 253.8961\n",
      "Epoch 971/1000\n",
      "\n",
      "Training sigma=4.5016440516710325\n",
      "1/1 [==============================] - 7s - loss: 251.1895 - mean_squared_error: 251.1893\n",
      "Epoch 972/1000\n",
      "\n",
      "Training sigma=6.012092100866687\n",
      "1/1 [==============================] - 6s - loss: 267.6334 - mean_squared_error: 267.6332\n",
      "Epoch 973/1000\n",
      "\n",
      "Training sigma=8.181159918677256\n",
      "1/1 [==============================] - 7s - loss: 265.6428 - mean_squared_error: 265.6426\n",
      "Epoch 974/1000\n",
      "\n",
      "Training sigma=7.106048364119086\n",
      "1/1 [==============================] - 7s - loss: 265.8078 - mean_squared_error: 265.8076\n",
      "Epoch 975/1000\n",
      "\n",
      "Training sigma=4.931650158965974\n",
      "1/1 [==============================] - 7s - loss: 264.9190 - mean_squared_error: 264.9188\n",
      "Epoch 976/1000\n",
      "\n",
      "Training sigma=0.09805732544667878\n",
      "1/1 [==============================] - 6s - loss: 261.6100 - mean_squared_error: 261.6098\n",
      "Epoch 977/1000\n",
      "\n",
      "Training sigma=3.6544147899279835\n",
      "1/1 [==============================] - 7s - loss: 260.7677 - mean_squared_error: 260.7675\n",
      "Epoch 978/1000\n",
      "\n",
      "Training sigma=2.8314949140336187\n",
      "1/1 [==============================] - 7s - loss: 263.6227 - mean_squared_error: 263.6226\n",
      "Epoch 979/1000\n",
      "\n",
      "Training sigma=7.8147192781407\n",
      "1/1 [==============================] - 7s - loss: 266.5096 - mean_squared_error: 266.5094\n",
      "Epoch 980/1000\n",
      "\n",
      "Training sigma=3.355856071447719\n",
      "1/1 [==============================] - 7s - loss: 276.8674 - mean_squared_error: 276.8672\n",
      "Epoch 981/1000\n",
      "\n",
      "Training sigma=7.857103925931378\n",
      "1/1 [==============================] - 7s - loss: 276.8402 - mean_squared_error: 276.8400\n",
      "Epoch 982/1000\n",
      "\n",
      "Training sigma=6.878236596514298\n",
      "1/1 [==============================] - 7s - loss: 267.6419 - mean_squared_error: 267.6417\n",
      "Epoch 983/1000\n",
      "\n",
      "Training sigma=6.52675049652207\n",
      "1/1 [==============================] - 6s - loss: 260.5661 - mean_squared_error: 260.5659\n",
      "Epoch 984/1000\n",
      "\n",
      "Training sigma=0.7601498811423091\n",
      "1/1 [==============================] - 7s - loss: 260.1498 - mean_squared_error: 260.1496\n",
      "Epoch 985/1000\n",
      "\n",
      "Training sigma=4.825778005420883\n",
      "1/1 [==============================] - 7s - loss: 265.1830 - mean_squared_error: 265.1828\n",
      "Epoch 986/1000\n",
      "\n",
      "Training sigma=2.6163906191695627\n",
      "1/1 [==============================] - 6s - loss: 277.4141 - mean_squared_error: 277.4139\n",
      "Epoch 987/1000\n",
      "\n",
      "Training sigma=1.412692668659955\n",
      "1/1 [==============================] - 7s - loss: 268.0232 - mean_squared_error: 268.0230\n",
      "Epoch 988/1000\n",
      "\n",
      "Training sigma=3.290529124493249\n",
      "1/1 [==============================] - 7s - loss: 262.5584 - mean_squared_error: 262.5583\n",
      "Epoch 989/1000\n",
      "\n",
      "Training sigma=2.223675327484962\n",
      "1/1 [==============================] - 7s - loss: 255.9568 - mean_squared_error: 255.9566\n",
      "Epoch 990/1000\n",
      "\n",
      "Training sigma=4.265233613942122\n",
      "1/1 [==============================] - 6s - loss: 257.0746 - mean_squared_error: 257.0745\n",
      "Epoch 991/1000\n",
      "\n",
      "Training sigma=4.4120827878125315\n",
      "1/1 [==============================] - 7s - loss: 267.9308 - mean_squared_error: 267.9307\n",
      "Epoch 992/1000\n",
      "\n",
      "Training sigma=3.036818747099171\n",
      "1/1 [==============================] - 6s - loss: 271.5083 - mean_squared_error: 271.5081\n",
      "Epoch 993/1000\n",
      "\n",
      "Training sigma=7.82562912378948\n",
      "1/1 [==============================] - 7s - loss: 260.9894 - mean_squared_error: 260.9893\n",
      "Epoch 994/1000\n",
      "\n",
      "Training sigma=4.479632269675865\n",
      "1/1 [==============================] - 7s - loss: 259.4893 - mean_squared_error: 259.4891\n",
      "Epoch 995/1000\n",
      "\n",
      "Training sigma=9.002041186127755\n",
      "1/1 [==============================] - 7s - loss: 260.3701 - mean_squared_error: 260.3698\n",
      "Epoch 996/1000\n",
      "\n",
      "Training sigma=5.164474538926571\n",
      "1/1 [==============================] - 6s - loss: 250.7645 - mean_squared_error: 250.7643\n",
      "Epoch 997/1000\n",
      "\n",
      "Training sigma=8.279334846893192\n",
      "1/1 [==============================] - 6s - loss: 252.8690 - mean_squared_error: 252.8688\n",
      "Epoch 998/1000\n",
      "\n",
      "Training sigma=4.5591218715046065\n",
      "1/1 [==============================] - 7s - loss: 258.8963 - mean_squared_error: 258.8961\n",
      "Epoch 999/1000\n",
      "\n",
      "Training sigma=8.683722067316456\n",
      "1/1 [==============================] - 7s - loss: 250.3216 - mean_squared_error: 250.3214\n",
      "Epoch 1000/1000\n",
      "\n",
      "Training sigma=2.5393605687054666\n",
      "1/1 [==============================] - 6s - loss: 259.3896 - mean_squared_error: 259.3895\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(gaussianGenerator, steps_per_epoch=1, nb_epoch=NUM_EPOCHS, verbose=1, nb_worker=1)\n",
    "model.save('model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training sigma=1.1299001118350593\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEZEip6AXESlyCnoRkSKnoBcRKXL/Dz+xRNcofc2mAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdc4c6b8f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power ttest:  0.820013967874\n",
      "Power NN-test:  0.834780005986\n",
      "False alarm ttest:  0.0508168788213\n",
      "False alarm NN-test:  0.0508168788213\n"
     ]
    }
   ],
   "source": [
    "X = gaussianGenerator.__next__()\n",
    "yPred = model.predict(X[0])\n",
    "\n",
    "plt.plot(X[1]['main_output'])\n",
    "plt.plot(yPred)\n",
    "plt.show()\n",
    "\n",
    "TH = 5\n",
    "THnn = 10\n",
    "power_ttest = np.sum( np.multiply(X[1]['main_output'] < TH, X[1]['hypothesis'] == 1) ) / np.sum( X[1]['hypothesis'])\n",
    "power_nntest = np.sum( np.multiply(yPred < THnn, X[1]['hypothesis'] == 1) ) / np.sum( X[1]['hypothesis'])\n",
    "\n",
    "fa_ttest = np.sum( np.multiply(X[1]['main_output'] < TH, X[1]['hypothesis'] == 0) ) / np.sum( X[1]['hypothesis'] == 0)\n",
    "fa_nntest = np.sum( np.multiply(yPred < THnn, X[1]['hypothesis'] == 0) ) / np.sum( X[1]['hypothesis'] == 0)\n",
    "\n",
    "print('Power ttest: ', power_ttest)\n",
    "print('Power NN-test: ', power_nntest)\n",
    "print('False alarm ttest: ', fa_ttest)\n",
    "print('False alarm NN-test: ', fa_nntest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
